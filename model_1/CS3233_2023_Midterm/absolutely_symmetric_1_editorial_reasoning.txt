We are given an n x n matrix A. We wish to decompose A into the sum of k absolutely symmetric matrices (each X_i satisfies |(X_i)_{p,q}| = |(X_i)_{q,p}| for every p,q) and minimize k.

First, note that the condition of absolute symmetry can be rewritten as:
  (X_i)_{p,q} = s * a, and (X_i)_{q,p} = s' * a, where a is a nonnegative integer and s, s' are in {-1, 1} such that |s * a| = |s' * a| = a.

However, note that the condition |T_{i,j}| = |T_{j,i}| is equivalent to T_{i,j} = T_{j,i} OR T_{i,j} = -T_{j,i}. But observe: if we have T_{i,j} = a and T_{j,i} = -a, then |a| = | -a | = |a| so it satisfies. Similarly, if both are a, then |a|=|a|, and if both are -a, then also |a|=|a|.

But note: the condition does not force the signs to be the same. It only forces the absolute values to be the same.

We can think of the matrix as having two types of entries:
  For i = j: diagonal entries. Since i=j, the condition becomes |a_{ii}| = |a_{ii}|, which is always true. So the diagonal entries are free: we can set them arbitrarily (as long as integer) and they don't impose a sign constraint.

  For i != j: the pair (i,j) and (j,i). We require that the absolute values of the two entries in the pair are equal. However, note that in the decomposition, each matrix X_i must have the property that for each pair (i,j) and (j,i), the absolute values are equal.

But note: the problem is to decompose the entire matrix A into k such matrices.

Let us denote:
  For each pair (i,j) and (j,i) (with i != j), we have two equations: 
      A_{i,j} = sum_{s=1}^{k} (X_s)_{i,j}
      A_{j,i} = sum_{s=1}^{k} (X_s)_{j,i}

And for each such pair and each matrix X_s, we have |(X_s)_{i,j}| = |(X_s)_{j,i}|.

Also, the diagonal entries: 
      A_{i,i} = sum_{s=1}^{k} (X_s)_{i,i}

But note: the diagonal entries in each X_s can be arbitrary integers? Actually, the condition for absolute symmetry for the diagonal is automatically satisfied. So we are free to set (X_s)_{i,i} arbitrarily.

However, the off-diagonals are constrained in pairs.

Observation:
  Consider the entire matrix. For each pair (i, j) with i < j, we have two values: A_{i,j} and A_{j,i}. The decomposition must assign these two values as sums of contributions from k matrices, and in each matrix the two contributions must have the same absolute value.

Let us denote for a fixed pair (i,j) (i<j) and a fixed matrix X_s:
    Let (X_s)_{i,j} = a_{s} and (X_s)_{j,i} = b_{s}. Then we require |a_{s}| = |b_{s}|.

Therefore, a_{s} and b_{s} can be:
    Case 1: a_{s} = b_{s} = c_{s}   (both the same positive/negative value, but note the absolute value condition is satisfied if they are equal in absolute value, so actually they can be different in sign? Wait: no, they can be different in sign? Actually, they can be: 
        Option 1: a_{s} = b_{s} = c (same value)
        Option 2: a_{s} = c, b_{s} = -c (opposite signs)
        Option 3: a_{s} = -c, b_{s} = c
        Option 4: a_{s} = -c, b_{s} = -c

But note that the condition is |a_{s}| = |b_{s}|. So we can have any of the above. However, note that if we set a_{s} = c and b_{s} = -c, then the two values are negatives. Similarly, we can have both positive or both negative.

Therefore, for the entire pair (i,j) and (j,i), we have:

    A_{i,j} = sum_{s} a_{s}
    A_{j,i} = sum_{s} b_{s}

and for each s, |a_{s}| = |b_{s}|.

This is reminiscent of decomposing two numbers (A_{i,j} and A_{j,i}) into sums of pairs (a_{s}, b_{s}) that lie in the set of pairs of the form (x, y) with |x| = |y|.

Note: the set of pairs (x,y) with |x| = |y| is the same as the set of vectors that are either of the form (c, c), (c, -c), (-c, c), (-c, -c) for some nonnegative integer c.

But note: we are allowed to use integers arbitrarily. Actually, we can write the pair (a_{s}, b_{s}) as:
    a_{s} = d_{s} * c_{s}
    b_{s} = e_{s} * c_{s}
where d_{s}, e_{s} are in { -1, 1 } and c_{s} is a nonnegative integer? Actually, no: because we can have 0 as well. Actually, if c_{s}=0 then it doesn't matter. Also note that we can have fractional assignments? But note: we are going to assign integers. The c_{s} must be nonnegative integers? Actually, the absolute value condition forces the absolute values to be the same, so we can let c_{s} = |a_{s}| = |b_{s}|. Then the signs are independent.

But note: the problem does not require the same c_{s} for the entire matrix, but we are free to choose per matrix.

However, we are to minimize k.

What constraints must hold for the entire pair (i,j) and (j,i)? 

Let S = (A_{i,j}, A_{j,i}). We want to express S as a sum of k vectors, each of which is in the set V = { (x,y) : |x| = |y| }.

But note: we can also use the diagonal independently? Actually, the diagonal entries are independent: they don't interact with off-diagonals? Actually, in the condition for absolute symmetry, the diagonal entry (i,i) is only constrained by itself: |(X_s)_{i,i}| = |(X_s)_{i,i}| -> always true. So we can set the diagonal arbitrarily.

Therefore, we can separate the problem:

  Step 1: Check if the decomposition exists.

  Step 2: Minimize k.

But note: the off-diagonals are independent per pair? Actually, each matrix X_s must assign values to every entry. However, we are free to assign the diagonal arbitrarily. The off-diagonals are independent per unordered pair {i, j} (with i != j) and the diagonal per index i.

However, the catch: the same matrix X_s must simultaneously satisfy the absolute symmetry condition for every pair (i,j). But note: the condition is local to each pair: the constraint for (i,j) does not affect (i,k) or (j,k). Therefore, we can decompose each pair independently? But wait: one matrix X_s assigns one value to (i,j) and one value to (j,i), and simultaneously to (i,k) and (k,i), etc. So the assignments for different pairs in the same matrix are independent? 

Actually, they are independent: the condition for absolute symmetry is per pair (i,j). Therefore, we can consider each pair independently? But then how do we assign the diagonal? 

But note: the diagonal is independent. For the diagonal, we have to assign the entire diagonal of A as the sum of the diagonals of the X_s. Since each diagonal entry A_{i,i} is an integer, and we can assign each (X_s)_{i,i} arbitrarily (as long as integer), we can always decompose the diagonal with at most 1 matrix? Actually, we can do:

  We have k matrices. The diagonal entries for each row i: we can assign the entire diagonal in one matrix? Actually, we can assign each matrix arbitrarily to the diagonals. 

However, we must also assign the off-diagonals. So we have to consider the entire matrix at once.

But note: the constraints for the off-diagonals (for each pair (i,j)) and the diagonals (for each i) are independent. Therefore, the minimal k must be at least the maximum of the minimal k needed for the diagonal and the minimal k needed for each off-diagonal pair.

But wait: we are using the same set of matrices to cover all entries. So we cannot assign independently per pair? Actually, we can: because the same matrix can assign a value to (i,j) and (j,i) and also to (p,q) and (q,p) and to diagonals arbitrarily. There is no constraint that links the value at (i,j) to the value at (p,q). So the entire matrix decomposition can be seen as:

  We are free to assign for each matrix X_s and for each pair (i,j) independently as long as the absolute symmetry condition holds for that pair.

  And for the diagonal, we are free to assign any integer to (X_s)_{i,i}.

Therefore, the problem decomposes into independent problems:

  For the diagonal: we have n numbers A_{i,i} (for i from 1 to n). We wish to express the vector D = (A_{1,1}, A_{2,2}, ..., A_{n,n}) as the sum of k vectors, each of which is an n-dimensional vector of integers. Since there are no constraints on the diagonal (other than being integers), we can use one matrix to set the entire diagonal? Actually, we can set the diagonal in multiple matrices arbitrarily. The minimal number of matrices for the diagonal is 1? Actually, no: we can set each diagonal entry independently in separate matrices? But we want minimal k.

However, note: we are already going to use k matrices for the off-diagonals. We can use the same matrices to set the diagonal. So we don't need extra matrices for the diagonal.

But wait: the minimal k must be sufficient to represent the off-diagonals and the diagonal simultaneously. Since we can assign the diagonal arbitrarily per matrix, the diagonal does not impose an extra constraint? Actually, we can always set the diagonal in the same k matrices that we use for the off-diagonals. So the diagonal does not force k to be larger.

Therefore, the existence and minimal k is determined solely by the off-diagonals.

But wait: what if the diagonal cannot be decomposed? Actually, we can always decompose the diagonal: we can set in the first matrix the entire diagonal as A_{i,i} and then set the diagonals of the other matrices to 0? But then the off-diagonals might require more than one matrix. However, we are free to distribute the diagonal arbitrarily. For example, we can set:

  (X_1)_{i,i} = A_{i,i}, and (X_2)_{i,i} = 0, ... 

But also, we can distribute the diagonal arbitrarily over the k matrices. So the diagonal is always decomposable.

Therefore, we focus on the off-diagonals.

For each unordered pair (i, j) with i < j, we have a pair (A_{i,j}, A_{j,i}) = (a, b). We wish to express (a, b) as a sum of k vectors, each in the set V = { (x,y) : |x| = |y| }.

What are the constraints on (a,b) for the existence of a decomposition? And what is the minimal k for this pair?

Note: the decomposition for the entire matrix: the same k matrices must cover every pair. Therefore, we need a k that works for every pair. So k must be at least the maximum k_i over all pairs i, and also k must be chosen such that the entire set of pairs can be decomposed in k matrices. However, note: we can use the same matrix to assign values to multiple pairs? Yes. But the condition for each pair is independent. However, the same matrix must assign a value to every pair? 

Yes, each matrix X_s has an entry for every (i,j). Therefore, we must assign to every pair (i,j) (with i != j) in every matrix. But we can set the value for a pair (i,j) to 0 in a particular matrix? Yes.

Therefore, the minimal k must be at least the minimal k required for the worst-case pair (i.e., the pair that requires the most matrices). But is that sufficient? 

Actually, we can combine the decompositions of different pairs in the same matrix? How?

Consider: we have two pairs: (i,j) and (p,q). We wish to assign in one matrix X_s:

  For (i,j): we assign (x, y) such that |x| = |y|.
  For (p,q): we assign (z, w) such that |z| = |w|.

There is no constraint between (x,y) and (z,w). So we can assign each pair independently in the same matrix. Therefore, we can do:

  For each pair (i,j) and for each matrix s, we assign a value for that pair. The assignment for one pair does not affect the others.

Therefore, the entire decomposition is separable per pair? Actually, no: because the same matrix is used for all pairs. But we are free to assign each pair independently. Therefore, the minimal k for the entire matrix is the maximum k that is needed for any single pair (i,j). Why?

  Because for a fixed pair (i,j), we need at least k0 matrices to represent (A_{i,j}, A_{j,i}), and if one pair requires k0, then we must have k >= k0. Then, we can set k = max_{pair} (minimal_k for that pair). For the pairs that require less than k, we can use the remaining matrices to assign 0 for that pair? Actually, we can always assign 0 to a pair in a matrix. 

But wait: what if one pair requires k0 and we set k = k0. Then for every matrix we have an assignment for that pair. For the other pairs, we can use the same k matrices to assign their required values? 

However, we must assign the entire vector (a, b) for a pair as the sum of the k vectors we assign in the k matrices. Since the matrices are independent per pair, we can design the decomposition for each pair independently and then combine: for matrix s, set the (i,j) and (j,i) to the value that the decomposition of the pair (i,j) assigned to matrix s. Then, the diagonal we can set arbitrarily? Actually, we have to set the diagonal as well.

But note: we haven't considered the diagonal. However, we can set the diagonal arbitrarily. So we can set the diagonal for each matrix s so that the sum of the diagonals over s for row i is A_{i,i}. How? 

  We have k matrices. We need to assign (X_s)_{i,i} for each s and each i such that:
        sum_{s} (X_s)_{i,i} = A_{i,i}.

  This is a system of n equations. We can assign arbitrarily? For example, we can set:

        (X_1)_{i,i} = A_{i,i}
        (X_2)_{i,i} = 0
        ...
        (X_k)_{i,i} = 0

  But note: we are free to distribute arbitrarily. This is always possible.

Therefore, the problem reduces to: for each off-diagonal pair (i,j) (with i<j), we must find the minimal number of vectors in V (where V = { (x,y) : |x| = |y| }) that sum to (a, b) = (A_{i,j}, A_{j,i}). Then the overall k must be at least the maximum over all pairs of the minimal number of vectors for that pair. Moreover, we can achieve k = max_{i<j} k_{i,j} by using that many matrices for every pair (assigning the required vectors for each pair independently in the k matrices, and zeros for the pairs that don't require as many matrices) and then setting the diagonal appropriately.

But wait: is it possible that the decompositions for different pairs conflict? They do not: because we assign independently to each pair. 

So the minimal k for the entire matrix is:

      k = max_{1<=i<j<=n} ( minimal number of vectors in V that sum to (A_{i,j}, A_{j,i}) )

However, we must also check that for every pair (i,j) the minimal number of vectors is finite (i.e., the pair (a,b) is representable as a sum of vectors in V). If for any pair the decomposition does not exist, we output -1.

Therefore, we are left with the following subproblem for a fixed pair (a,b):

      Find the smallest integer t (t>=0) such that there exists nonnegative integers c_1, c_2, ..., c_t and sign vectors (d_i, e_i) in { (1,1), (1,-1), (-1,1), (-1,-1) } for i=1..t, satisfying:

          a = d_1 * c_1 + d_2 * c_2 + ... + d_t * c_t
          b = e_1 * c_1 + e_2 * c_2 + ... + e_t * c_t

But note: we can also have zero vectors? Actually, if t=0 then (a,b) must be (0,0). 

Alternatively, note that the set V is symmetric and closed under addition? Actually, no: the sum of two vectors in V is not necessarily in V. But we are taking linear combinations with nonnegative integer coefficients? Actually, we are taking a linear combination of vectors with coefficients 1? But each vector we choose is scaled by the nonnegative integer c_i? Actually, we are choosing a vector (d_i * c_i, e_i * c_i) for each term? So it's:

          (a, b) = sum_{i=1}^{t} (d_i, e_i) * c_i

where (d_i, e_i) is in { (1,1), (1,-1), (-1,1), (-1,-1) } and c_i is a nonnegative integer.

But note: we can combine the same vector type multiple times. Actually, we can think of the representation as:

          a = (c1 + c2) - (c3 + c4)   [where c1, c2, c3, c4 are nonnegative integers?]
          b = (c1 - c2) + (c3 - c4)   ??? 

Alternatively, we can assign:

          Let:
            Let u = number of vectors of type (1,1)
            v = number of vectors of type (1,-1)
            w = number of vectors of type (-1,1)
            x = number of vectors of type (-1,-1)

          But note: we can use the same vector multiple times? Actually, we are using t vectors, each of which is one of the four types, and we multiply by a nonnegative integer coefficient? Actually, no: the above representation is additive: each vector is (d_i, e_i) multiplied by a nonnegative integer c_i. But we can collapse: we can let:

            Let u = total from vectors of type (1,1): meaning we have a term u in the first coordinate and u in the second.
            v: from (1,-1): first coordinate +v, second -v.
            w: from (-1,1): first -w, second +w.
            x: from (-1,-1): first -x, second -x.

          Then:

            a = u + v - w - x
            b = u - v + w - x

          And u, v, w, x are nonnegative integers (since each c_i is nonnegative and we are summing over all vectors of the same type).

          The total number of vectors t = u + v + w + x.

          We wish to minimize t.

But note: the same vector type can be used multiple times? Actually, we are grouping by type. So we have:

          a = u + v - w - x
          b = u - v + w - x

          and we wish to minimize u+v+w+x.

This is a system of linear equations in nonnegative integers u, v, w, x.

We can solve for u, v, w, x:

          Let s = u - x. Then:

            a = (u - x) + (v - w)
            b = (u - x) - (v - w)

          Let d = v - w.

          Then:
            a = s + d
            b = s - d

          So:

            s = (a+b)/2
            d = (a-b)/2

          Therefore, we require:

            a+b must be even, and a-b must be even.

          Then we set:

            u - x = s = (a+b)/2
            v - w = d = (a-b)/2

          Now, we must express s and d in terms of nonnegative integers u, v, w, x such that:

            u, x >=0 and u - x = s
            v, w >=0 and v - w = d

          How? We can set:

            u = max(s,0) + t1, x = max(-s,0) + t1   for some t1>=0? Actually, we can set:

            We can set u = s + x, but then x must be chosen so that u>=0 -> s+x>=0 -> x>=-s. But x>=0, so if s>=0, we can set x=0 then u=s. If s<0, then we set u=0, then x = -s? But then we have u=0 and x = -s (which is positive). 

          Similarly for v and w: if d>=0, set v=d, w=0. If d<0, set v=0, w=-d.

          Then the minimal representation for the pair (a,b) would be:

            t = |s| + |d|? Because:

              u + x = |s|? 
                if s>=0: u=s, x=0 -> u+x = s.
                if s<0: u=0, x=-s -> u+x = -s = |s|.

              Similarly, v+w = |d|.

          So total t = |s| + |d| = |(a+b)/2| + |(a-b)/2|.

          But note: we must also have that s and d are integers, i.e., a+b and a-b are even.

          Therefore, the minimal t is:

            If a+b and a-b are even, then t = |(a+b)/2| + |(a-b)/2|.
            Otherwise, no solution.

But wait: what if we set u and x arbitrarily? Actually, the minimal representation is:

          u = max(s,0)   and x = max(-s,0)   -> then u+x = |s|
          v = max(d,0)   and w = max(-d,0)   -> then v+w = |d|

          and then t = |s|+|d|.

          But is this minimal? Yes, because:

            u + x >= |u-x| = |s|, so u+x>=|s|, and similarly v+w>=|d|. Therefore, the total t = u+v+w+x>= |s|+|d|.

          And we achieve |s|+|d| by the above.

Therefore, for a pair (a,b):

  Condition: 
        a+b must be even and a-b must be even -> equivalent to a and b must be both even or both odd? 

        Because: 
          (a+b) mod 2 = (a mod 2 + b mod 2) mod 2
          (a-b) mod 2 = (a mod 2 - b mod 2) mod 2 = (a mod 2 + b mod 2) mod 2? 
          Actually: (a-b) mod 2 = (a+b) mod 2? 

        Actually: 
          (a+b) mod 2 = (a-b) mod 2   because adding or subtracting modulo 2 is the same? 

        But: 
          (a+b) mod 2 = (a-b) mod 2  <=> 2b mod2 =0, which is always true? 

        Actually, no: 
          (a+b) mod2 = (a mod2 + b mod2) mod2
          (a-b) mod2 = (a mod2 - b mod2) mod2 = (a mod2 + (-b mod2)) mod2 = (a mod2 + (b mod2)) mod2? because -1 mod2=1.

        So (a-b) mod2 = (a+b) mod2.

        Therefore, the condition that both a+b and a-b are even is equivalent to (a+b) is even? Because if (a+b) is even, then (a-b)= (a+b)-2b is even too? 

        Actually: 
          a+b even -> a-b = (a+b) - 2b -> even minus even is even.

        So the condition is simply: a+b must be even.

        Then we have:

            s = (a+b)/2, d = (a-b)/2.

        Then minimal t = |s| + |d| = |(a+b)/2| + |(a-b)/2|.

But note: we can also use fractional assignments? But we are forced to nonnegative integers. However, we have an alternative representation: we are using u, v, w, x that are nonnegative integers. The minimal t is |s|+|d|, but we must also be able to represent s and d as:

          u - x = s, and u, x>=0 -> requires that u>=max(s,0) and x>=max(-s,0). The minimal u+x is |s|.

          Similarly, the minimal v+w is |d|.

        Therefore, the minimal t is |s|+|d|.

But note: we are counting the number of vectors. And each vector is one of the four types. However, in our grouping we have combined multiple vectors of the same type. The total number of vectors is u+v+w+x = |s|+|d|? 

        Actually: 
          u = max(s,0) and x = max(-s,0) -> then u+x = |s|.
          v = max(d,0) and w = max(-d,0) -> then v+w = |d|.

        So total vectors = |s| + |d|.

        However, note: we can also set:

          u = |s| + t1, x = t1? Then u-x = |s| (if s>=0) or u-x = -(|s|) if we set u=t1, x = |s|+t1? 

        Actually, to get u-x = s, we can set:

          u = (if s>=0) then s + t1, x = t1? Then u-x = s.
          or if s<0, then u = t1, x = -s + t1 -> then u-x = t1 - (-s+t1) = s.

        Then the total vectors for the (1,1) and (-1,-1) types is u+x = |s| + 2*t1.

        Similarly, for v and w: we can set v = |d| + t2, w = t2? Then v-w = |d|? But we need v-w = d.

        Actually, we need v-w = d. If d>=0, then set v = d + t2, w = t2 -> then v-w = d.
        If d<0, then set v = t2, w = -d + t2 -> then v-w = t2 - (-d+t2) = d.

        Then the total vectors for (1,-1) and (-1,1) types is v+w = |d| + 2*t2.

        Then the total vectors = |s| + |d| + 2*(t1+t2).

        Therefore, the minimal is when t1=t2=0: then total vectors = |s|+|d|.

        So the minimal t for the pair (a,b) is |s|+|d| = |(a+b)/2| + |(a-b)/2|.

        But note: |(a+b)/2| + |(a-b)/2| = (|a+b| + |a-b|)/2.

        And we know that |a+b| + |a-b| = 2 * max(|a|,|b|) when a and b have the same sign? Actually, no: 

          If a and b are both nonnegative: 
              |a+b| = a+b, |a-b| = |a-b| -> but if a>=b then |a-b| = a-b, so total = (a+b+a-b)/2 = (2a)/2 = a = max(|a|,|b|) if a>=b? Actually, we get a.

          But we have (|a+b|+|a-b|)/2 = max(|a|,|b|) if a and b are both nonnegative? 

          Actually, it is always max(|a|,|b|) when a and b are real? 

          Consider: 
            If a and b have opposite signs: 
                Let a=5, b=-3: 
                  |5-3| + |5+3| = |2|+|8| = 10 -> 10/2=5 = max(5,3).

            How about a=-5, b=3: same.

            How about a=-5, b=-3: 
                  |a+b| = 8, |a-b| = | -5 - (-3)| = | -2 | = 2 -> total 10, then 5 = max(5,3).

          Actually, we have the identity: 
              max(|a|, |b|) = (|a+b| + |a-b|)/2   ??? 

          But let a=1, b=1: 
              (|2|+|0|)/2 = (2+0)/2 = 1 = max(1,1).

          a=1, b=2: (|3|+| -1 |)/2 = (3+1)/2=2 = max(1,2).

          Therefore, the minimal t = (|a+b|+|a-b|)/2 = max(|a|, |b|).

          But wait: |(a+b)/2| + |(a-b)/2| = (|a+b|/2) + (|a-b|)/2 = (|a+b|+|a-b|)/2 = max(|a|,|b|).

          So the minimal number of vectors for the pair (a,b) is max(|a|,|b|) provided that a+b is even. If a+b is not even, then no solution.

But note: we have to assign the entire matrix. Therefore, for the entire matrix:

  Condition for existence: For every pair (i<j), we require that A_{i,j} + A_{j,i} is even.

  Then the minimal k for the entire matrix is:

        k = max_{i<j} ( max( |A_{i,j}|, |A_{j,i}| ) )   ??? 

  But wait: what if we have a pair (a,b) = (3,1): 
        Condition: 3+1=4 even -> ok.
        Then minimal t = max(3,1)=3.

  How do we represent (3,1) in 3 vectors? 

        We have: 
            s = (3+1)/2 = 2, d = (3-1)/2=1.

        Then u-x=2, v-w=1.

        Minimal representation: 
            u=2, x=0 -> u+x=2.
            v=1, w=0 -> v+w=1.
            total vectors=3.

        Then the vectors are:
            Two vectors of type (1,1): (1,1) and (1,1) -> contributes (2,2) to the sum? 
            One vector of type (1,-1): (1,-1) -> then total = (2+1, 2-1) = (3,1). 

        But wait: we have (2,2) from the two (1,1) and then (1,-1) -> total (3,1). 

        Alternatively, we could have used:
            (1,1): 1
            (1,1): 1
            (1,-1): 1
            -> then (1+1+1, 1+1-1) = (3,1).

        So that works.

  However, note: we might be able to use negative vectors? But we have included negative vectors in the types.

  Therefore, the minimal k for the entire matrix is:

        k0 = max( {0} ∪ { max(|A_{i,j}|, |A_{j,i}|) for all i<j } )

  Why {0}? Because if the entire matrix is zero, then k0=0? But then we would output 0? However, the problem: we need to decompose the matrix. But if the matrix is zero, then we can use k=0? But the problem says: "decompose into a sum of k matrices". If k=0, then the sum is the zero matrix? That matches.

  However, note the sample: 
        Sample Input #1: 3x3 zero matrix -> output k=1 and then the zero matrix? Why not k=0? 

  Let me read the problem: "SoCCat wants to decompose this matrix A into a sum of matrices X1 + X2 + ... + Xk". If k=0, then the sum is the zero matrix. But the problem says "all matrices X_i consist of integers and are absolutely symmetric". The zero matrix is absolutely symmetric. 

  However, the sample output is 1 and then the zero matrix. Why?

  The problem says: "minimize the number k". The minimal k is 0? But then why output 1?

  Actually, the problem states: "if k<=50, you should output the k matrices". For k=0, we would output nothing? The problem does not specify. But note: the problem says "the first line of output should contain one integer k". So if k=0, we output 0 and then nothing.

  However, the sample output is:

        1
        0 0 0
        0 0 0
        0 0 0

  Why not 0? 

  Let me check the problem statement: SoCCat has a matrix A. We are to decompose A into a sum of k matrices. If A is zero, then k=0 is valid: the empty sum is the zero matrix. However, the problem might require at least one matrix? Or is 0 acceptable?

  But note: the problem does not say "nonempty decomposition". The minimal k is 0.

  However, the sample output is 1. So what is going on?

  Let me reexamine: the condition for the diagonal. We said the diagonal is independent. But for the diagonal, we can set k=0: then the diagonal must be the empty sum -> 0. That matches. 

  But for the off-diagonals: in the zero matrix, every pair (i,j) has (0,0). Then the minimal k for each pair is max(0,0)=0. Then k0=0.

  Therefore, k=0 is acceptable.

  However, the problem sample output is 1. This suggests that the minimal k might be 1? Why?

  But note: the problem says "if there are multiple possible decompositions, SoCCat will be satisfied by any". So if we output k=0, that is minimal. But the problem says "minimize the number k".

  Why does the sample output k=1? 

  I see: the problem does not require the minimal k to be 0? Actually, 0 is minimal. But the problem also says: "if k<=50, output the k matrices". For k=0, we output:

        0

  and then stop. But the sample output has 1 and then a zero matrix.

  Therefore, the sample output is not minimal? 

  But wait: the problem says "minimize k". So we must output the minimal k. The minimal k for the zero matrix is 0.

  However, the problem sample output is:

        1
        0 0 0
        0 0 0
        0 0 0

  This suggests that the problem might not allow k=0? Or perhaps the problem requires at least one matrix? 

  Let me read the problem statement again: "SoCCat wants to decompose this matrix A into a sum of matrices X1 + X2 + ... + Xk". The word "sum" might imply at least one? But mathematically the empty sum is zero.

  However, the problem does not explicitly forbid k=0.

  But note: the problem says "if there are multiple possible decompositions, SoCCat will be satisfied by any". So k=0 is valid and minimal.

  However, the problem sample output is 1. 

  Therefore, we must reconsider: 

      We have a pair (0,0): minimal k0=0. 
      Then k = max_{i<j} k0 = 0? 

  But then we output 0. 

  However, what about the diagonal? The diagonal is also zero. So we output k=0.

  But the problem sample output is 1. 

  Alternatively, note that the problem says: "if k<=50, you should output the k matrices". For k=0, we output only the integer 0. 

  But the sample output has:

        1
        0 0 0
        0 0 0
        0 0 0

  So the sample expects k=1.

  Why might k=0 not be acceptable? 

  Consider: the decomposition must satisfy that each X_i is absolutely symmetric. The zero matrix is absolutely symmetric. And the empty sum is the zero matrix. So k=0 is acceptable.

  However, the problem might be allowing k=0? But the sample output is 1. 

  This discrepancy suggests that we must check the problem constraints: 

      The first sample input is:

          3
          0 0 0
          0 0 0
          0 0 0

      The sample output is:

          1
          0 0 0
          0 0 0
          0 0 0

  Therefore, the problem does not consider k=0 as acceptable? Or is the minimal k defined to be at least 1? 

  But note: the problem does not say that the decomposition must be nonempty. 

  However, there is a nuance: the diagonal. We said we can set the diagonal arbitrarily. But if k=0, then the diagonal must be zero. In the zero matrix, the diagonal is zero. So it works.

  What about the problem statement: "SoCCat wants to decompose this matrix A into a sum of matrices ...". The empty decomposition is a valid decomposition.

  But the sample output is 1. 

  Therefore, we must look at the problem again: it says "if there are multiple possible decompositions, SoCCat will be satisfied by any". So k=0 is valid and minimal. But why output 1?

  Alternatively, the problem might have an additional constraint: the decomposition must be nonempty? 

  Actually, the problem does not specify. But note: the problem says "SoCCat will also want to find out if no such decomposition exists." for the zero matrix, k=0 is a decomposition.

  However, the problem sample output is 1. 

  This leads me to suspect that the minimal k might be defined differently? 

  We said: k = max_{i<j} (minimal t for the pair (i,j)). For the zero matrix, that max is 0. But then we set k=0. 

  However, we also have the diagonal. The diagonal is independent, but we can set it in the same k=0 matrices. 

  But what if the diagonal is nonzero? Then we need at least one matrix? 

  Actually, we can set the diagonal arbitrarily in the matrices we are using for the off-diagonals. But if we are using k=0 matrices for the off-diagonals, then we must also set the diagonal with k=0 matrices. Therefore, if the diagonal is nonzero, then we must have at least one matrix? 

  However, we said: for the diagonal, we can distribute arbitrarily. But if k=0, then the diagonal must be zero. So if the diagonal is nonzero, then k cannot be 0.

  Therefore, the minimal k is:

        k = max( 
               (minimal k required by the off-diagonals: that is, max_{i<j} min_t for (i,j)),
               (minimal k required by the diagonal: which is 0 if the diagonal is all zero, and 1 if at least one diagonal entry is nonzero? But wait: we can set a nonzero diagonal in one matrix: set that matrix to have the diagonal and zeros elsewhere. But then that matrix must be absolutely symmetric: which it is because off-diagonals are zero -> absolute symmetry holds. So the diagonal being nonzero requires at least one matrix? 

          Actually, no: we can set the diagonal in the same matrices that we use for the off-diagonals. But if the off-diagonals require k0 matrices, then we can set the diagonal in those k0 matrices arbitrarily. So the diagonal does not force k to be at least 1 if it is nonzero? 

          How? 

          Suppose we have k0 matrices for the off-diagonals (which we set to zero for the off-diagonals if k0=0). Then we have to set the diagonal as the sum of the diagonals of these k0 matrices. But if k0=0, then the diagonal must be zero. 

          Therefore, if the diagonal is nonzero, we must have at least one matrix. 

          So the minimal k is:

            k = max( k0, (1 if the diagonal is nonzero) ? )

          But note: k0 is the max over the off-diagonals, and if the diagonal is nonzero then we require k>=1. However, if k0>=1, then we are already satisfying the diagonal. If k0=0 and the diagonal is nonzero, then we need k=1.

          Therefore, k = max( k0, (1 if the diagonal is nonzero) )? 

          But if k0=0 and the diagonal is zero, then k=0.

  So for the zero matrix: k0=0 and the diagonal is zero -> k=0.

  But the sample output is k=1. 

  This suggests that the problem does not allow k=0? 

  Alternatively, the problem might have an additional constraint: the decomposition must have at least one matrix? 

  The problem statement does not say. 

  However, looking at the sample output, we must output 1. 

  And the problem says: "if k<=50, you should output the k matrices". For k=0, we would not output the matrices. But the problem sample output does output a matrix.

  Therefore, we suspect that the minimal k might be defined as:

        k = max(1, max_{i<j} min_t for (i,j), (minimal for diagonal))

  But wait: the minimal for the diagonal is 0 if the diagonal is zero, and if the diagonal is nonzero then we require at least one matrix. But we can set the diagonal in one matrix: 

        We can use one matrix: set the diagonal to A, and the off-diagonals to zero. This matrix is absolutely symmetric? 
          For i=j: no constraint. 
          For i!=j: we set (i,j)=0 and (j,i)=0 -> |0|=|0| -> satisfied.

        So if the off-diagonals require 0 matrices and the diagonal is nonzero, we can use one matrix.

        Therefore, the minimal k is:

          k = max( k0, (1 if the diagonal is nonzero) )   [if k0=0 and diagonal nonzero, then k=1]

          But if k0=0 and diagonal zero, then k=0.

  However, the sample input has a zero matrix. Then k=0.

  But the sample output is 1. 

  Therefore, we must reconsider the off-diagonals: even if the off-diagonals are zero, we might use one matrix? But that is not minimal.

  Alternatively, note that the problem does not require the matrices to be nonzero. But k=0 is minimal.

  I see the problem: the problem states that the zero matrix can be decomposed as the sum of one zero matrix. That is also valid. But k=0 is smaller.

  However, the problem says "minimize k". So k=0 is better.

  Therefore, the sample output is not minimal? 

  But the problem sample output is provided by the problem. So we must output the same.

  After reexamination: the problem says "SoCCat wants to decompose this matrix A into a sum of matrices". It might be that SoCCat requires at least one matrix? 

  Or, note the constraints: n>=1. The matrix has at least one entry. 

  But the problem does not specify.

  Given the sample output, we must output at least one matrix? 

  Actually, the problem does not explicitly say that k=0 is allowed. In many decomposition problems, the empty decomposition is not considered. 

  Therefore, we will assume that k=0 is allowed only if A is the zero matrix? But then we output 0. 

  However, the sample output for the zero matrix is 1. 

  Alternatively, note that the problem says: "if k<=50, output the k matrices". For k=0, we output nothing after the 0. The problem does not say that we must output the matrices for k=0. So it is acceptable.

  But the sample output for the zero matrix is 1 and then the matrix. 

  I think there might be a mistake in the problem sample. 

  However, let me check the second sample:

        Input: 
            2
            0 1
            0 0

        Output: -1.

        Condition for the pair (1,2): (0,1) -> a=0, b=1? 
            a+b = 1 -> odd -> no solution.

        So we output -1.

  Third sample:

        2
        1 3
        5 7

        The matrix:
            Row0: 1, 3
            Row1: 5, 7

        Pairs: 
            (0,1): (A0,1=3, A1,0=5) -> (3,5)
            Check: 3+5=8 even -> ok.
            Then minimal t = max(|3|,|5|)=5.

        But the output says k=2.

        And then outputs two matrices.

        Therefore, our reasoning for the minimal k per pair is not the entire story? 

  I see the flaw: we assumed that the same set of k matrices can be used for every pair independently. But note: we are constrained by the fact that each matrix must be absolutely symmetric. In particular, for a fixed matrix X_s, we must assign to every pair (i,j) a pair (x_{i,j}, x_{j,i}) such that |x_{i,j}|=|x_{j,i}|. 

  However, we are also allowed to distribute arbitrarily over the matrices. But we also have to consider that the entire matrix X_s must be absolutely symmetric. We are free to assign each pair independently. 

  But the minimal k for the entire matrix is the maximum over the pairs of the minimal number of vectors for that pair. Why is the sample third input having k=2? 

  Let me compute for the pair (3,5): 
        s = (3+5)/2 = 4, d = (3-5)/2 = -1.
        Then minimal t = |4|+| -1| = 5.

  Then the overall k should be at least 5? 

  But the sample output is 2.

  This indicates that our per-p独立分解 is not the whole story. 

  Why? Because we can use the same vector to contribute to multiple pairs. 

  We had assumed that the matrices we use can assign each pair independently. This is true. But the minimal k for the entire matrix is not the max over the pairs of the minimal number of vectors for that pair, because one matrix can cover many pairs simultaneously. 

  We cannot decompose each pair independently and then combine arbitrarily because the same matrix is used for all pairs. 

  We must find a set of k matrices that are absolutely symmetric and whose sum is A. 

  This is a global decomposition.

  Therefore, we must find the minimal k such that there exist k absolutely symmetric matrices summing to A.

  How to do that?

  Let us denote the unknown matrices by X_1, X_2, ..., X_k.

  The condition for each matrix X_s: 
        (X_s)_{i,j} = (X_s)_{j,i} OR (X_s)_{i,j} = - (X_s)_{j,i}   is not linear. 

  But note: the condition |a|=|b| is equivalent to (a = b) or (a = -b). This is a union of two linear subspaces. 

  Alternatively, we can write the condition as: 
        (X_s)_{i,j} = c_{s, i,j} * d_{s, i}
        (X_s)_{j,i} = c_{s, i,j} * d_{s, j}

  where d_{s, i} in {-1, 1} for each node i and each matrix s? 

  But note: consider three indices i,j,k. Then for the matrix X_s, we have:
        (X_s)_{i,j} = c_{s, i,j} * d_{s, i} * d_{s, j}   ??? 

  This is because:
        (X_s)_{i,j} = d_{s, i} * (something) and (X_s)_{j,i} = d_{s, j} * (something), and then | (X_s)_{i,j} | = | (X_s)_{j,i} | implies that the something can be chosen appropriately.

  Actually, we can let:

        (X_s)_{i,j} = d_{s, i} * d_{s, j} * Y_{s, i,j}

  then (X_s)_{j,i} = d_{s, j} * d_{s, i} * Y_{s, i,j} = (X_s)_{i,j} 
        -> then the matrix X_s is then not just absolutely symmetric but in fact symmetric? 

  But note: the condition |a|=|b| is weaker than a=±b. In this representation, we have (X_s)_{i,j} = (X_s)_{j,i} if we use the same signs. To get the possibility of negative signs between i and j, we would need to allow Y_{s, i,j} to be negative? 

  Alternatively, we can let:

        (X_s)_{i,j} = d_{s, i} * d_{s, j} * Y_{s, i,j}

  then (X_s)_{j,i} = d_{s, j} * d_{s, i} * Y_{s, i,j} = (X_s)_{i,j} 

  so it is always symmetric. 

  Therefore, this representation only captures the symmetric matrices, not the absolutely symmetric matrices that might have (i,j) = - (j,i). 

  How to capture the antisymmetric part? 

  Note: the condition |a|=|b| for off-diagonals (i!=j) allows two possibilities: 
        a = b  -> symmetric
        a = -b -> antisymmetric

  So we can view the matrix as a combination of a symmetric matrix and an antisymmetric matrix? 

  But wait: in one matrix, we can have some pairs handled by symmetric assignment and some by antisymmetric? 

  For example, for a fixed matrix X_s, we can partition the off-diagonal pairs into two sets: 
        Set E: the pairs that will be even (symmetric: a=b)
        Set O: the pairs that will be odd (antisymmetric: a=-b)

  Then the matrix is not globally symmetric or antisymmetric, but locally per pair. 

  Therefore, the representation is not in terms of a signing of the indices. 

  This complexity suggests a different approach. 

  Let me try to write the condition in terms of linear algebra over the integers. 

  We have to decompose A into k matrices X_s, each of which satisfies for every i,j: | (X_s)_{i,j} | = | (X_s)_{j,i} |.

  This is a non-convex condition. 

  However, note the following: 

        For the diagonal: no condition.
        For off-diagonals: the only constraint is that the 2x2 submatrix for (i,j) and (j,i) has the form:

              [ a   b ]
              [ c   d ]

        with |a|=|d| and |b|=|c|. But wait, no: the condition is only between (i,j) and (j,i): 
              |a| = |c| and |b| = |d|? 

        Actually, the condition is: 
              | (X_s)_{i,j} | = | (X_s)_{j,i} |   for every i,j.

        This means that for the 2x2 submatrix, we have:
              |a| = |c| 
              |b| = |d|

        and for the diagonal entries (i,i) and (j,j), no condition. 

        But note: the condition must hold for every pair, including (i,i) (which is trivial) and for (i,j) and (j,i) as well as (i,i) and (i,j) (which is not a condition because the condition is for (i,j) and (j,i) only).

        Therefore, the constraints are pairwise. 

  Given the complexity, and the small n (<=50), and that the minimal k might be small (because in the sample it is 2 for a 2x2 matrix), we may try to find a decomposition with k as small as possible, but note the matrix entries can be as large as 10^9, and k might be up to ?? 

  In the per-pair analysis, we had k up to 10^9, which is too large. 

  Therefore, we must find a more efficient representation. 

  Let me think of the entire matrix. 

  We can split the matrix into two parts: 
        Let B be the matrix where for i>=j, B_{i,j} = A_{i,j}, and for i<j, we set B_{i,j} = sign(i,j) * |A_{j,i}| ? 
  This is not helpful. 

  Alternatively, note: the condition for a matrix X to be absolutely symmetric is that the matrix X is a signed version of a symmetric matrix in the following sense: 

        There exists a symmetric matrix Y (Y_{i,j}=Y_{j,i}) and a signing matrix D = diag(d_1, d_2, ..., d_n) with d_i in { -1, 1 } such that 
              X = D * Y * D.

        Then for off-diagonal: 
              X_{i,j} = d_i * Y_{i,j} * d_j.
              X_{j,i} = d_j * Y_{j,i} * d_i = d_j * Y_{i,j} * d_i = X_{i,j} 
          -> wait, this is always symmetric. 

  So this only captures the case where X is symmetric. 

  To capture the case where we might have antisymmetric off-diagonals, we would need to allow Y to be not necessarily symmetric? 

  How about: 
        X = D * Y * D, but Y is an arbitrary matrix? 
        Then X_{i,j} = d_i * Y_{i,j} * d_j.
        condition: |X_{i,j}| = |X_{j,i}| -> |d_i * Y_{i,j} * d_j| = |d_j * Y_{j,i} * d_i| -> |Y_{i,j}| = |Y_{j,i}|.

        So this is exactly the condition. 

  Therefore, we can write: 
        X = D * Y * D, 
        where D is a diagonal matrix with +/-1 on the diagonal, and Y is any matrix such that |Y_{i,j}| = |Y_{j,i}| for all i,j. 

  But then we can further decompose Y? 

  Note: the condition on Y is that it is absolutely symmetric in the same way. So we haven't gained much. 

  Alternatively, we can view Y as a matrix that is symmetric in the absolute values. 

  Then the decomposition of A is:

        A = sum_{s=1}^{k} D_s * Y_s * D_s

  where D_s is a diagonal matrix of signs for matrix s, and Y_s is a matrix that is absolutely symmetric.

  but wait, then we haven't reduced the problem. 

  Given the complexity and the small n (<=50) but large matrix entries, and the fact that the sample k is only 2 for a 2x2 matrix, we may try to find a decomposition with k=2 or k=1 or k=0. 

  Let me try for the sample: 
        A = [[1,3],
             [5,7]]

        We want to decompose into two absolutely symmetric matrices.

        Sample output: 
            X1 = [[1, -1],
                   [1, -1]]
            X2 = [[0, 4],
                   [4, 8]]

        Check: 
            X1 is absolutely symmetric: 
                |1|=|1|, 
                |-1| = |1| = 1, 
                |1| = |-1| = 1, 
                |-1| = |-1| -> for the diagonal it's 1 and -1: for (1,1): |1|=|1| -> true, for (2,2): |-1|=|-1| -> true.
                But wait, the matrix X1 is:
                      [1, -1]
                      [1, -1]

                For (1,2): | -1 | = | 1 | -> 1=1, true.
                For (2,1): | 1 | = | -1 | -> 1=1, true.

            X2 = [[0,4],
                   [4,8]]
                This is symmetric, so |0|=|0|, |4|=|4|, |4|=|4|, |8|=|8|.

            Sum: 
                [1+0, -1+4] = [1,3]
                [1+4, -1+8] = [5,7] -> matches.

        How to come up with this? 

        We want to minimize k. 

        Try k=1: 
            We would need an absolutely symmetric matrix X such that X = A.
            For (1,2) and (2,1): we require |3| = |5| -> 3!=5 -> cannot be.

        Therefore, k=1 is not possible.

        Then try k=2.

        How to decompose? 

        We can try to set:

            X1 = [[ a, c ],
                   [ d, e ]]

            X2 = [[ g, i ],
                   [ j, h ]]

            Conditions: 
                a+g = 1
                c+i = 3
                d+j = 5
                e+h = 7

                For X1: |a|=|a| (trivial), |c|=|d|, |d|=|c| (redundant), |e|=|e|.
                       -> |c| = |d|.
                For X2: |i| = |j|.

        We have freedom. 

        The sample solution sets:
            a=1, e=-1 -> then from a+g=1, g=0.
            e=-1, and e+h=7 -> h=8.
            then for X1: 
                 c and d must satisfy |c|=|d|.
            and for X2: 
                 i and j must satisfy |i|=|j|, and also c+i=3, d+j=5.

        In the sample: 
            X1: c=-1, d=1 -> then |c|=1=|d|, 
            then for X2: i = 3 - c = 3 - (-1) = 4, j = 5 - d = 5 - 1 = 4, and |4|=|4|.

        and then X2 = [[0,4],[4,8]].

        So it is a valid solution.

        Therefore, for n=2, we can try to solve with small k. 

        In general, we may try to use k=0,1,2. 

        But what is the minimal k? 

        From the condition: for a matrix to be absolutely symmetric, the off-diagonal pairs (i,j) and (j,i) must have absolute values equal. 

        For the entire matrix A, consider the off-diagonal pairs. For a given pair (i,j), the values A_{i,j} and A_{j,i} must be decom into k parts. 

        In one matrix, the contribution to (i,j) and (j,i) must have the same absolute value. 

        Therefore, the difference between A_{i,j} and A_{j,i} in each matrix must be such that the sum over matrices of the difference in signs might help. 

        We can view the entire decomposition as:

              A = X_1 + X_2 + ... + X_k

        For off-diagonal (i,j) and (j,i):

              A_{i,j} = x_1 + x_2 + ... + x_k
              A_{j,i} = y_1 + y_2 + ... + y_k

              with |x_s| = |y_s| for each s.

        This is the same as the per-pair condition we had. 

        then the condition for the pair (i,j) is: 
              A_{i,j} + A_{j,i} must be even (because each (x_s+y_s) is even? because x_s and y_s are either both even or both odd? 

        Let me check: 
              If x_s = 
        we had: x_s and y_s are from a set that has: 
              (c, c): then x_s+y_s = 2c -> even.
              (c, -c): then x_s+y_s = 0 -> even.
              (-c, c): even.
              (-c, -c): even.

        Therefore, for every matrix, x_s+y_s is even. Therefore, the sum (x_1+...+x_k) + (y_1+...+y_k) = A_{i,j}+A_{j,i} is even. 

        So necessary condition: for every i, j, A_{i,j}+A_{j,i} must be even. 

        For the sample third: 
              A_{0,1}=3, A_{1,0}=5 -> 3+5=8 even.
              diagonal: 1+7=8 even.

        For the sample second: 
              A_{0,1}=1, A_{1,0}=0 -> 1+0=1, which is not even -> no decomposition.

        Therefore, the condition we identified is necessary. 

        Now, is it sufficient? 

        We also need to 
 
 We also need to assign the diagonal. 

        For the diagonal, we have no such condition. 

        Therefore, the necessary and sufficient condition for the off-diagonals is: for every i, j, A_{i,j}+A_{j,i} is even.

        Then, what is the minimal k? 

        In the per-pair condition, we (incorrectly) thought that the minimal k for the entire matrix is the maximum over i<j of (|A_{i,j}|, |A_{j,i}|) (// not exactly, but a function of that) -> 
            minimal vectors for the pair (i,j) = (|A_{i,j}+A_{j, i}| / 2) + (|A_{i,j}-A_{j,i}| / 2) = max(|A_{i,j}|, |A_{j,i}|).

        But then for the sample third: pair (0,1): 
              max(|3|,|5|)=5, so we would need 5 matrices. 

        But we found a decomposition in 2 matrices. 

        Therefore, the per-pair bound is a lower bound? 

        Because we must have at least max(|A_{i,j}|,|A_{j,i}|) for the pair (i,j), but we may cover multiple pairs in the same matrix. 

        In fact, the matrices we use can cover all pairs simultaneously. 

        Therefore, the minimal k is the minimum number such that there exists a decomposition for every pair (i,j) into k terms, and these decompositions for different pairs are compatible in the sense that they come from the same set of matrices. 

        How to achieve that? 

        We can use at most two matrices! 

        Claim: k=2 is sufficient for any matrix A that has the property that for every i,j, A_{i,j}+A_{j,i} is even. 

        And for the zero matrix, we use k=0. 

        For a matrix with at least one nonzero entry, we can use at most 2. 

        How to see that? 

        Let me define two matrices:

          X = (x_{i,j})
          Y = (y_{i,j})

          such that X+Y = A, and both X and Y are absolutely symmetric.

        We and also we can set the diagonal arbitrarily. 

        For off-diagonal (i,j) and (j,i):

          We have:
                x_{i,j} + y_{i,j} = A_{i,j}
                x_{j,i} + y_{j,i} = A_{j,i}

          and |x_{i,j}| = |x_{j,i}|, |y_{i,j}| = |y_{j,i}|.

        We can try to set:

                x_{i,j} = a, x_{j,i} = either a or -a.
                y_{i,j} = A_{i,j} - a, y_{j,i} = either A_{j,i} - a or -(A_{j,i} - a) but with the condition |y_{i,j}| = |y_{j,i}|.

        This is per-pair and we have to choose the signs. 

        Alternatively, we can try a global signing. 

        We can put the entire matrix A into two matrices by: 

          X = (A + B) / 2
          Y = (A - B) / 2

        where B is a matrix such that B_{i,j} = B_{j,i} (symmetric) and also B_{i,j} = - B_{j,i} (antisymmetric) -> then B=0. 

        This is not helpful. 

        Another idea: 

          Let D be a diagonal matrix of signs, to be chosen. 

          We try to set:
                X = D * C * D
                Y = A - X

          and we want Y to be absolutely symmetric. 

          This is not obviously linear. 

        Given the small n (<=50) and the sample that k=2 works for the third sample, and also for the first sample (k=1) and the second sample (k=0) is not allowed because condition fails, we can try to 
        output k=0 if the matrix is zero, 
        otherwise k=2 (or sometimes k=1) if the matrix is not zero.

        When can we use k=1? 
                We need A to be absolutely symmetric. 
                That is, for every i,j, |A_{i,j}| = |A_{j,i}|.

        For example, if A is symmetric, then it is absolutely symmetric. 
        If A is antisymmetric, then also (because |a|=|-a|).

        Therefore, if for every i,j, |A_{i,j}| = |A_{j,i}|, then we can use k=1.

        Otherwise, we use k=2.

        But is k=2 always sufficient if the condition (every off-diagonal pair has even sum) holds? 

        Let me try to construct for a general matrix A with the even-sum condition for every off-diagonal pair.

        And also, the diagonal can be anything. 

        We will construct two absolutely symmetric matrices X and Y such that X+Y = A.

        For each off-diagonal pair (i,j) and (j,i), we have:

              A_{i,j} = x_{i,j} + y_{i,j}
              A_{j,i} = x_{j,i} + y_{j,i}

        and we require |x_{i,j}| = |x_{j,i}| and |y_{i,j}| = |y_{j,i}|.

        We can try: 
              x_{i,j} = (A_{i,j} - A_{j,i}) / 2
              x_{j,i} = - (A_{i,j} - A_{j,i}) / 2   [ then |x_{i,j}| = | (A_{i,j}-A_{j,i})/2 | = |x_{j,i}| ]

        then 
              y_{i,j} = A_{i,j} - x_{i,j} = (A_{i,j} + A_{j,i}) / 2
              y_{j,i} = A_{j,i} - x_{j,i} = A_{j,i} + (A_{i,j}-A_{j,i})/2 = (A_{i,j}+A_{j,i})/2 = y_{i,j} -> so Y will be symmetric.

        Then for matrix X: for the pair (i,j) and (j,i), we have:
              x_{i,j} = (A_{i,j}-A_{j,i})/2
              x_{j,i} = - (A_{i,j}-A_{j,i})/2 = - x_{i,j} -> then |x_{i,j}| = |x_{j,i}|.

        For matrix Y: as above, it is not only absolutely symmetric but in fact symmetric.

        Therefore, both X and Y are absolutely symmetric.

        And for the diagonal: we can set arbitrarily. 

        How to set the diagonal in the two matrices? 
              We require: 
                  for each i, 
                      x_{i,i} + y_{i,i} = A_{i,i}

              We can set, for example, 
                  x_{i,i} = 0, 
                  y_{i,i} = A_{i,i} 
              or distribute arbitrarily.

        Therefore, the decomposition is:

            X: 
                  for i!=j: 
                         X_{i,j} = (A_{i,j} - A_{j,i}) / 2
                         X_{j,i} = - (A_{i,j} - A_{j,i}) / 2   [ note: = (A_{j,i} - A_{i,j})/2 ]
                  for i=j: we can set X_{i,i}=0.

            Y: 
                  for i!=j: 
                         Y_{i,j} = (A_{i,j} + A_{j,i}) / 2 = Y_{j,i}   (symmetric)
                  for i=j: Y_{i,i} = A_{i,i} 

        But note: the condition for the off-diagonal only works if (A_{i,j}-A_{j,i}) is even? -> the even-sum condition is given, so (A_{i,j}+A_{j,i}) is even, then (A_{i,j}-A_{j,i}) = (A_{i,j}+A_{j,i}) - 2A_{j,i} is even. Therefore, the division by 2 is integer.

        Therefore, this decomposition in two matrices always works for any matrix A that has the even-sum condition for every off-diagonal pair.

        What if the matrix is absolutely symmetric? 
            then for every i,j: |A_{i,j}| = |A_{j,i}|.
            but note: our decomposition might not yield X=0. 
            For example, if A is symmetric, then A_{i,j}=A_{j,i}:
                   X_{i,j} = (A_{i,j}-A_{j,i})/2 =0.
                   Y_{i,j} = (A_{i,j}+A_{j,i})/2 = A_{i,j}.
                   then we have Y = A, and X=0.
            But the zero matrix is absolutely symmetric. 
            However, we have two matrices: but the problem says to minimize k. 
            If A is absolutely symmetric, then we can use one matrix: 
                  X1 = A.
            and we don't need two.

        Therefore, we can do:

            if the matrix A is the zero matrix, output k=0.
            else if for every off-diagonal pair (i,j), |A_{i,j}| = |A_{j,i}|, then output k=1 and then the matrix A.
            else, output k=2 and then the two matrices as above.

        But wait: the even-sum condition is necessary for the off-diagonal pairs. We already require that for every off-diagonal pair, A_{i,j}+A_{j,i} is even. This is the condition for the existence of the decomposition. 

        Therefore, if we find an off-diagonal pair with A_{i,j}+A_{j,i} not even, we output -1.

        Summary of the solution:

          Step 1: For every pair (i,j) (i<j), check that (A_{i,j} + A_{j,i}) is even.
                   If not, output -1.

          Step 2: For the diagonal, we have no conditions.

          Step 3: 
                  if the matrix A is the zero matrix, then output k=0.
                  else, if for every pair (i,j) (i!=j), we have A_{i,j} = A_{j,i} or A_{i,j} = -A_{j,i} (within absolute value condition, which is equivalent to |A_{i,j}| = |A_{j, i}|), then output k=1 and output the matrix A.

                  else, output k=2 and output the two matrices:
                         Matrix X: 
                                for i<j: 
                                      X_{i,j} = (A_{i,j} - A_{j,i}) / 2
                                      X_{j,i} = (A_{j,i} - A_{i,j}) / 2   [ note: this = - (A_{i,j}-A_{j,i})/2 ]
                                for i=j: 
                                      X_{i,i} = 0.

                         Matrix Y:
                                for i<j:
                                      Y_{i,j} = (A_{i,j} + A_{j,i}) / 2
                                      Y_{j,i} = (A_{i,j} + A_{j,i}) / 2   [ symmetric ]
                                for i=j:
                                      Y_{i,i} = A_{i,i}

          However, note: the diagonal for matrix X is set to 0, and for matrix Y is set to the whole diagonal of A.

          But we could distribute the diagonal arbitrarily as long as the sum is A_{i,i}. 
          For example, we could set:
                  X_{i,i} = a_i, 
                  Y_{i,i} = A_{i,i} - a_i.
          for any a_i. 
          To be simple, we set a_i=0.

        Let me test with the sample third: 
              A = [[1,3],
                     [5,7]]

              even-sum for off-diagonal: 3+5=8 even.

              Check if A is absolutely symmetric: 
                    |3| = 3, |5|=5, not equal. -> not absolutely symmetric.

              Then output k=2.

              Matrix X:
                    off-diagonal: 
                          (0,1): X_{0,1} = (3-5)/2 = -1, 
                                  X_{1,0} = (5-3)/2 = 1.
                          diagonal: 
                                  X_{0,0}=0, X_{1,1}=0.
                    so X = [[0, -1],[1,0]] -> wait, but the sample output for the first matrix is [[1,-1],[1,-1]].

              Matrix Y:
                    off-diagonal: 
                          (0,1): Y_{0,1} = (3+5)/2 = 4, 
                                  Y_{1,0} = 4.
                    diagonal: 
                          Y_{0,0}=1, Y_{1,1}=7.
                    so Y = [[1,4],[4,7]]

              Sum: 
                    [[0+1, -1+4] = [1,3],
                     [1+4, 0+7] = [5,7]] -> matches.

              But the sample output for the first matrix is [[1,-1],[1,-1]] and the second is [[0,4],[4,8]].
              Note: there are many decompositions.

          Our decomposition is valid and absolutely symmetric for X and Y.

          However, the sample output has a different decomposition. 

          The problem says: "if there are multiple, any is acceptable".

        Therefore, we output this.

        But wait: the sample output for the second matrix has diagonal [0,8] -> then the first matrix has diagonal [1,-1] and the second [0,8] -> sum for diagonal: 1+0=1, -1+8=7 -> matches.

        So there are decompositions.

        Why in the sample output the first matrix has diagonal [1,-1]? 
        Because they distributed the diagonal arbitrarily. 

        In our decomposition, we set the entire diagonal to Y. We could distribute otherwise. 

        To exactly match the sample output, we might distribute the diagonal as:

            for matrix X: set the diagonal to [1, -1] (arbitrary) and for matrix Y: set the diagonal to [0,8] (so that 1+0=1, -1+8=7).

        then the matrix X becomes:
              [[1, -1],
               [1, -1]]

          -> then we require that X is absolutely symmetric. 
                 |1|=|1|, 
                 |-1| and |1|: for (0,1): | -1 | = 1, for (1,0): |1| = 1 -> equal.
                 for (0,0): |1|=|1|, for (1,1): |-1|=|-1|.

          so it is valid.

        Therefore, we have freedom in the diagonal.

        However, the off-diagonal assignment in the sample output for the first matrix is not the same as ours. 

        But the problem does not require a specific decomposition.

        Therefore, we can choose any distribution for the diagonal as long as the sums are A_{i,i}. 

        To and also for the off-diagonal, our assignment is fixed by the formulas.

        But the sample output off-diagonal for the first matrix is: 
              (0,1): -1, (1,0):1 -> in our method we have (0,1):-1, (1,0):1 -> the same.

        then why the sample output for the first matrix has (1,0)=1 and also (0,0)=1, (1,1)=-1. 

        In our decomposition, matrix X is:

              [[0, -1],
               [1, 0]]

        but the sample has:

              [[1, -1],
               [1, -1]]

        This is different. 

        How did they get that? 

        They might have used a different decomposition. 

        Therefore, we can choose any decomposition as long as the two matrices are absolutely symmetric and sum to A.

        The diagonal distribution is free. 

        For matrix X, we can set the diagonal to any values, and then matrix Y will have the rest. 

        And for the off-diagonal, we have a specific assignment in mind that is guaranteed to work.

        Alternatively, we might use a different off-diagonal assignment. 

        For example, in the sample, they did:

           for matrix X1: 
                 off-diagonal: 
                       (0,1) = -1, (1,0)=1  -> to have | -1 | = |1| =1.
                 diagonal: 
                       (0,0)=1, (1,1)=-1.

           for matrix X2: 
                 off-diagonal: 
                       (0,1)=4, (1,0)=4.
                 diagonal: 
                       (0,0)=0, (1,1)=8.

        This also works.

        Therefore, we have freedom in the diagonal and also in the off-diagonal as long as the conditions for the matrix to be absolutely symmetric are met and the sum is A.

        However, the off-diagonal assignment we did (using the antisymmetric for X and symmetric for Y) is systematic and integer and works.

        Therefore, we do:

          if the matrix is zero, then output k=0.
          else if the matrix is absolutely symmetric (i.e., for every i,j, |A_{i,j}| = |A_{j,i}|), then output k=1 and output the matrix A.
          else (matrix has even-sum for every off-diagonal pair and is not absolutely symmetric) -> output k=2.

          then for the two matrices:

              Matrix1 (antisymmetric for off-diagonals): 
                      for i != j: 
                             Matrix1_{i,j} = (A_{i,j} - A_{j,i}) / 2
                      for i = j: 
                             Matrix1_{i,i} = we can set to 0, or distribute arbitrarily (as long as we set Matrix2_{i,i} = A_{i,i} - Matrix1_{i,i}).

              Matrix2 (symmetric for off-diagonals):
                      for i != j: 
                             Matrix2_{i,j} = (A_{i,j} + A_{j,i}) / 2   [ and also Matrix2_{j,i} = (A_{i,j}+A_{j,i})/2 ]
                      for i = j: 
                             Matrix2_{i,i} = A_{i,i} - Matrix1_{i,i}

          But note: we are free to distribute the diagonal. To avoid large numbers and for simplicity, we set Matrix1_{i,i}=0 for all i.

        However, the sample output for the first matrix distributed the diagonal as [1,-1]. 

        If we distribute arbitrarily, we must ensure that the entries are within [-10^18, 10^18]. 

        Since the matrix entries in A are up to 10^9, and our off-diagonal assignment yields integers that are at most around 10^9 in absolute value, the diagonal assignment is also within 10^9, so it is safe.

        But if we set the diagonal of Matrix1 to be part of A_{i,i}, then the diagonal of Matrix2 will be the rest. 

        However, there is a condition: the matrix must be integer, and it is.

        Therefore, we can choose any values for the diagonal of Matrix1 as long as the sum with Matrix2's diagonal is A_{i,i}. 

        To be concrete, we choose 0 for the diagonal of Matrix1.

        But the sample output for the first matrix has nonzero diagonal. 

        The problem does not require any specific distribution.

        Therefore, we can choose either. 

        However, note: the matrix might have diagonal entries that are very large, and if we set the diagonal of Matrix1 to 0, then the diagonal of Matrix2 might be up to 10^9, which is within the output limit (10^18).

        So we do:

          Matrix1 = [[ for i in range(n): for j in range(n): 
                         if i==j: 0
                         else: 
                             if i<j: (A[i][j]-A[j][i])//2 
                             if i>j: (A[i][j]-A[j][i])//2   [ note: for i>j, we are at (i,j) and also we have a formula? Actually, we want for i>j: 
                                    we should use the formula for (i,j) and (j,i): 
                                         Matrix1[i][j] = (A[i][j] - A[j][i])//2   [ but wait, for i>j, this might be the negative of what we have for (j,i) ]

          Let me index: 
               For i>j, we want Matrix1[i][j] = (A[i][j] - A[j][i])//2.

          But note: for the pair (i,j) with i>j, we have 
                Matrix1[i][j] = (A[i][j] - A[j][i])//2
                Matrix1[j][i] = (A[j][i] - A[i][j])//2 = - Matrix1[i][j]

          so it is antisymmetric.

          Matrix2 = 
                for i in range(n):
                   for j in range(n):
                         if i==j: 
                             Matrix2[i][j] = A[i][j]   (since Matrix1[i][j]=0)
                         else:
                             Matrix2[i][j] = (A[i][j] + A[j][i]) // 2   [ and note: for (j,i) we would set Matrix2[j][i] = (A[j][i]+A[i][j])//2 = the same, so symmetric ]

        But wait: in the off-diagonals, for a fixed (i,j) with i!=j, we have defined Matrix1 in a way that for i<j and i>j. 

        Alternatively, we can define for the entire off-diagonal (i != j) for Matrix1: 
                Matrix1[i][j] = (A[i][j] - A[j][i]) // 2

        then for the entire off-diagonal for Matrix2:
                Matrix2[i][j] = (A[i][j] + A[j][i]) // 2

        Then for the diagonal: 
                Matrix1[i][i] = 0
                Matrix2[i][i] = A[i][i]

        Then the sum at (i,j) for i!=j is: 
                Matrix1[i][j] + Matrix2[i][j] = (A[i][j]-A[j][i])/2 + (A[i][j]+A[j][i])/2 = A[i][j]

        And at (i,i): 0 + A[i][i] = A[i][i]

        Also, for Matrix1: 
                For i!=j, we have Matrix1[i][j] = (A[i][j]-A[j][i])/2, and Matrix1[j][i] = (A[j][i]-A[i][j])/2 = - Matrix1[i][j] -> then |Matrix1[i][j]| = |Matrix1[j][i]|.

        For Matrix2: 
                Matrix2[i][j] = (A[i][j]+A[j][i])/2 = Matrix2[j][i] -> then |Matrix2[i][j]| = |Matrix2[j][i]|.

        Therefore, both are absolutely symmetric.

        However, note: the above for Matrix1 is not necessarily antisymmetric in the usual sense (i.e., Matrix1[i][j] = -Matrix1[j][i]), which is what we have. 

        This is valid.

        Therefore, the final solution:

          Step 1: Check for every pair (i<j): 
                     if (A[i][j] + A[j][i]) % 2 != 0, then output -1.

          Step 2: 
                 if the matrix A is the zero matrix, then output k=0.

                 else, check if for every i!=j, |A[i][j]| = |A[j][i]|. 
                         if yes, then output k=1 and output A.

                 else, output k=2 and output two matrices:

                         Matrix1: 
                                for i in range(n):
                                   for j in range(n):
                                         if i==j: 
                                             0
                                         else:
                                            (A[i][j] - A[j][i]) // 2

                         Matrix2: 
                                for i in range(n):
                                   for j in range(n):
                                         if i==j: 
                                            A[i][j]
                                         else:
                                            (A[i][j] + A[j][i]) // 2

        Note: the matrix indices: when we are at (i,j) in the output, we are in row i and column j.

        But in the formula for Matrix1 and Matrix2, we use A[i][j] and A[j][i] for the same pair.

        Example for the third sample in our decomposition:

            Matrix1 = 
                 [0, (A[0][1]-A[1][0])//2 = (3-5)//2 = -1]
                 [(A[1][0]-A[0][1])//2 = (5-3)//2 = 1, 0]

            Matrix2 = 
                 [1, (3+5)//2=4]
                 [(5+3)//2=4, 7]

        Then we output for Matrix1:
                 0 -1 
                 1  0

            Matrix2:
                 1 4 
                 4 7

        This is different from the sample output, but valid.

        However, the sample output for the first matrix is:

                1 -1
                1 -1

        which is not the same as our Matrix1.

        But the problem accepts any.

        But note: the sample output for the second matrix is:

                0 4
                4 8

        which has the off-diagonals as 4 and 4, but the diagonal is 0 and 8.

        In our Matrix2, we have diagonal 1 and 7.

        Therefore, we must allow any distribution of the diagonal. 

        If we want to exactly mimic the sample output, we could distribute the diagonal as in the sample output. 

        However, the off-diagonal in the sample output for the first matrix is -1 and 1 (at (0,1) and (1,0)) -> which is the same as our Matrix1.

        So what is the difference in the first matrix? 
              The sample has for (0,0):1 and (1,1):-1, while we have 0's.

        Therefore, to output the sample output, we would do:

          Matrix1_sample = 
                [1, -1]
                [1, -1]

          Matrix2_sample = 
                [0, 4]
                [4, 8]

        How to achieve this in general? 

          We can let:
                Matrix1 = 
                      off-diagonal: as in our systematic method: (A[i][j]-A[j][i])//2 for off-diagonal i!=j.
                      diagonal: we can set to any values, say we set it to a vector d that we choose arbitrarily.

                Matrix2 = 
                      off-diagonal: (A[i][j]+A[j][i])//2 for off-diagonal.
                      diagonal: A[i][i] - d[i]

          In the sample, we would set d[0]=1, d[1]=-1.

          But the problem does not require a specific distribution.

        Therefore, we are free to choose d arbitrarily. 

        To keep it simple and avoid large numbers, we can set d to 0.

        But note: the sample output used a nonzero d.

        Why might nonzero be useful? 

          In our systematic method with d=0, the off-diagonal might be within [-10^9,10^9] and the diagonal of the second matrix might be within [ -10^9, 10^9], so it is safe.

        However, if we use nonzero d, then the entries might become larger? 

          For example, if we set d[i] = A[i][i] for all i, then the second matrix would have diagonal 0, but the first matrix would have diagonal A[i][i]. 
          Then the off-diagonal of the first matrix is the same. 

          But the matrix might have up to 50*50=2500 entries, and the absolute value of any entry in the first matrix would be at most |A[i][i]| or | (A[i][j]-A[j][i])//2 |, which is at most 10^9, and the output requirement is 10^18, so it is safe.

        Therefore, we can choose any distribution.

        Since the problem sample output used a specific distribution, we are not required to match it.

        But note: the problem sample input #1 is the zero matrix. We said output k=0. 
        The sample output is k=1 and the zero matrix. 

        We must therefore reconcile.

        In the problem sample input #1, the matrix is zero. We condition: 
              if the matrix is zero -> output k=0.

        But the sample output is k=1.

        What is the correct minimal k? 

          The empty decomposition (k=0) is valid.

        Therefore, we should output 0.

        However, the problem sample output is 1.

        This suggests that the problem does not allow k=0. 

        After rethinking: 
          The problem says "decompose into a sum of matrices". In mathematics, the empty sum is conventionally the zero matrix. 
          And the zero matrix is absolutely symmetric. 

          So it is valid.

        But the sample output is 1.

        To verify, we should try the sample input in the online judge.

        But the problem statement sample output is provided. 

        Therefore, we must output as the sample.

        What do the sample inputs and outputs of the problem say?

          Sample input #1 is a 3x3 zero matrix. The sample output is:

                1
                0 0 0
                0 0 0
                0 0 0

          Therefore, we must output k=1 for the zero matrix.

        Why would they do that? 

          Note: the problem might not consider the empty decomposition. 

          Or, the problem might require that the decomposition uses at least one matrix.

          In the sample input #1, they output one matrix which is the zero matrix.

          This is also valid.

          But the minimal k is 0, not 1.

          However, the problem says "minimize the number k", and 0 is smaller than 1.

        There is a possibility: the problem might be allowing the empty decomposition only if the matrix is zero. But then the minimal is 0.

        Given the sample output, we must output 1 for the zero matrix.

        Therefore, we will not output k=0 for any matrix. 

        Instead, we do:

          if the matrix is absolutely symmetric (which includes the zero matrix), then we can use k=1.

          In fact, the zero matrix is absolutely symmetric (|0|=|0| for every pair).

          Therefore, we can simply: 

                if there exists an off-diagonal pair (i,j) (i<j) such that (A[i][j]+A[j][i]) is not even, then output -1.

                else, 
                   if the matrix is absolutely symmetric, output k=1 and output A.
                   else, output k=2 and output the two matrices as above.

          This will output k=1 for the zero matrix.

        But wait, what if the matrix is not the zero matrix but is absolutely symmetric? 
            For example, a nonzero symmetric matrix or a nonzero antisymmetric matrix.

        Example: 
             2
             0 1
             -1 0

          This is antisymmetric: |1|=| -1| =1, and the even-sum for the off-diagonal: 1 + (-1)=0 even.

          And it is absolutely symmetric. 
          So we output k=1.

        This is minimal.

        Therefore, the solution is:

          Step 1: For every i<j, if (A[i][j] + A[j][i]) % 2 != 0, then output -1.

          Step 2: Check if for every i, j, |A[i][j]| = |A[j][i]|. 
                   Note: this is for every pair, including diagonal (which is always true: |A[i][i}| = |A[i][i]|).

          Step 3: 
                   if true, then output k=1 and output the matrix A.

                   else, output k=2 and output:
                         Matrix1: 
                               for i in range(n):
                                   for j in range(n):
                                       if i == j:
                                          0   // or could be chosen differently, but we choose 0 for simplicity.
                                       else:
                                          (A[i][j] - A[j][i]) // 2   [even though i and j are not ordered, we do the same for every cell]

                         Matrix2:
                               for i in range(n):
                                   for j in range(n):
                                       if i == j:
                                          A[i][j]   // because we set the first matrix's diagonal to 0.
                                       else:
                                          (A[i][j] + A[j][i]) // 2

        But note: in Matrix1, when we are at (i,j) with i>j, we have A[i][j] and A[j][i] are swapped compared to when i<j. 
              Specifically, at (i,j) with i>j, 
                     value = (A[i][j] - A[j][i]) // 2.
              At (j,i) (which is (i,j) swapped), we would have in Matrix1: 
                     value = (A[j][i] - A[i][j]) // 2 = - (A[i][j] - A[j][i])//2.

          which is consistent with our requirement: the matrix will be such that the (i,j) and (j,i) entries are negative of each other.

        Let me test for the third sample with our method:

            A = [[1,3],
                  [5,7]]

            We output k=2.

            Matrix1 = 
                  [0, (3-5)//2 = -1]
                  [ (5-3)//2 = 1, 0]

            Matrix2 = 
                  [1, (3+5)//2=4]
                  [ (5+3)//2=4, 7]

            So we output for the first matrix:
                  0 -1
                  1  0

            for the second matrix:
                  1 4
                  4 7

        And the sample output is:

            Matrix1_sample = 
                  1 -1
                  1 -1
            Matrix2_sample = 
                  0 4
                  4 8

        Which is different, but ours is also valid.

        Therefore, we output ours.

        However, note: the problem sample output might be generated by a different distribution of the diagonal and also a different choice for the off-diagonal decomposition. 

        But the problem does not require a specific decomposition.

        Therefore, we are safe.

        One more sample: the zero matrix.

            We output k=1 and the matrix A (which is zero).

        Sample input #1: 
             3
             0 0 0
             0 0 0
             0 0 0

        then output:
             1
             0 0 0
             0 0 0
             0 0 0

        This matches.

        Sample input #2: 
             2
             0 1
             0 0

        Check: 
             off-diagonal pair: (0,1): A[0][1]=1, A[1][0]=0 -> 1+0=1, which is not even -> output -1.

        This matches.

        Therefore, the final solution.

        Note: we must be cautious with integer division in Python: if the number is negative, //2 rounds to floor, but we want the integer that is (a-b)/2.

        Example: (3-5) = -2, -2//2 = -1, which is correct.

        But note: the even condition ensures that (A[i][j]-A[j][i]) is even.

        We do the same for (A[i][j]+A[j][i])//2.

        Code outline in Python-like pseudocode (not for output, but for understanding):

          n = int(input())
          A = [ list of n lists ]

          # Step 1: Check every off-diagonal pair has even sum
          for i in range(n):
             for j in range(i+1, n):
                 if (A[i][j] + A[j][i]) % 2 != 0:
                     print(-1)
                     exit(0)

          # Step 2: Check if the matrix is absolutely symmetric: 
          is_abs_sym = True
          for i in range(n):
             for j in range(n):
                 if abs(A[i][j]) != abs(A[j][i]):
                     is_abs_sym = False
                     break
             if not is_abs_sym:
                 break

          If is_abs_sym:
             print(1)
             for i in range(n):
                 print(" ".join(str(x) for x in A[i]))

          else:
             print(2)
             # output matrix1
             for i in range(n):
                 row = []
                 for j in range(n):
                     if i == j:
                         row.append(0)
                     else:
                         # (A[i][j] - A[j][i]) // 2
                         row.append(str((A[i][j]-A[j][i])//2))
                 print(" ".join(row))

             # output matrix2
             for i in range(n):
                 row = []
                 for j in range(n):
                     if i == j:
                         row.append(str(A[i][j]))
                     else:
                         row.append(str((A[i][j]+A[j][i])//2))
                 print(" ".join(row))

        However, note: the condition for absolutely symmetric is only for the off-diagonals? 
          The diagonal: |A[i][i]| = |A[i][i]| is always true.

        But the condition we are checking in the double loop also for the diagonal: 
                 if abs(A[i][i]) != abs(A[i][i]))  -> always true.

        So it is not a problem.

        But note: the absolutely symmetric condition is: for every i,j, |A[i][j]| = |A[j][i]|. 
          This includes the diagonal.

        Therefore, the double loop is checking the entire matrix.

        But the diagonal is always: |A[i][i]| = |A[i][i]|, so it is redundant. 

        We can optimize by only checking i and j not necessarily with i<=j, but it doesn't matter.

        However, we can avoid the diagonal.

          is_abs_sym = True
          for i in range(n):
             for j in range(n):
                 if abs(A[i][j]) != abs(A[j][i]): 
                     is_abs_sym = False
                     break
             if not is_abs_sym:
                 break

        This is correct.

        But note: the matrix might be large? n up to 50, so it is fine.

        Let me test with a 2x2 matrix that is symmetric: 
              A = [[1,2],[2,1]]
              Step1: off-diagonal: 2+2=4 even.
              Step2: 
                   i=0,j=0: abs(1)==abs(1) -> true.
                   i=0,j=1: abs(2)==abs(2) -> true.
                   i=1,j=0: same as above.
                   i=1,j=1: true.
              -> is_abs_sym = True, so output k=1.

        Test with a 2x2 matrix that is antisymmetric: 
              A = [[0, 1],[-1,0]]
              Step1: 1+(-1)=0 even.
              Step2: 
                   for (0,1): abs(1)==abs(-1) -> 1==1 -> true.
                   similarly for others.
              -> output k=1.

        Test with a matrix that is not: 
              A = [[0,3],[5,0]]  -> from the sample third, but we to make it absolutely symmetric? 
                   |3| and |5| are not equal, so is_abs_sym = false -> output k=2.

        This is as intended.

        Therefore, the solution is as above.

        However, note: the condition for the entire matrix to be absolutely symmetric is not just for the off-diagonals. It is for every entry. And we are checking that.

        We'll code accordingly.

        But note: the matrix might have negative numbers. The condition |A[i][j]| = |A[j][i]| is the same as abs(A[i][j]) = abs(A[j][i]).

        So we use abs.

        Finally, note: the output must have the matrices if k<=50. 
              Since k is either 1 or 2, we always output the matrices.

        However, if the matrix is large, the entries in the decomposition might be as large as 10^9, which is within the requirement of the output (between -10^18 and 10^18).

        Therefore, we are done.

        Let me run the provided samples.

        Sample #1: 
            n=3, zero matrix.
            Step1: for every off-diagonal: 0+0=0 even.
            Step2: it is absolutely symmetric -> output k=1 and the zero matrix.

        Sample #2:
            n=2: 
               0 1
               0 0
            Step1: for (0,1): 1+0=1 -> not even -> output -1.

        Sample #3:
            n=2:
               1 3
               5 7
            Step1: (0,1): 3+5=8 even -> ok.
            Step2: 
                  for (0,0): |1|==|1| -> true.
                  (0,1): |3|==|5|? 3!=5 -> false -> not absolutely symmetric.
            then output k=2 and the two matrices as described.

        Output for sample #3 by our program:

            Matrix1 = 
                  [0, (3-5)//2 = -1]
                  [(5-3)//2 = 1, 0]
            so:
                  0 -1
                  1  0

            Matrix2 = 
                  [1, (3+5)//2=4]
                  [ (5+3)//2=4, 7]
            so:
                  1 4
                  4 7

        But the sample output is:
            2
            1 -1
            1 -1
            0 4
            4 8

        This is different. 

        However, our decomposition is valid. The problem says any decomposition.

        Therefore, we output ours.

        But the sample output might have been generated by a different method. 

        We are safe.

        However, the problem sample output has the property that the first matrix has the off-diagonals as well, and also distributed the diagonal.

        To and also the second matrix has a different diagonal.

        Our method sets the diagonal of the first matrix to zero and the second matrix to A's diagonal.

        This is by far the simplest.

        We'll output that.

        One more: the matrix that is symmetric but not with even off-diagonal sums? 
            Actually, we require even off-diagonal sums for every pair, which is necessary. 
            And if the matrix is symmetric, then A[i][j] = A[j][i], so A[i][j]+A[j][i] = 2A[i][j] -> even.

        Therefore, symmetric matrices are always allowed.

        Similarly, antisymmetric: A[i][j] = -A[j][i], then A[i][j]+A[j][i]=0 -> even.

        Therefore, our method works.

        We'll implement accordingly.

        Note: if n=1, then there are no off-diagonal pairs. 
            Then the even-sum condition holds vacuously.
            Then we check if the matrix is absolutely symmetric: 
                  for the only element: |A[0][0]| = |A[0][0]| -> true.
            Therefore, output k=1 and output the matrix.

        This is correct.

        Therefore, the editorial:

          We first check that for every pair (i<j) the sum A[i][j]+A[j][i] is even. If not, output -1.

          Then, if the matrix A is absolutely symmetric (i.e., for every i,j, |A[i][j]| = |A[j][i]|), then we output k=1 and the matrix A.

          Otherwise, we output k=2 and two matrices:
                Matrix1: 
                      for i and j:
                         if i==j, 0
                         else, (A[i][j] - A[j][i]) // 2   [as an integer, which is possible because the difference is even]
                Matrix2:
                      for i and j:
                         if i==j, A[i][j]
                         else, (A[i][j] + A[j][i]) // 2

          Note: the division is integer division and the even condition ensures it is an integer.

        This decomposition is valid and minimizes k: 
             k=1 if possible, which is when the matrix is already absolutely symmetric.
             otherwise, k=2.

        And there is no decomposition with k=0 (except the zero matrix, but we are outputting k=1 for the zero matrix) because if the matrix is nonzero, we use at least one matrix. 
        But note: the zero matrix is handled by the absolutely symmetric branch (k=1), so we never output k=0.

        Why is k=1 minimal for a matrix that is not absolutely symmetric? 
             Because if we could use one matrix, then that matrix would have to be absolutely symmetric, so its entries would satisfy |X_{i,j}| = |X_{j,i}| for every i,j. 
             Then the matrix A = X would have to be absolutely symmetric.

        Therefore, when the matrix is not absolutely symmetric, we must use at least two matrices.

        And we have a decomposition with two.

        Hence, the minimal k is either 1 or 2.

        And the even-sum condition is necessary.

        Therefore, the solution is complete.

        Note: the sample outputs might differ in the actual numbers in the decomposition (especially in the diagonal distribution), but the provided decomposition is valid.

        We output accordingly.

        Let me test on a small matrix that is not absolutely symmetric:

          n=1: not possible to be not absolutely symmetric.

          n=2: 
               A = [[0, 2], 
                    [0, 0]]

          Step1: off-diagonal: A[0][1]=2, A[1][0]=0 -> 2+0=2 even.
          Step2: 
               for i=0,j=0: |0|=|0| -> true.
               i=0,j=1: |2| = |0| -> 2 !=0 -> not absolutely symmetric.

          then output k=2.

          Matrix1 = 
                [0, (2-0)//2 = 1]
                [(0-2)//2 = -1, 0]
          Matrix2 = 
                [0, (2+0)//2=1]
                [ (0+2)//2=1, 0]

          Then the sum:
                Matrix1 + Matrix2 = 
                     [0, 1+1=2]
                     [-1+1=0, 0+0=0] -> matches A.

          And Matrix1 is absolutely symmetric: 
               (0,0):0=0, (0,1):|1|=|-1| -> 1=1, (1,0): same, (1,1):0=0.

          Matrix2: 
               (0,0):0, (0,1):1, (1,0):1, (1,1):0 -> which is symmetric, so absolutely symmetric.

        This is correct.

        Therefore, we are done.