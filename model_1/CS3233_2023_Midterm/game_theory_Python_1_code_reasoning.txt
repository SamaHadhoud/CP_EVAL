 We are given multiple test cases, but the total n over test cases <= 300000.
 We set S = 550 (since sqrt(300000) is about 547.7).
 We'll use two DP arrays: dp0[i] for when the first player (maximizer) is at cell i, and dp1[i] for when the second player (minimizer) is at cell i.
 However, note: the problem says that the token is initially placed by SoCCat on the s-th cell, and then the first move is by the maximizer (you). 
 Therefore, for a starting cell s, we are interested in dp0[s] (since the first move is by the maximizer).

 Steps:
  For a cell i:
    - The current player adds a_i to the score.
    - Then they choose a multiple k of p_i (so k = t * p_i, t>=1) such that either:
          x = i + k <= n   -> then we move to cell x
          or if no such x (i.e., i + k > n) then the game ends.

  Therefore, from state i (with turn information) we have:
      If it's the maximizer's turn (state dp0[i]):
          Option 1: end the game -> score = a_i.
          Option 2: choose a valid k (so that j = i + k <= n) and then the game continues from j with the minimizer's turn -> score = a_i + dp1[j].
          Then dp0[i] = max( a_i, a_i + max_{valid j} { dp1[j] } )   [if there are valid moves]

      Similarly, for the minimizer's turn (state dp1[i]):
          dp1[i] = min( a_i, a_i + min_{valid j} { dp0[j] } )

  But note: the minimizer might have multiple moves to choose from? Actually, the minimizer will choose the move that minimizes the total score.

 However, the catch: we cannot iterate over all multiples for every i because that would be O(n) per cell -> worst-case O(n^2).

 We use sqrt decomposition: 
   Let S = 550.
   For divisors (step sizes) d = p_i:
        - If d <= S (small step): we precompute for each residue mod d the best values for dp0 and dp1? 
          Actually, we want for a given residue r mod d, the best value for transitions from a cell j that is ≡ r mod d and j>=current i? 
          But note: we process from right to left. For a fixed modulus d, we can maintain an array for each residue class, and update as we move left.

        Specifically, for each d in [1, S], we maintain:
            max1[d][r] = maximum value of dp1[j] for j that is ≡ r mod d and j>i (since we process from right) but we update as we go left.
            min0[d][r] = minimum value of dp0[j] for j that is ≡ r mod d and j>i.

        Then for a cell i with step size d_val = p_i, and d_val<=S, we look at residue r = i mod d_val. Then the next moves are to j = i + k (k multiple of d_val) -> j ≡ i mod d_val? Actually, j = i + t*d_val -> j ≡ i mod d_val? Not exactly: 
            j = i + t*d_val -> j mod d_val = (i mod d_val) because we add multiples of d_val.

        So for step size d_val, we are only interested in the residue class of i mod d_val? Actually, yes: all j that can be reached from i are in the same residue class.

        But note: we require j>i and j<=n. Since we process from right to left, when we are at i, all j>i have been processed.

        So for small d_val: we can get:
            candidate0 = max1_list[d_val][r]   # the best dp1[j] for j in residue r mod d_val and j>i (and j<=n) that we've processed so far.
            candidate1 = min0_list[d_val][r]   # similarly for dp0[j] (for minimizer's next state).

        However, note: the candidate0 for the maximizer is the maximum dp1[j] in the residue class? But we are going to use the same residue class for the next step.

        But wait: the residue class is fixed? Actually, the next cell j must be ≡ i mod d_val? 
            Let r0 = i mod d_val, then j = i + k = i + t*d_val -> j mod d_val = (i mod d_val) + (t*d_val mod d_val) = r0.

        So indeed, the next cells are in the same residue class.

        Therefore, for a small step d_val, we can get the candidate for the maximizer from the residue class r0: the best dp1[j] for j in the residue class r0 and j>i (which we have computed for j>i).

   For large d_val (d_val > S): the number of multiples we can jump to is at most n/d_val <= n/S ≈ 300000/550 ≈ 546. So we can iterate over the multiples.

 Implementation:

   We'll create:
        max1_list: for d in range(1, S+1), we have an array of length d (for residues 0..d-1) initialized to -inf (for max) and +inf for min?
        Actually, for the minimizer's candidate we need min0_list: an array for each d in [1,S] of residues, storing the minimum dp0[j] for that residue.

   Then we process i from n down to 1:

        For the current i with step size d_val = p[i-1] (since our arrays are 0-indexed, the cell i is at index i-1 in p and a):

        If d_val <= S:
            r = i % d_val   (but note: if d_val=1, then residue 0? Actually, modulo operation: for d_val>=1, we do r = i % d_val, but if i % d_val==0, then residue 0? Yes.)

            Then we look at max1_list[d_val][r] and min0_list[d_val][r]:
                If max1_list[d_val][r] is -inf (meaning we haven't seen any j in this residue class) then there is no valid next move? -> then we have to end the move: candidate0 = None? or we just use the terminal move.

        Else (d_val > S):
            Then we iterate j = i + d_val, i + 2*d_val, ... until <= n.
            Then we want:
                candidate0 = max_{j} { dp1[j] }   for j in the set of multiples from i (by adding multiples of d_val) and j<=n.
                candidate1 = min_{j} { dp0[j] }   for the same j's.

        Then:
            If candidate0 is None (meaning no valid next move) then:
                dp0[i] = a_i
                dp1[i] = a_i
            Else:
                dp0[i] = max( a_i, a_i + candidate0 )   # because the maximizer can choose to end (a_i) or move to j and get a_i + dp1[j] (and then the minimizer plays)
                dp1[i] = min( a_i, a_i + candidate1 )   # similarly for the minimizer.

        Then, for updating the small step structures: for each d from 1 to S, we update the residue class of i mod d.

            For d in [1, S]:
                r = i % d
                if dp1[i] > max1_list[d][r]:
                    max1_list[d][r] = dp1[i]
                if dp0[i] < min0_list[d][r]:
                    min0_list[d][r] = dp0[i]

   However, note: we are processing from right to left, so when we update the residue class for a modulus d, we are including the current i and then we'll use it for cells to the left? But the moves from a left cell i0 to i (which is to the right) is allowed? Yes, because i>i0.

   But note: when we are at a cell i, the residue arrays for each modulus d are built from indices j>=i. So when we update, we are including i. Then when we go to a left cell, we can use i as a candidate.

   However, the residue arrays are built for each modulus d: for each residue r, we store the best value for indices j that we have processed (which are >= current i). Since we are going backwards, we start at n and go to 1, so at i we have processed all j>i.

   But when we update the residue arrays at i, we are adding the current i. Then when processing a left cell, they will consider i as a candidate.

   This is exactly what we want: the moves from a left cell i0 can jump to i (if i0 + k = i and k is a multiple of p_i0, and note that i>i0) and we have the value for i already computed.

   However, note: the residue arrays for a modulus d are storing the best value for indices j that are ≡ r mod d and that we have processed (j from i to n). Since we update as we go left, we are always including the best from the right.

   But caution: for a fixed d and residue r, we are storing the best value for j>=current i? Yes.

   However, we must note that when we start a modulus d array, we initialize to -inf for max and +inf for min. Then as we process from n down to 1, we update the residue arrays. Then when at cell i, we can use the residue arrays for modulus d to get the best value for j in the residue class that we have already processed (which are j>=i). This is correct.

   But note: the residue arrays are shared for all d from 1 to S. We have to reset them for each test case.

   Also note: the modulus d arrays are for each modulus d from 1 to S. We have max1_list[d] and min0_list[d] for d in [1,S]. We'll create them as lists of lists? 

   However, the total memory: for d from 1 to S, the total number of entries is d for each d: so total memory = 1+2+...+S = S(S+1)/2 ≈ 550*551/2 ≈ 150K, which is acceptable.

   Steps:

        Precomputation for each test case:
            max1_list: for d from 1 to S: an array of length d, each element is set to -10**18 (NEG_INF) for max1 (for dp1) and 10**18 (INF) for min0 (for dp0).

        Then for i from n down to 1:

            d_val = p[i-1]
            a_i = a_list[i-1]

            candidate0 = None   # for the maximizer: we want the max of dp1[j] over valid j (if any exists) for the next state
            candidate1 = None   # for the minimizer: we want the min of dp0[j] over valid j

            If d_val <= S:
                r = i % d_val
                # Check if we have any j in this residue class? 
                # Since we have updated the residue arrays for all j from i to n, if max1_list[d_val][r] is not NEG_INF, then we have at least one j?
                if max1_list[d_val][r] > NEG_INF:
                    candidate0 = max1_list[d_val][r]
                    candidate1 = min0_list[d_val][r]
            Else:
                # d_val > S: iterate over multiples
                j_index = i + d_val
                vals0 = []   # we don't need all, just the max and min? we can keep running max and min.
                vals1 = []
                # Actually, we want:
                #   candidate0 = max{ dp1[j] for j = i + d_val, i+2*d_val, ... <= n }
                #   candidate1 = min{ dp0[j] for the same j }
                c0 = NEG_INF
                c1 = INF
                count = 0
                while j_index <= n:
                    count += 1
                    if dp1[j_index] > c0:
                        c0 = dp1[j_index]
                    if dp0[j_index] < c1:
                        c1 = dp0[j_index]
                    j_index += d_val
                if count > 0:
                    candidate0 = c0
                    candidate1 = c1

            If candidate0 is not None:
                dp0[i] = max(a_i, a_i + candidate0)
                dp1[i] = min(a_i, a_i + candidate1)
            Else:
                dp0[i] = a_i
                dp1[i] = a_i

            Then update for each modulus d from 1 to S (including d that are not step sizes? but we have to update for every modulus? because we need for future cells that might use modulus d and residue i mod d? Yes, we do for every d from 1 to S).

            For d in range(1, S+1):
                r = i % d
                if dp1[i] > max1_list[d][r]:
                    max1_list[d][r] = dp1[i]
                if dp0[i] < min0_list[d][r]:
                    min0_list[d][r] = dp0[i]

        Then after processing all i from n to 1, we output dp0[1] to dp0[n] as the answers for starting positions 1 to n? 
            But note: the problem says for starting cell s, the first move is by the maximizer, so we use dp0[s].

   However, note: the total n over test cases <= 300000, and for each test case we iterate from n down to 1. For each i, we do:
        For d_val <= S: O(1)
        For d_val > S: O(n / d_val) = O(sqrt(n)) per i -> worst-case total over all i in one test case: 
            Sum_{i=1}^{n} [if p_i > S then O(n / p_i) else O(1) ]

        Since the p_i are distinct? Actually, the problem says: "p_i ≠ p_j if i≠j", so distinct.

        But note: the condition d_val> S: then the step sizes are large. The worst-case total work for large steps is:
            For each i with p_i > S, we do O(n/p_i). 
            Then the total work for one test case for large steps is sum_{p_i > S} (n / p_i).

        How to bound? 
            The distinct p_i: the values of p_i are distinct and in [1, n]. 
            The number of distinct p_i that are > S is at most n (but actually the array p has n distinct numbers? yes, permutation? yes: "p_i ≠ p_j if i≠j" -> it's a permutation of [1..n]? 
            Actually, the problem says: "p_i ≠ p_j if i≠j" and 1<=p_i<=n -> so it is a permutation.

        Therefore, the distinct p_i are the integers 1 to n.

        Then the total work for large steps in one test case is:
            Sum_{d=S+1}^{n} (number of indices i such that p_i = d) * (n / d) 
            But each d appears exactly once, so it's: 
                Sum_{d=S+1}^{n} (n / d) 
            = n * (H_n - H_S) 
            ≈ n * (ln n - ln S) 
            which is about O(n log n). 

        But the total n over test cases is 300000, and the worst-case n per test case can be 300000, then the worst-case for one test case is about 300000 * log(300000) ≈ 300000 * 12.6 = 3.78e6, which is acceptable? 

        However, the problem states that the sum of n over all test cases is <= 300000. So the worst-case is that there are 300000 test cases each with n=1? Then the total n is 300000. But note: the constraint says "the sum of n over all test cases does not exceed 300000". 

        Therefore, the worst-case total n is 300000. But the total work for large steps is over distinct d that are large? And the distinct d are the distinct p_i in the entire test case? 

        Actually, we have to consider: the work per test case for large steps is O(n * (number of distinct large divisors considered?)) but note: we are iterating for each i and for each i with large p_i we do O(n/p_i). 

        But the total work for one test case for large steps is the sum over i (with p_i> S) of (n / p_i). 

        And the entire test case has n (the number of cells) and we are summing over the distinct p_i? Actually, no: we are iterating for each i, so if we have a permutation, then each d from S+1 to n appears exactly once. Therefore, the total work for one test case is:
            T = sum_{d = S+1}^{n} (n / d) 
            Since n is the size of the array in that test case.

        But the constraint: the sum of n over test cases <= 300000. However, note that the worst-case n per test case can be up to 300000, but the total n is 300000? Then there is only one test case? 

        Actually, the problem says: "the sum of n over all test cases does not exceed 300000". So the worst-case is one test case with n=300000. Then the large steps work is about 300000 * H_{300000}? 

        Actually, H_{300000} ≈ 13, so 300000 * 13 = 3.9e6, which is acceptable in Python? 

        But note: worst-case 300000 test cases each with n=1: then the total n=300000. Then the large steps: for each test case of n=1, we check if p_i (which is 1) is <=S -> so we don't do the large steps. So the total work is 300000*O(1) = 300000.

        However, worst-case one test case with n=300000: we do about 300000 * (1 for small steps) + (work for large steps: sum_{d=S+1}^{n} (n/d) ) which is about 300000 * (1) + 300000 * (H_n - H_S) ≈ 300000 + 300000 * (13-6) = 300000 + 2.1e6 = 2.4e6, which is acceptable.

   But note: the inner loop for updating the residue arrays for d in [1,S] is O(S) per i. Then total for one test case: O(n * S) = 300000 * 550 = 165e6, which might be borderline in Pyton? But note: the total n over test cases is 300000, so worst-case one test case with n=300000: 300000 * 550 = 165e6 operations. 

   We must optimize the residue update: we are doing for each i, for each d in [1, S] (which is 550 iterations) and each iteration is O(1). 165e6 iterations in Python might be acceptable? But we have to see the constraints: worst-case 165e6 operations in Pyton might be around 1-2 seconds? But the problem says total n over test cases <=300000, so worst-case one test case with n=300000 -> 300000*550 = 165e6, and we have 1.0s time limit. In Pyton, 165e6 operations might be borderline in Pyton (in Pyton, 1e8 operations per second is optimistic? so 165e6 might be 1.65s?).

   How to improve? We note that we only need to update for modulus d that are in the set [1,S] but we don't need to update for every modulus? Actually, we have to update for every modulus d from 1 to S because any modulus d might be used by a future cell to the left. 

   But we cannot avoid. 

   Alternatively, we can note that the modulus d arrays are independent. But we must update for each d and residue r = i % d.

   We have to do 550 * 1 = 550 operations per i -> 300000 * 550 = 165e6. 

   We can try to use S = 400? Then 400 * 300000 = 120e6? But sqrt(300000) is about 547, so we cannot lower S arbitrarily because then the large step part becomes too heavy: the large steps are for d > S. Then the number of distinct large d in one test case is about n, and the work per large step is O(n/d) and the total work is n * (H_n - H_S). If we lower S to 400, then H_n - H_S is about 13 - 6.5 = 6.5 -> 300000 * 6.5 = 1.95e6, which is acceptable. But the residue update becomes 400 * 300000 = 120e6, which is better.

   However, the problem states that the total n over test cases is 300000. So worst-case one test case with n=300000: 
        residue update: 300000 * S -> we want S as small as possible to reduce this, but then the large steps become more expensive.

   We can choose S = sqrt(n) per test case? But the total n over test cases is 300000, but we don't know n per test case. And the constraint says the sum of n over test cases <=300000. 

   But worst-case one test case: n=300000, then S = sqrt(300000) ≈ 547. Then the residue update: 547 * 300000 ≈ 164e6.

   Alternatively, we can set S = 500. Then 500 * 300000 = 150e6.

   We'll set S = 500 and hope that it passes in Pyton? Or we can set S = 400 and then the large steps become about 300000 * (H_{300000} - H_400) ≈ 300000 * (13 - 6) = 2.1e6, which is acceptable.

   But note: the worst-case for residue update is 400 * 300000 = 120e6, which in Pyton might be acceptable in PyPy or in C++ but in Python we need to be efficient.

   However, the problem says the total n over test cases <= 300000. And if there are multiple test cases, the worst-case total n is 300000. Then the worst-case total residue update work is 300000 * S, and we set S=500 -> 150e6.

   Let's set S = 500.

   But note: the sample input: two test cases: n=10 and n=4 -> total n=14. Then the residue update work is 14*500 = 7000, which is acceptable.

   However, worst-case: one test case with n=300000: 300000*500 = 150e6, which might be acceptable in Pyton in Pyton if we run in Pyton with PyPy or in a fast environment? But in CP Python, 150e6 might be borderline (about 1.5 seconds in Pyton? in Pyton, 10e6 per second is typical? then 150e6 -> 15 seconds? too slow).

   Therefore, we must optimize the residue update: we are doing for each i, for each d in [1, S] (500 iterations) and each iteration is a modulus operation and two comparisons. 

   We can reduce S? Let S = 300. Then the residue update per test case: n * 300. For n=300000, that's 90e6. And the large steps: then for each large step (d>300) we do O(n/d) per occurrence. Since there are about 300000 - 300 distinct large d, and the total work for large steps is sum_{d=301}^{300000} (n/d) = 300000 * (H_{300000} - H_300) ≈ 300000 * (13 - 5.7) = 300000 * 7.3 = 2.19e6. So total work per test case (n=300000) is 90e6 (residue update) + 2.19e6 (large steps) = 92.19e6, which is acceptable in Pyton? 

   But note: the problem says the total n over test cases is 300000, so worst-case one test case with n=300000: 92e6 operations? That should be acceptable in Pyton (if we write in Pyton and use Pyton PyPy or Pyton in Pyton with pypy, but in CP Python 10^8 operations per second is optimistic? so 92e6 is 0.92 seconds?).

   Alternatively, we can set S = 250 -> then residue update: 250 * 300000 = 75e6, and large steps: 300000 * (H_{300000} - H_250) ≈ 300000 * (13 - 5.5) = 300000 * 7.5 = 2.25e6 -> total 77.25e6.

   We'll set S = 250.

   But note: the constraint says the total n over test cases <= 300000. So worst-case one test case: n=300000. Then 77.25e6 is acceptable.

   However, we can set S = sqrt(n) for the current test case? But the problem says the total n over test cases is 300000, so worst-case n=300000. Then S = sqrt(300000) ≈ 547. Then residue update: 547 * 300000 = 164.1e6 -> 164 million operations, which might be borderline in Pyton (in C++ it would be acceptable, but in Pyton? we have 1s time limit?).

   How about we set S = 200? Then residue update: 200 * 300000 = 60e6, large steps: 300000 * (13 - 5.3) ≈ 300000 * 7.7 = 2.31e6 -> total 62.31e6.

   We'll set S = 200.

   But note: we must be cautious: the large steps for one cell i with large d_val: the number of multiples is n/d_val, and the total over all large d_val is about 2.31e6 for one test case. Then the residue update is 200 * n = 200 * 300000 = 60e6. Total 62.31e6 operations, which in Pyton should be acceptable.

   However, the operations per residue update: 
        For each d in [1,200]:
            r = i % d
            compare and update: two comparisons and two updates.

   And for the large steps: for each i with d_val>200, we iterate multiples: each multiple we do two comparisons.

   So the total operations: 
        residue update: 200 * n = 60e6 (for one test case with n=300000)
        large steps: 2.31e6 (which is acceptable)

   Therefore, we set S = 200.

   Let's code accordingly.

   Important: we must initialize for each test case the residue arrays for d in [1, S] (with S=200). 

   Steps for one test case:

        n = given
        p = list of n integers
        a_list = list of n integers

        S = 200   # we set S to 200

        # Precompute arrays for residues for d in [1, S]:
        max1_list = [None]  # index 0 unused; we will index from 1 to S
        min0_list = [None]
        for d in range(1, S+1):
            # for modulus d, we need an array of length d
            max1_list.append([NEG_INF] * d)   # max1_list[d] is an array of length d for residues 0..d-1
            min0_list.append([INF] * d)

        dp0 = [0] * (n+1)   # dp0[1..n]
        dp1 = [0] * (n+1)   # dp1[1..n]

        # Process from i=n down to 1
        for i in range(n, 0, -1):
            a_i = a_list[i-1]
            d_val = p[i-1]

            candidate0 = None   # for the maximizer: the best dp1[j] for next state
            candidate1 = None   # for the minimizer: the best dp0[j] for next state (min)

            if d_val <= S:
                r = i % d_val
                # Check if we have seen any j in residue class r for modulus d_val? 
                if max1_list[d_val][r] > NEG_INF:
                    candidate0 = max1_list[d_val][r]
                    candidate1 = min0_list[d_val][r]
            else:
                # d_val > S: iterate over multiples
                j = i + d_val
                c0 = NEG_INF   # for max of dp1[j]
                c1 = INF       # for min of dp0[j]
                found = False
                while j <= n:
                    found = True
                    if dp1[j] > c0:
                        c0 = dp1[j]
                    if dp0[j] < c1:
                        c1 = dp0[j]
                    j += d_val
                if found:
                    candidate0 = c0
                    candidate1 = c1

            if candidate0 is not None:
                # The maximizer: choose between ending the game (a_i) or moving (a_i + candidate0) -> take max
                dp0[i] = max(a_i, a_i + candidate0)
                # The minimizer: choose between ending the game (a_i) or moving (a_i + candidate1) -> take min
                dp1[i] = min(a_i, a_i + candidate1)
            else:
                dp0[i] = a_i
                dp1[i] = a_i

            # Update the residue arrays for each modulus d in [1, S]
            for d in range(1, S+1):
                r = i % d
                # For modulus d, residue r: update the max1_list and min0_list with the current i
                if dp1[i] > max1_list[d][r]:
                    max1_list[d][r] = dp1[i]
                if dp0[i] < min0_list[d][r]:
                    min0_list[d][r] = dp0[i]

        # After processing, for starting positions s from 1 to n, the answer is dp0[s]
        res_arr = [str(dp0[i]) for i in range(1, n+1)]

   Then output.

   Let's test with the sample: 
        "4
         4 3 2 1
         3 2 3 3"

        n=4, p=[4,3,2,1], a=[3,2,3,3]

        We'll process from i=4 down to 1.

        i=4: 
            a_i = 3, d_val = p[3]=1 -> 1<=S (S=200) -> residue r = 4 % 1 -> but modulus 1: residues? 4 mod 1 = 0. 
            Check max1_list[1][0]: initially NEG_INF -> so candidate0 is None -> then dp0[4]=3, dp1[4]=3.
            Then update for d in [1,200]: 
                d=1: r = 4 % 1 = 0 -> max1_list[1][0] = max(-inf, 3) -> 3; min0_list[1][0] = min(inf, 3) -> 3.

        i=3:
            a_i = 3, d_val = p[2]=2 -> <=200 -> residue r = 3 % 2 = 1.
            Check max1_list[2][1]: initially -inf? -> so candidate0 is None -> dp0[3]=3, dp1[3]=3.
            Then update for d in [1,200]:
                d=1: r=3%1=0 -> update: max1_list[1][0]=max(3,3)=3; min0_list[1][0]=min(3,3)=3.
                d=2: r=3%2=1: set max1_list[2][1]=3, min0_list[2][1]=3.

        i=2:
            a_i=2, d_val = p[1]=3 -> <=200 -> residue r=2%3=2.
            Check max1_list[3][2]: initially -inf -> candidate0=None -> dp0[2]=2, dp1[2]=2.
            Then update for d in [1,200]:
                d=1: r=2%1=0: max1_list[1][0] = max(3,2) -> still 3; min0_list[1][0]=min(3,2)=2.
                d=2: r=2%2=0: max1_list[2][0] = -inf -> set to 2? but note: we haven't updated residue 0 for modulus2? 
                    Actually, for d=2: we have residues 0 and 1. 
                    For residue0: currently -inf -> set to 2? 
                ... so we update: for each d, we update the residue r.

        i=1:
            a_i=3, d_val = p[0]=4 -> >200? -> large step: we iterate multiples: j=1+4=5 -> out of bound? so no multiple -> candidate0=None -> dp0[1]=3, dp1[1]=3.

        Then we output: [dp0[1], dp0[2], dp0[3], dp0[4]] = [3,2,3,3] -> matches sample.

        But the sample output is "3 2 3 3", so it matches.

   However, the first sample:

        "10
         3 1 5 2 4 9 6 10 8 7
         1 -2 3 -4 5 -6 7 -8 9 -10"

        Expected: "8 7 3 -4 14 -6 7 -8 9 -10"

        We'll compute for s=1:8, s=2:7, etc.

        We'll compute for i from 10 down to 1.

        i=10: d_val=p[9]=7 -> 7<=200 -> residue=10%7=3 -> check max1_list[7][3]: initially -inf -> candidate0=None -> dp0[10]=a[9]=-10, dp1[10]=-10.
            Then update for d in [1,200]: 
                for each d: r=10%d, update.

        i=9: d_val=8 -> <=200 -> residue=9%8=1 -> check: initially -inf -> candidate0=None -> dp0[9]=9, dp1[9]=9.

        i=8: d_val=10 -> <=200 -> residue=8%10=8 -> check: -inf -> candidate0=None -> dp0[8]=-8, dp1[8]=-8.

        i=7: d_val=6 -> <=200 -> residue=7%6=1 -> check: -inf -> candidate0=None -> dp0[7]=7, dp1[7]=7.

        i=6: d_val=9 -> <=200 -> residue=6%9=6 -> check: -inf -> candidate0=None -> dp0[6]=-6, dp1[6]=-6.

        i=5: d_val=4 -> <=200 -> residue=5%4=1. 
            We need to check: what have we updated for modulus4? 
                We have updated i=10: for modulus4: 10%4=2 -> so residue2: max1_list[4][2] = max( current, dp1[10]=-10) -> -10.
                i=9: 9%4=1 -> max1_list[4][1] = max( current, 9) -> 9.
                i=8: 8%4=0 -> max1_list[4][0]=-8
                i=7: 7%4=3 -> max1_list[4][3]=7
                i=6: 6%4=2 -> max1_list[4][2]=max(-10, -6) -> -6? 
            So at i=5: residue1 -> max1_list[4][1]=9 -> candidate0=9 -> 
                dp0[5] = max(a5, a5+9) = max(5, 5+9)=14.
                dp1[5] = min(5,5+min0_list[4][1]) 
                    min0_list[4][1]: 
                        i=10: residue2: min0_list[4][2]=-10
                        i=9: residue1: min0_list[4][1]=9
                        i=8: residue0: min0_list[4][0]=-8
                        i=7: residue3: min0_list[4][3]=7
                        i=6: residue2: min0_list[4][2]=min(-10,-6)=-10
                    so min0_list[4][1]=9? 
                then dp1[5]=min(5,5+9)=min(5,14)=5? 
            But the expected for s=5 is 14? and we are storing dp0[5]=14 -> that's the answer for starting at 5? yes.

        i=4: d_val=2 -> residue=4%2=0.
            We look at residue0 for modulus2: 
                i=10: residue0 -> 10%2=0 -> min0_list[2][0] = min(inf, dp0[10]=-10) -> -10? and max1_list[2][0]=max(-inf, dp1[10]=-10) -> -10.
                i=9: residue1 -> 9%2=1 -> so residue0: not updated by i=9.
                i=8: residue0: min0_list[2][0]=min(-10,-8) -> -10; max1_list[2][0]=max(-10,-8) -> -8? 
                i=7: residue1: skip.
                i=6: residue0: min0_list[2][0]=min(-10,-6) -> -10; max1_list[2][0]=max(-8,-6) -> -6.
                i=5: residue1: skip.
            So for modulus2, residue0: max1_list[2][0] = -6? -> candidate0 = -6? 
            Then dp0[4] = max(a4, a4 + (-6)) = max(-4, -4-6)=max(-4,-10) -> -4.
            But expected for s=4 is -4 -> matches.

        i=3: d_val=5 -> residue=3%5=3.
            Check modulus5: residue3: 
                i=10: residue0 -> not 3 -> skip.
                i=9: residue4 -> skip.
                i=8: residue3: then max1_list[5][3] = max(initial, dp1[8]=-8) -> -8.
                i=7: residue2 -> skip.
                i=6: residue1 -> skip.
                i=5: residue0: skip.
                i=4: residue4 -> skip.
                So we have max1_list[5][3] = -8? -> candidate0 = -8? 
            Then dp0[3]=max(3, 3-8)=max(3,-5)=3 -> matches.

        i=2: d_val=1 -> residue=2%1=0 -> max1_list[1][0] = max( ... all dp1[i] for i>=2? 
            We have updated: 
                i=10: for modulus1: residue0: max1_list[1][0]=max(initial, -10) -> -10? then updated by i=9:9, then i=8: max(9,-8)=9, then i=7:max(9,7)=9, i=6:max(9,-6)=9, i=5:max(9,5? but wait: we stored dp1[5]=5? 
            Actually, we stored for i=5: dp1[5]=min(5,5+min0_list[4][1]) = min(5,5+9)=min(5,14)=5.
            Then i=4: dp1[4]=min(-4, -4 + min0_list for modulus2? ... 
                For i=4: we computed: 
                    candidate1: we look at min0_list[2][0] = -10? 
                    Then dp1[4]=min(-4, -4-10)=min(-4,-14)=-14? 
            Then for modulus1: we update: 
                i=4: residue0: max1_list[1][0]=max(9, -14) -> still 9? 
            Then at i=2: max1_list[1][0]=9? 
            Then dp0[2]=max(a2, a2+9)=max(-2, -2+9)=max(-2,7)=7 -> matches.

        i=1: d_val=3 -> residue=1%3=1 -> 
            Check modulus3: residue1: 
                i=10: residue1: 10%3=1 -> max1_list[3][1] = max(initial, dp1[10]=-10) -> -10
                i=9: residue0: skip.
                i=8: residue2: skip.
                i=7: residue1: max1_list[3][1]=max(-10,7)=7
                i=6: residue0: skip.
                i=5: residue2: skip? 5%3=2 -> skip.
                i=4: residue1: 4%3=1 -> max1_list[3][1]=max(7, -14)=7? 
                i=3: residue3: skip? 3%3=0 -> skip.
                i=2: residue2: skip? 2%3=2 -> skip.
            Then candidate0=7? 
            Then dp0[1]=max(1, 1+7)=max(1,8)=8 -> matches.

   Therefore, the code should be correct.

   We'll code accordingly.

   Note: we must use a sufficiently large negative and positive for initial values.

   Let NEG_INF = -10**18, and INF = 10**18.

   We'll run the code.

   But note: the constraints say a_i can be as low as -10^9, and we are adding, so we must use a negative that is lower than -10^9 * 300000? Actually, worst-case score: the game can have at most about 300000 moves? and each a_i is at least -10^9, so the worst-case total score is -3e14? We use -10**18 which is safe.

   Similarly for positive.

   We set S=200.

   However, note: the total n over test cases is 300000, so worst-case one test case with n=300000: 
        residue update: 200 * 300000 = 60e6 operations -> acceptable.

   Let's code accordingly.

   Note: the modulus operation: 
        for d in [1,200] and for i from 1 to n: we do i % d. This is O(1) per operation.

   We'll write the code accordingly.

   IMPORTANT: we must avoid using too much memory. For each test case, we have:
        max1_list: list of S lists, the total length of all lists is 1+2+...+S = S(S+1)/2 ≈ 200*201/2=20100 integers -> negligible.

   So we are safe.

   Code:

        S = 200
        for each test case:
            n = ... 
            p = ... 
            a_list = ... 
            # Precompute residue arrays: max1_list and min0_list for d in [1,S] -> each is a list of length d
            # Initialize to NEG_INF and INF respectively.

            dp0 = [0]*(n+1)   # 1-indexed: index 1..n
            dp1 = [0]*(n+1)

            for i from n down to 1:
                a_i = a_list[i-1]
                d_val = p[i-1]
                candidate0 = None
                candidate1 = None

                if d_val <= S:
                    r = i % d_val
                    if max1_list[d_val][r] > NEG_INF:   # meaning we have at least one j in this residue class
                        candidate0 = max1_list[d_val][r]
                        candidate1 = min0_list[d_val][r]
                else:
                    j = i + d_val
                    c0 = NEG_INF
                    c1 = INF
                    found = False
                    while j <= n:
                        found = True
                        if dp1[j] > c0:
                            c0 = dp1[j]
                        if dp0[j] < c1:
                            c1 = dp0[j]
                        j += d_val
                    if found:
                        candidate0 = c0
                        candidate1 = c1

                if candidate0 is not None:
                    dp0[i] = max(a_i, a_i + candidate0)
                    dp1[i] = min(a_i, a_i + candidate1)
                else:
                    dp0[i] = a_i
                    dp1[i] = a_i

                for d in range(1, S+1):
                    r = i % d
                    if dp1[i] > max1_list[d][r]:
                        max1_list[d][r] = dp1[i]
                    if dp0[i] < min0_list[d][r]:
                        min0_list[d][r] = dp0[i]

            # Then output dp0[1..n]

   We'll run the sample test cases.

   However, note: the problem has multiple test cases, and the total n over test cases <=300000. We must reset the residue arrays for each test case.

   We do: for each test case, we create new residue arrays.

   This is acceptable because the residue arrays are only O(S^2) per test case? Actually, we only create arrays for d in [1,S] and the total memory is S(S+1)/2 per test case. But the total test cases: the sum of n over test cases is 300000, but the number of test cases C can be up to 300000? 

        The problem: "The input starts with a line containing an integer C, denoting the number of test cases (1<=C<=300000)".

        And the next constraint: "the sum of n over all test cases does not exceed 300000".

        So worst-case: C=300000, and each n_i=1 (so 300000 test cases of n_i=1). Then the residue arrays per test case: we create 200 arrays of total length 200*201/2=20100? But wait, we create for each d in [1,200] an array of length d. The total memory per test case: about 200* (average length) = 200*100.5 = 20100 integers? and 20100 * 4 bytes = 80400 bytes per test case? Then for 300000 test cases: 300000 * 80400 bytes = 24.12e9 bytes = 24.12 GB -> too much.

   Therefore, we must not create the residue arrays for each test case with such heavy memory.

   How to fix? 

        We note: the residue arrays (max1_list and min0_list) are only used for the current test case. But the total memory per test case for residue arrays is O(S^2) = O(200^2) = 20100 integers. And the total memory for residue arrays over all test cases: 20100 * C. 

        But the constraint: the sum of n over test cases is 300000, but the number of test cases C can be up to 300000? 

        However, the constraint says: the sum of n over test cases <=300000. But note: the residue arrays per test case do not depend on n? They depend only on S (which is fixed). 

        But worst-case: 300000 test cases, each with residue arrays of 20100 integers: 300000 * 20100 = 6.03e9 integers. Each integer is 8 bytes? (we are storing integers that can be as large as 10^18, so we use Python int, but in terms of memory, each int in Python is about 28 bytes? or more? but worst-case we might not be able to allocate that much).

        Alternatively, we can avoid creating the residue arrays for each test case if we do:

            Preallocate for each test case: we know we need two arrays for each d in [1,S]: one for max1 and one for min0, each of length d.

            The total memory per test case: 2 * (1+2+...+S) = 2 * S(S+1)/2 = S(S+1) ≈ 200*201 = 40200 integers? 

            Then for 300000 test cases: 300000 * 40200 = 12.06e6 * 100? = 12.06e6 * 100? Actually, 300000 * 40200 = 12.06e9 integers? That's 12.06 billion integers? which is about 12.06e9 * 28 bytes (for Python int) = 337.68e9 bytes = 337.68 GB -> too much.

        Therefore, we must change the approach: we cannot create the residue arrays for each test case in the worst-case of 300000 test cases.

        How about we note: the constraint says the sum of n over test cases <=300000. Then the number of test cases C is at most 300000, but worst-case the sum of n is 300000. However, the residue arrays per test case are independent of n? We create the same structure for residue arrays for each test case? 

        Actually, the residue arrays are of size O(S^2) per test case, and S is fixed (200). But 200*201/2=20100 integers per test case. Then the total memory over test cases: 20100 * C. And C can be up to 300000? Then 20100*300000 = 6.03e6 * 1000? = 6.03e9 integers? which is too much.

        We must avoid creating residue arrays for each test case? 

        Alternative: we can reuse the residue arrays by resetting them for the next test case. But then we only need one copy. However, we must reset them for each test case. How? 

        We can do:

            Before the loop for test cases:
                Precompute the residue arrays for d in [1,S]? But each test case must have its own.

            Instead, we can create the residue arrays for the current test case and then discard after the test case.

        But the memory usage: worst-case one test case: residue arrays: 20100 integers -> negligible. But worst-case 300000 test cases: then we create 300000 * 20100 integers? which is 6.03e9 integers -> 6.03e9 * 24 bytes = 144.72e9 bytes = 144.72 GB? which is too much.

        How to solve? 

        We note: the constraint says the sum of n over test cases <=300000. And the worst-case number of test cases is 300000, but then each test case has n=1. So the total n=300000. But we are creating residue arrays for each test case: 300000 test cases * 20100 integers = 6.03e9 integers -> too much.

        Therefore, we must avoid creating the residue arrays for each test case? 

        But we must: the residue arrays are specific to the current test case.

        Alternatively, we can avoid creating the residue arrays as a list of lists? We can use a flat array? But the total memory per test case is O(S^2) which is fixed, but the number of test cases is large.

        We must note: the problem states that the total n over test cases is 300000, but the number of test cases C can be up to 300000 only if each n_i=1. Then the total memory for residue arrays: 300000 * (1+2+...+200) * 2 (for two arrays) * size_of_int? 

        In Pyton, we can use:

            total_ints = 2 * (200*201//2) * 300000 = 200*201*300000 = 12060e6 = 12.06e9 integers? -> 12.06e9 * 24 bytes = 289.44e9 bytes = 289 GB -> too much.

        Therefore, we must change the residue arrays to be reset without reallocating? 

        We can preallocate one set of residue arrays and reset them for each test case. How? 

            We know S=200, so the total memory for one residue arrays set is 20100 * 2 = 40200 integers. Then we can have one set and reset it for each test case.

            How to reset: for each d in [1,200]: for each residue r in [0,d-1]: set to NEG_INF and INF.

            The cost to reset: for d from 1 to 200, we do 1+2+...+200 = 20100 assignments per test case. Then for 300000 test cases: 300000 * 20100 = 6.03e9 assignments -> too slow? 

        Alternatively, we can create one set of residue arrays and then for each test case, we reset only the necessary? But the residue arrays are independent per test case.

        We must find a solution: 

        Insight: the total n over test cases is 300000. But the residue arrays per test case are only 20100 integers? Then the total memory for residue arrays over all test cases is 300000 * 20100 = 6.03e9 integers? which is 6.03e9 * 24 bytes = 144.72 GB? which is too much.

        How about we change the residue arrays to a global structure that we reuse? But we cannot because each test case is independent.

        Alternative approach: do not use residue arrays? 

        But we need to quickly get the best value for a residue class for modulus d for cells j>i.

        We can store for each modulus d and residue r, a list that we update by adding the current i and then popping? But we are going backwards. 

        Actually, we process i from n down to 1. For a fixed d and residue r, we want to know the best value of dp1[j] for j in residue r and j>i. 

        We can use a separate structure: an array for each d of a list? and then update by inserting the current i? and then we can maintain the best value? 

        But then the update per i: for each d, we update the residue class of i mod d. And then to get the best value for a residue class, we just look at the stored best value? 

        But we want to reset for each test case. And the memory per test case is still O(S^2). 

        And the total memory over test cases: same problem.

        Therefore, we must avoid the residue arrays if the number of test cases is huge? 

        How about we only create the residue arrays once? and then for each test case, we reset by setting all to NEG_INF and INF? 

        But the reset cost per test case: 20100 assignments, and the total over test cases: 300000 * 20100 = 6.03e9 assignments, which in Pyton is 6.03e9 operations. In Pyton, 6e9 operations might be 60 seconds? (if each assignment takes 10 ns, then 60 seconds). And we have 300000 test cases? That might be borderline in C++ but in Pyton it will be too slow.

        How to avoid resetting by assignment? 

        We can use a fresh list for each test case? But then we are allocating memory, and the total allocation is 300000 * 20100 * 8? (for pointers) -> 300000 * 20100 * 8 bytes = 48.24e9 bytes = 48.24 GB? which is acceptable in Pyton? 

        But the problem memory limit is 1024 MB = 1 GB.

        Therefore, we cannot create 300000 test cases each with 20100 integers.

        We must find a better way.

        Alternative: do not use residue arrays at all for small steps? 

        Instead, for each cell i, for each small step size d (<=200) that might be used by a future cell to the left, we update a global array? 

        But the future cell to the left will have its own step size. 

        Actually, we only care about when we are at a cell i, and it has a small step size d_val, then we want the best value in the residue class i mod d_val for cells j>i.

        We can maintain for each d in [1,200] and each residue r, a heap? but then we are updating and removing? and we go from right to left, so we are adding and never removing? 

        And then to get the best value for residue r in modulus d, we can store:

            best0[d][r] = best value so far (for dp0 and dp1) for residue r in modulus d for cells j>=i (that have been processed).

        And we update as we go left. 

        But we are already doing that: in our original approach, we update the residue arrays as we go from n down to 1. 

        The issue is that we cannot create a new residue arrays for each test case because of memory and reset time.

        We can reuse the residue arrays for the next test case by resetting them to NEG_INF and INF. But the reset time for one test case is 20100, and the total reset time over 300000 test cases is 6.03e9, which is 6.03e9 operations. In Pyton, 6e9 operations might take minutes.

        How about we use a single global residue arrays and then for each test case, we only reset the residues that were touched? 

        But the number of residues touched per test case: we only update for i from 1 to n (n is the size of the test case) and for each i, we update for each d in [1,200]. But the residue arrays are of size 20100, and we reset them to initial state for the next test case.

        We can store the entire residue arrays in a 2D array and reset only the parts that we use? But we use all residues.

        Alternatively, we can store the residue arrays in a list of lists, and for each test case, we create a new list of lists for the residue arrays? and then let the garbage collector handle it? 

        But the memory will be 300000 * 20100 * 8? (assuming each integer is stored in 8 bytes? but in Pyton, an integer is more, but the lists are stored as pointers). The lists: we have 200 lists for max1 and 200 for min0? per test case. The total number of lists: 300000 * 400 = 120e6 lists? and each list has a small length (at most 200) so the total memory might be dominated by the integers: 300000 * 20100 * 2 * 28? (28 bytes per integer) -> 300000*40200*28 = 300000*1.1256e6 = 337.68e9 bytes = 337.68 GB.

        This is not feasible.

        Therefore, we must change the sqrt value S to be smaller? 

        Let S = 100. Then the total memory per test case for residue arrays: 1+2+...+100 = 5050 * 2 = 10100 integers per test case. Then for 300000 test cases: 300000 * 10100 = 3.03e9 integers -> 3.03e9 * 28 = 84.84e9 bytes = 84.84 GB -> still too much.

        S=50: then 1+..+50 = 1275 * 2 = 2550 integers per test case. 300000 * 2550 = 765e6 integers -> 765e6 * 28 = 21.42e9 bytes = 21.42 GB -> still too much.

        S=10: then 1+..+10 = 55 * 2 = 110 integers per test case. 300000 * 110 = 33e6 integers -> 33e6 * 28 = 924e6 bytes = 0.924 GB, which is within 1 GB.

        But then the large steps part: for d_val > 10, the work per cell i with large step size is O(n/d_val). The total work for large steps in one test case: sum_{d_val>10} (n/d_val) = n * (H_n - H_10). For n=300000, H_n≈13, H_10≈2.93, so 300000 * (13-2.93) = 300000*10.07 = 3.021e6 per test case. But the worst-case total n over test cases is 300000, so if there is one test case with n=300000, then the large steps work is 3.021e6.

        However, the residue update work per test case: for each i in [1, n] and for each d in [1,10]: 10 * n. For one test case with n=300000: 300000*10 = 3e6.

        Then total work for one test case (n=300000): 3e6 (residue update) + 3.021e6 (large steps) = 6.021e6, which is acceptable.

        But the total work over all test cases: 
            residue update: for each test case, we do 10 * n_i, and the total n over test cases is 300000, so total residue update work = 10 * 300000 = 3e6.
            large steps: for each test case, we do for each cell i with large step size (d_val>10) the work O(n_i/d_val). 
                In one test case with n_i cells, the work is sum_{d_val>10} (n_i/d_val) but wait: distinct d_val? 
                Actually, we are iterating for each cell i: if p_i>10, then we iterate multiples. 
                The total work for large steps in one test case is: sum_{i: p_i>10} (n_i / p_i).
                And since the p_i are distinct (permutation of 1..n_i), the work is: n_i * (H_{n_i} - H_{10}) 
                Then the total work over test cases: sum_{test case} [ n_i * (H_{n_i} - 2.93) ]

            The worst-case one test case with n_i=300000: 300000 * (13-2.93) = 3.021e6.
            The total over test cases: the sum of n_i is 300000, but the work for large steps is not additive in n_i? 
            Actually, we have multiple test cases. Let the test cases have sizes n1, n2, ... with sum n_i = 300000.
            The work for large steps in test case j: n_j * (H_{n_j} - H_{10}) 
            We want to bound: sum_j [ n_j * (H_{n_j} - 2.93) ]

            What is the maximum of this sum? 
                Note: H_{n} is about O(log n), and n_j are positive integers summing to 300000.
                The function f(n) = n * log(n) is convex? so the maximum is when one test case has n=300000 and the others have n=0? then total work = 300000 * (log(300000)) which is about 300000 * 12.6 = 3.78e6.

            So total work for large steps over test cases: <= 3.78e6.

        Therefore, total work over test cases: residue update 3e6 + large steps 3.78e6 = 6.78e6, plus the other overheads, is acceptable.

        But note: the large steps work for a test case of size n_i is not 300000 * log(300000) but n_i * log(n_i). And the sum of n_i is 300000, but the sum of n_i * log(n_i) is maximized when one n_i=300000 and the others are 0, then it is 300000 * log(300000) ≈ 3.78e6.

        And the residue update work: 10 * (sum of n_i) = 10 * 300000 = 3e6.

        Then total operations: 3e6 + 3.78e6 = 6.78e6, which is very acceptable.

        However, the memory: 
            per test case, we create residue arrays for d in [1,10]: total integers = (1+2+...+10)*2 = 55*2 = 110 integers.
            for 300000 test cases: 300000 * 110 = 33e6 integers, which is 33e6 * 24 bytes = 792e6 bytes = 792 MB, which is within 1024 MB.

        Therefore, we choose S=10.

        Let's test with the sample with S=10.

        First sample: n=10, then for i=1..10, the step sizes: [3,1,5,2,4,9,6,10,8,7] -> all are>=1, and we consider d_val<=10 as small? and d_val>10 as large? but there is no d_val>10 since n=10. 
        Then it should work.

        We'll run the first sample: 
            i=10: d_val=7 (<=10) -> residue=10%7=3 -> then update residue arrays for d in [1,10] for residue 10 mod d.

            ...

        We already know the sample works.

        But note: the sample output for the first sample is "8 7 3 -4 14 -6 7 -8 9 -10", which we got with S=200, and we will get the same with S=10.

        Why? because in the first sample, the largest step size is 10, and 10<=10, so we use the residue arrays.

        The second sample: n=4, step sizes: [4,3,2,1] -> all <=10, so we use residue arrays.

        Therefore, we set S=10.

   Final code:

        S = 10

        for _ in range(C):
            n = int(input())
            p = list of n integers
            a_list = list of n integers

            # Precomputation for residue arrays for d in [1,S]:
            max1_list = []  # index from 0 to S: we will do 0-indexed for d=0 unused, then d=1..S at indices 1..S
            min0_list = []
            max1_list.append(None)  # index0
            min0_list.append(None)
            for d in range(1, S+1):
                max1_list.append([NEG_INF] * d)
                min0_list.append([INF] * d)

            dp0 = [0]*(n+1)
            dp1 = [0]*(n+1)

            for i in range(n,0,-1):
                a_i = a_list[i-1]
                d_val = p[i-1]
                candidate0 = None
                candidate1 = None

                if d_val <= S:
                    r = i % d_val
                    if max1_list[d_val][r] > NEG_INF:
                        candidate0 = max1_list[d_val][r]
                        candidate1 = min0_list[d_val][r]
                else:
                    j = i + d_val
                    c0 = NEG_INF
                    c1 = INF
                    found = False
                    while j <= n:
                        found = True
                        if dp1[j] > c0:
                            c0 = dp1[j]
                        if dp0[j] < c1:
                            c1 = dp0[j]
                        j += d_val
                    if found:
                        candidate0 = c0
                        candidate1 = c1

                if candidate0 is not None:
                    dp0[i] = max(a_i, a_i + candidate0)
                    dp1[i] = min(a_i, a_i + candidate1)
                else:
                    dp0[i] = a_i
                    dp1[i] = a_i

                for d in range(1, S+1):
                    r = i % d
                    if dp1[i] > max1_list[d][r]:
                        max1_list[d][r] = dp1[i]
                    if dp0[i] < min0_list[d][r]:
                        min0_list[d][r] = dp0[i]

            # Output dp0[1] to dp0[n]

   We hope this works.

   Note: the worst-case memory for residue arrays per test case is 110 integers, and 300000 test cases: 33e6 integers, which is acceptable.

   And the total work: residue update: 10 * (total n) = 10 * 300000 = 3e6.
        large steps: sum_{test case} [ work for large steps in that test case ] 
            = sum_{test case} [ sum_{i: p_i > S} (number of multiples of p_i starting from i) ]
            = sum_{test case} [ sum_{d_val=S+1}^{n} (if d_val appears, then the work is O(n_i/d_val)) ]
            But note: in one test case, the work is: sum_{d_val=S+1}^{n} (n_i / d_val) = n_i * (H_{n_i} - H_S) 
            and the total over test cases: sum_{test case} [ n_i * (H_{n_i} - H_S) ]

        Since the sum of n_i is 300000, and the function f(n)=n*log(n) is convex, the worst-case is one test case with n_i=300000: 300000 * (H_{300000} - H_10) ≈ 300000 * (13-2.93) = 3.021e6.

        Then total work: 3e6 + 3.021e6 = 6.021e6, which is acceptable.

   Let's run the sample.

   We'll submit with S=10.

   Note: we must use sys.stdin for speed.

   We'll write the code accordingly.

   IMPORTANT: the modulus 0: for d=1, residue0: i%1 is 0 for every i. So we are safe.

   Let's hope.

   We'll run the provided samples.

   Sample 1: n=10, p=[3,1,5,2,4,9,6,10,8,7], a=[1,-2,3,-4,5,-6,7,-8,9,-10] -> output "8 7 3 -4 14 -6 7 -8 9 -10"

   Sample 2: n=4, p=[4,3,2,1], a=[3,2,3,3] -> output "3 2 3 3"

   We'll run locally.

   We'll code accordingly.

   Note: we use INF = 10**18, NEG_INF = -10**18.

   We'll write the code.