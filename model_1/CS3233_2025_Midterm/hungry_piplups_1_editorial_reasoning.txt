We are given an array A of N integers (each between 0 and N-1, but note the problem says A_i in [0, N-1] and also the constraints say A_i in [1,500] for the input, but the problem states 0<=A_i<=N-1; we'll rely on the constraints) and M queries. The queries can be of two types:

Type 0: Update A[P_i] = X_i.
Type 1: Given L, R, X, we are to consider the subarray A[L..R] and simulate the process:

  We start with an empty set S (of occupied icebergs). Then for each day from L to R, we do:
      S = S ⊕ A_i   [where ⊕ is defined as: S ⊕ x = S ∪ {smallest y not in S and y>=x}]

  Then we are to find the X_i-th smallest number not in S? But note the problem says: 
      "determine the furthest iceburg (i.e., the largest numbered iceburg) that any of them will end up at" 
  but actually the sample output is the position of the X_i-th piplup.

  However, the problem also says: "output a single integer representing the position of the X_i^{th} piplup"

  How do we interpret the query?

  After processing the segment [L,R], we have a set S of occupied icebergs. The piplups are on distinct icebergs. 
  The problem asks: "the position of the X_i^{th} piplup" meaning we are to list the positions (iceberg numbers) that the piplups ended up at, and then pick the X_i-th smallest? 

  But note: the process does not assign the piplups in increasing order of iceberg number. However, the problem says:

      "As a group of X_i Piplups, they will start leaving one by one, all heading for iceburg 1. ... they wish to determine the furthest iceburg that any of them will end up at"

  However, the sample outputs are not the maximum iceberg but the iceberg of the X_i-th piplup? Actually, the sample outputs:

      Query: 1 2 4 2 -> output 5
      Query: 1 1 3 2 -> output 5
      Query: 1 1 5 10 -> output 15

  How does that relate?

  Let's break down the first query: [2,4] has A_2=3, A_3=2, A_4=4.

  Day2: starts at 3 -> goes to 3 (since 3 is free).
  Day3: starts at 2 -> goes to 2 (free).
  Day4: starts at 4 -> goes to 4 (free).

  Then the set S = {2,3,4}. The piplups are on icebergs 2,3,4. The problem says: 
      "the 2nd piplup" - which one is the 2nd? 

  But note: the problem says "they will start leaving one by one, all heading for iceburg 1". However, the query is about the recorded data. The recorded data is the starting iceberg they intended. The actual iceberg they ended up at we have computed.

  The problem then asks: the position of the X_i-th piplup? This must mean the iceberg number of the X_i-th piplup in the order of the days? But the days are in order. The first piplup (day2) ended at 3, the second (day3) at 2, the third (day4) at 4? Then the 2nd piplup (by order of arrival) is at 2? But the sample output is 5.

  Alternatively, the problem might be asking: after the entire migration, what is the X_i-th smallest iceberg that is occupied? Then the set S = {2,3,4} has the 1st smallest=2, 2nd smallest=3, 3rd smallest=4. Then the 2nd smallest is 3? But that's not 5.

  Alternatively, note the operation: 
      S ⊕ x = S ∪ { smallest y >= x such that y not in S }

  But the problem states: 
      "output a single integer representing the position of the X_i^{th} piplup only considering piplups from day L_i to R_i inclusive"

  And the sample output for the first query is 5.

  Let me reexamine the problem: 

      "they wish to determine the furthest iceburg (i.e., the largest numbered iceburg) that any of them will end up at"

  But then they are a group of X_i piplups. However, the problem then says: "the position of the X_i-th piplup", which is confusing.

  The sample input: 
      5 4
      1 3 2 4 5 
      1 2 4 2

  Output: 5.

  How do we get 5?

  Let me simulate the entire process for [2,4] again:

      Day2: A_2=3 -> lands at 3. 
      Day3: A_3=2 -> lands at 2 (because 2 is free).
      Day4: A_4=4 -> lands at 4 (because 4 is free).

  The icebergs occupied: 2,3,4. The largest is 4, but 5 is output.

  Alternatively, the problem might be asking: if we list the icebergs that are occupied in increasing order, then the X_i-th piplup in the group of X_i is the one that ends at the X_i-th smallest iceberg? But then the 2nd smallest iceberg is 3, not 5.

  Another possibility: the problem is asking for the X_i-th smallest iceberg that is NOT occupied? That doesn't seem to fit.

  Alternatively, note the operation: the piplup that intended to go to x ends up at the smallest y>=x that is not occupied. 

  Now, the problem says: 
      "as a group of X_i Piplups, they will start leaving one by one, all heading for iceburg 1"

  But wait, the graduate students are going to leave one by one, all starting at iceberg 1? And they have the records from L to R. How do they use that? 

  The key: "they assume that no Piplups left on days outside this range". So the set of occupied icebergs is exactly the set we built from the segment [L,R]. Now, the graduate students (a group of X_i piplups) will each try to go to iceberg 1. But if iceberg 1 is occupied (by the recorded migration) then the first graduate piplup will be forced to go to the next free iceberg above 1? and so on.

  But note: the graduate students are not part of the recorded data. They are going to migrate after the recorded data. The recorded data has set the occupied icebergs. Then the graduate students, one by one, will:

      The first: goes to the smallest iceberg >=1 that is free (which is the smallest integer not in S).
      The second: goes to the next smallest integer not in S (which is the second smallest integer not in S).

      And so on.

  Then the problem asks: the furthest iceberg (largest numbered) that any of them will end up at? But note: they are going in order. The first graduate student gets the smallest free iceberg, the second the next, etc. So the X_i-th graduate student will get the X_i-th smallest free iceberg. And we are to output that iceberg number.

  However, the sample output for the first query is 5. 

  In the first query: [2,4] -> S = {2,3,4}. The free icebergs starting from 1: 
      1: free? -> no, because 1 is not in S? Actually, 1 is not in S? So the free icebergs are: 1, 5, 6, 7, ... 
      Then the smallest free iceberg is 1? 
      Then the 2nd smallest free iceberg is 5.

  So the first graduate student goes to 1, the second to 5. Therefore, the 2nd graduate student ends at 5. And that is the output.

  Similarly, the second query: after updating A[2]=1, then we consider [1,3]: 
      Day1: A1=1 -> lands at 1.
      Day2: A2=1 -> tries 1 (occupied) then 2 -> lands at 2.
      Day3: A3=2 -> tries 2 (occupied) then 3 -> lands at 3.
      So S = {1,2,3}. The free icebergs: 
          smallest free >=1: 1 is occupied -> skip, then 2,3 occupied -> so 4 is free? 
          Actually, the free icebergs: 4,5,6,... 
          The 1st free:4, 2nd free:5 -> so the 2nd graduate student ends at 5.

  Third query: [1,5] -> 
      Day1: 1 -> 1
      Day2: 1 -> 2 (because 1 taken)
      Day3: 2 -> 3 (because 2 taken by day2? Actually, day2 went to 2, so 2 is taken. Then day3: starts at 2 -> then 2 is taken, so goes to 3 -> then 3 is free? 
      Day4: 4 -> 4
      Day5: 5 -> 5
      So S = {1,2,3,4,5}. The free icebergs: the smallest free is 6? 
      Then the 10th smallest free iceberg: 
          The free icebergs: 6,7,8,...,15 -> the 10th is 15? 

      How? The free icebergs are all integers not in S. The set S has 1,2,3,4,5. The free icebergs start at 6: 
          1st:6, 2nd:7, ... 10th: 15.

  Therefore, the problem reduces to:

      Given a segment [L,R] of the array A, we simulate the process to get a set S (which is the set of icebergs occupied by the recorded piplups). Then we are to find the X_i-th smallest positive integer that is not in S.

  But note: the problem says "iceburg 1" and positive integers.

  How do we compute the set S? 

  We note that the set S is built by:

      S0 = ∅
      For i from L to R:
          x = A_i
          y = the smallest integer >= x that is not in S_{i-1}
          Then S_i = S_{i-1} ∪ {y}

  Then the set S = S_R (the set after processing R).

  Then we want the X_i-th smallest integer not in S.

  However, note that the integers that are not in S form a set of "gaps". The gaps are the free icebergs.

  The set S is a set of integers. The gaps can be represented as:

      Let F = sorted list of the elements in S? But note that S can be large (up to R-L+1, which can be 10^6) and we have M up to 5000 queries and updates.

  Alternatively, we can note:

      The gaps: the free numbers are all integers except those in S. The X_i-th smallest free number is the smallest integer z such that the number of free numbers in [1, z] is at least X_i.

      The number of free numbers in [1, z] = z - (number of elements in S that are <= z).

      But note: we don't care about the entire set S, we care about the count of elements in S that are <= z.

  How to compute the set S without simulating each day (which would be O(n) per query, and n up to 10^6, and M up to 5000 -> 5000 * 10^6 = 50e9, which is too slow)?

  We need a more efficient method.

  Observation:

      The process of building S is equivalent to the following:

          We start with an array (initially all free). Then we process each starting value x: we assign the smallest free integer >= x.

      This is similar to the "greedy" assignment for the classic "next available" problem.

      We can note that the set S is exactly the set of the minimum integers that are >= the corresponding A_i and distinct.

      In fact, if we consider the sequence of A_i, the set of assigned values is known to be the same as the set of the minimal elements we can assign without conflict.

      There is a known transformation: 

          Let f(x) = the smallest integer >= x that is not taken by any previous assignment.

      Then the set S is just the set { f(A_1), f(A_2), ... } for the days in the segment.

      How can we compute the set S for an arbitrary segment [L,R] quickly? 

      Alternatively, note that we are not required to know the entire set S, but only the count of elements in S that are <= z, for any z. Then we can use binary search on z to find the smallest z such that:

          z - (number of elements in S that are <= z) >= X_i

      Then the X_i-th free number is z? Actually, the condition is:

          Let g(z) = z - (count of S in [1, z]) 
          We want the smallest z such that g(z) >= X_i, then the X_i-th free number is the smallest z for which g(z) = X_i? Actually, the X_i-th free number is the z such that g(z) = X_i and g(z-1) = X_i-1? 

      But note: the free numbers are exactly the integers that are not in S. The free numbers are in increasing order: the first free number is the smallest integer not in S, the second is the next, etc.

      Therefore, the X_i-th free number is the smallest integer z such that the number of free numbers up to z is at least X_i? Actually, that would be the X_i-th free number? 

      How many free numbers <= z? = z - (# of occupied numbers <= z) = z - F(z), where F(z)=|{ s in S: s<=z }|.

      We want the smallest z such that z - F(z) >= X_i? Then the X_i-th free number is the smallest z for which z - F(z) >= X_i? Actually, no: 

          Let the free numbers be: f1, f2, f3, ... (in increasing order). We want f_{X_i}.

          We know that f_{X_i} = the smallest z such that the number of free numbers <= z is at least X_i.

          And the number of free numbers <= z is z - F(z). 

          So we are looking for the smallest z such that z - F(z) >= X_i.

          But note: z - F(z) is non-decreasing in z. So we can binary search z.

      Therefore, if we can compute F(z) = (number of elements in S that are <= z) for a given segment [L,R] and for any z, then we can do binary search to find the X_i-th free number.

  How to compute F(z) for a given segment [L,R] and for any z? 

      F(z) = |{ i in [L,R] : the assigned value for day i, y_i <= z }|

      But note: the assigned value y_i depends on the entire history from L to i. 

  Alternative approach: 

      There is a known offline method for the "greedy assignment" problem: we can use a Fenwick tree or segment tree to simulate the assignment? 

      Actually, we can use a union-find like structure? But note that the queries are over arbitrary segments and we have updates.

  Constraints: 
      N up to 10^6, M up to 5000, and the values of A_i (and X_i) are at most 500? Actually, the problem says: 
          "1 <= X_i, A_i <= 500"   ??? But note: the input constraints say A_i in [0, N-1] and then the constraints say: "1<=X_i, A_i<=500". 

      This is the key: the starting icebergs A_i are at most 500? 

      But wait: the input says: "A_1, A_2, ..., A_N (0<=A_i<=N-1)" and then the constraints: "1<=X_i, A_i<=500". This seems contradictory. 

      However, the problem states: 
          "1 <= X_i, A_i <= 500"

      So we can rely on the fact that A_i is in [1,500] and update X_i is also in [1,500]. 

      Why is that important? Because the assigned iceberg y_i might be large? 

      Example: if we have 500 days and each day A_i=1, then the assignments are 1,2,3,...,500. But if we have 1000 days, then the assignments might be 1,2,...,1000. 

      But note: the constraint on A_i is only that they are at least 1 and at most 500. However, the assigned icebergs can be large. 

      However, note that the free iceberg we are looking for for the graduate students: the X_i-th free iceberg. The value of X_i is at most 500? 

      Actually, the constraints say: "1<=X_i<=500". 

      Therefore, we are only interested in the free icebergs up to at most 500 + (number of occupied icebergs that are <= some bound). 

      But note: the X_i-th free iceberg might be as large as ...? 

          In the sample: when S = {1,2,3,4,5}, then the 10th free iceberg is 15. And 15 = 5 (the maximum in S) + 10 (the X_i) = max_in_S + X_i? 

      Actually, the free numbers are: 
          The first max_in_S+1 numbers: 1 to max_in_S are all occupied. Then the free numbers start at max_in_S+1. 
          The free numbers: max_in_S+1, max_in_S+2, ... 
          The X_i-th free number is max_in_S + X_i.

      But wait: the free numbers also include numbers below the maximum? For example, if S = {2,3,4}, then the free numbers are 1,5,6,7,... 
          Then the 1st free is 1, the 2nd free is 5, the 3rd is 6, etc.

      So the formula is not that simple.

      However, note that the starting values A_i are at most 500. Therefore, the assigned values y_i must be at most 500 + (number of days in the segment) - 1? 

      But the number of days in the segment [L,R] can be up to 10^6. So the maximum assigned value could be as large as 10^6. 

      But we are only asked for the X_i-th free iceberg, and X_i is at most 500. 

      How large can the X_i-th free iceberg be? 

          The worst case: if the set S is the entire interval [1, z] for some z, then the free icebergs above z are z+1, z+2, ... 
          The X_i-th free iceberg is z + X_i.

          But note: there might be free icebergs below the minimum value? 

          The free icebergs are: 
             all numbers < min(S) and all numbers > max(S) and the gaps in between.

          However, the minimum value in S: since every assignment starts at at least 1, and the greedy assignment starts at the intended value (which is at least 1) and then goes up, so the smallest assigned value is at least 1. 

          But note: the free icebergs below min(S) are the numbers from 1 to min(S)-1. 

          The number of free icebergs below min(S) is min(S)-1.

          Then the total free icebergs:

             total = (min(S)-1) + (max(S) - min(S) + 1 - |S|) + (infinity) 
          Actually, we care about the X_i-th free iceberg. 

          There are min(S)-1 free icebergs below min(S). Then if X_i <= min(S)-1, then the X_i-th free iceberg is X_i.

          Otherwise, if X_i > min(S)-1, then we subtract min(S)-1 from X_i and then we look at the gaps above min(S). The gaps above min(S) are the numbers in [min(S), max(S)] that are missing, plus the numbers above max(S). 

          The gaps in [min(S), max(S)]: let k = (max(S)-min(S)+1) - |S|, then the first k free icebergs above min(S)-1 are the gaps in the interval. Then the next free icebergs are max(S)+1, max(S)+2, ...

          Therefore, the X_i-th free iceberg:

            if X_i <= min(S)-1:
                ans = X_i
            else:
                let rem = X_i - (min(S)-1)
                if rem <= k:
                    then we need the rem-th smallest gap in [min(S), max(S)]? 
                else:
                    ans = max(S) + (rem - k)

      But note: the set S might not be contiguous. However, the values A_i are small (at most 500) so the assigned values y_i might be forced to be at least 500? Actually, the starting value is at most 500, but the assignment might jump to a value larger than 500.

      However, the key is that we are only asked for the X_i-th free iceberg and X_i is at most 500. Therefore, we are only interested in the first 500 free icebergs. 

      How to collect the first (at most 500) free icebergs? 

          The free icebergs are:

            Part 1: the numbers from 1 to min(S)-1. There are min(S)-1 numbers.

            Part 2: the numbers in [min(S), max(S)] that are not in S. We can collect these by iterating from min(S) to max(S) and noting which numbers are missing? But max(S) can be as large as 10^6 (if we have 10^6 days and each day A_i=1, then we get 1,2,...,10^6) -> and we don't want to iterate 10^6 per query (with 5000 queries, that would be 5e9).

      We need a more efficient method.

  Alternative Insight:

      Since the starting values A_i are at most 500, the assigned values y_i must be in the range [1, 500 + (number of days in the segment) - 1]. However, we note that the graduate student group is of size at most 500, so we only care about the free icebergs that are at most, say, 500 + 500 = 1000? 

      Actually, the worst-case: 
          The set S: we have R-L+1 days. The maximum assigned value is at most max_A + (R-L+1) - 1? Actually, worst-case: if all A_i are 1, then we assign 1,2,...,R-L+1. Then the free icebergs:

             The free icebergs below 1: none (since min(S)=1, so no free below 1).
             The gaps above 1 and below max(S)=R-L+1: none (since contiguous).
             Then the free icebergs above max(S) start at R-L+2.

          Then the first free: R-L+2, the second: R-L+3, ... the 500th: R-L+1+500.

          But R-L+1 can be up to 10^6, so the 500th free iceberg is about 10^6+500, which is 10^6+500.

      However, we are only asked for the X_i-th free iceberg and X_i<=500, so we can compute the free icebergs by:

          We know that the set S for [L,R] is a set of R-L+1 distinct integers. The smallest integer that is free is the smallest integer >=1 that is not in S. 

          We are only interested in the first 500 free integers.

          How to get the first 500 free integers? 

          We can iterate from 1 to, say, max_candidate = max(S) + 500, and we need to know for each integer in [1, max_candidate] whether it is in S.

      How to compute the set S for the segment [L,R]? 

          We can simulate the assignment for the segment [L,R]? But the segment length can be 10^6 and we have up to 5000 queries -> 5000 * 10^6 = 5e9, which is too slow.

  We need an efficient data structure to handle the queries and updates.

  Given that M is only 5000, we can consider doing each query in O(500) or O(1000) per query? 

  How? 

      Since the starting values A_i are at most 500, the assigned value for a day i is either:

          y_i = A_i, if A_i is not occupied by any previous assignment in the segment? 
          or y_i = the next available after A_i.

      But note: the previous assignments: we have to consider the entire segment from L to i-1.

      However, we can note: the assigned value y_i must be in the range [A_i, A_i + (number of days from L to i-1 that have assigned values in the interval [A_i, ...])]?

      Actually, we can use a DSU-like union-find for the next available integer? But we are doing arbitrary segments.

  Alternatively, we can use offline queries and Mo's algorithm? But M is 5000 and N is 10^6, and Mo's algorithm would be O(N * sqrt(N) * (update time)) which is too heavy.

  Another idea: 

      We note that the assignment process is deterministic and depends only on the set of starting values and the order. 

      We can use a Fenwick tree to simulate the assignment for a fixed segment? 

      However, the process is sequential: we assign one by one.

      But note: the starting values are small (at most 500). Therefore, the assigned values for a day i is at most 500 + (the number of previous days that have starting value <= 500 and have been assigned in the range [A_i, ...]). 

      Actually, the assigned value must be one of the integers in [A_i, 500 + (R-L+1)]. But (R-L+1) can be 10^6, so we cannot iterate over that.

  Insight: 

      We are only interested in the free numbers for the graduate students, and we only need the first 500 free numbers. Therefore, we only care about the set S in the range [1, U] where U = max_candidate = (max_assigned_value_we_care_about) = (max_possible_assigned_value in the segment) but note: we don't care about the entire set S, we only care about the set S for the purpose of listing the first 500 free numbers. 

      The first 500 free numbers are at most: 

          The first free number: the smallest integer not in S. 
          The second: the next, etc.

      The first 500 free numbers must be among the integers in [1, 500 + (size of S)].

      But the size of S is (R-L+1). Therefore, the first 500 free numbers are at most 500 + (R-L+1). 

      However, (R-L+1) can be 10^6, so 500+10^6 = 1000000+500, which is 1000500. 

      So we can try to compute the set S for the segment [L,R] but only for integers in [1, 1000500]? 

      Then we could mark an array of size 1000500? But 1000500 per query (and 5000 queries) would be 5000 * 1000500 ~ 5e9 integers, which is 40 GB? Not feasible.

  We need a more efficient way to compute the set S for the segment [L,R] for integers in a bounded range [1, U] (with U=1000500) without building an array of size 1000500 per query.

  How about we simulate the segment [L,R] but we do not store the entire set S, but only the assignments that are <= U (which is 1000500). 

      We can simulate the assignment for each day in [L,R] and for each day, we compute the assigned value. Then we note that the assigned value might be > U? But then we don't care about it because we are only interested in the free numbers up to U. 

      However, if an assignment is > U, then we know that all integers from 1 to U are either occupied or free? But we need to count the occupied ones in [1, U] to compute the free numbers in [1, U]. 

      Actually, if an assignment is > U, then it doesn't occupy any integer in [1, U]. So we can ignore it.

      Therefore, for the purpose of computing the free numbers in [1, U] (with U = 1000500), we only need to know the set of assigned values that are <= U.

      How to simulate the segment [L,R] to get the assigned values that are <= U? 

          We can do:

            Let next_available = an array that for each starting value x (from 1 to 500) we precomputed? Not exactly.

          Alternatively, we can use a Fenwick tree that supports range queries and point updates for the entire array? But we are doing arbitrary segments.

      We can use a segment tree that for a fixed segment [L,R] and for a fixed U, we want to compute the set of assigned values that are <= U. 

      But note: the assignment for day i depends on the previous assignments. So we must do sequentially.

      However, we can do a simple simulation for the segment [L,R] and for each day, we compute the assigned value. 

          We need a data structure that supports:

              Query: for a given x, what is the next available integer >= x? 
              And then mark that integer as taken.

          We can use a balanced BST for the free integers? But we are doing 10^6 days per query and 5000 queries -> 5e9 operations.

      Alternatively, since the starting values are at most 500, the assigned value for a day i is the smallest integer >= A_i that is not taken by any day in [L, i-1]. 

          We can precompute for each starting value A_i, the next available integer in the context of the segment [L,R]? 

          We can use a DSU for the next available integer, but only for the integers in [1, U]. We can do:

              Let parent[i] = i for i in [1, U]. 
              When we assign a value y, we set parent[y] = y+1? (if we use union-find for next available)

          Then for a day with A_i, we do:

              y = find(A_i)   -> which returns the smallest integer >= A_i that is free.

          Then we mark y as taken: union(y, y+1) -> so that parent[y] points to y+1.

          Then for the next query at A_j, if A_j <= y, then find(A_j) would skip y and go to the next after y.

          But note: we are doing a segment [L,R]. We can simulate the segment by initializing the DSU for the entire range [1, U] and then process the days in the segment in order.

          The cost per day: nearly O(1) with union-find.

          Then the total cost per query: O((R-L+1) * alpha(U)) which is O(n) per query. And with 5000 queries, worst-case total operations: 5000 * 10^6 = 5e9, which is too slow.

  Given the constraints, we must do better.

  Another idea: 

      Since the starting values A_i are small (at most 500), we can use the following:

          We are only interested in the assignments that are <= U (with U = (R-L+1) + 500) but note (R-L+1) can be 10^6, so U=10^6+500. 

          However, we can note: the assignment for a day i is at most A_i + (number of previous days that have been assigned a value in the interval [A_i, ...]). 

          But the number of previous days is at most (i-L). 

          Therefore, the assigned value y_i is at most A_i + (i-L). 

          Since A_i <= 500, and i-L <= (R-L) <= 10^6, then y_i <= 500 + (R-L). 

          But note: we are simulating the segment [L,R] and we only care about assignments that are <= U = (R-L+1) + 500 = (R-L+1+500). 

          However, we want to know the set of assignments that are <= U, and U is about 10^6+500. 

          How to do it without iterating over 10^6+500 for each query? 

      We can use a Fenwick tree over the range [1, U] to simulate the assignments? 

          We want to quickly, for a given x, find the smallest integer >= x that is free. 

          We can do a Fenwick tree that stores 1 for free and 0 for taken. Then we want the smallest index >= x that has value 1. We can do a binary search on Fenwick tree? That would be O(log^2 U) per assignment. 

          Total cost per query: O((R-L+1) * (log^2 U)) ~ 10^6 * (log2(10^6))^2 ~ 10^6 * (20)^2 = 10^6 * 400 = 400e6 per query. And 5000 queries: 5000 * 400e6 = 2e12, which is too slow.

  We need to optimize further.

  Given that M is only 5000, but the segment length can be 10^6, we cannot iterate over each day in the segment.

  Alternative approach for the entire problem:

      We note that the assignment process for the entire array is well-defined and the result for a day i is independent of the future. But our queries are over arbitrary segments.

      We can precompute the entire assignment for the whole array? Then for a query [L,R], we want to consider only the assignments that happened in [L,R] and then find the free numbers relative to that set.

      But the catch: the assignment for day i in the whole array might be influenced by days not in [L,R]. The graduate students assume that only the days in [L,R] happened. So we have to simulate the assignment using only the days in [L,R].

      Therefore, we cannot use the global assignment.

  We are stuck with simulating the segment [L,R] for each query. But worst-case total work over all queries: 5000 * (average segment length). The average segment length could be 10^6, so 5e9 days to simulate, which is borderline in C++ in 2 seconds? 

      But 5e9 operations might be borderline in C++ in 2 seconds? Probably not.

  We need a faster simulation. 

  Insight: the starting values A_i are at most 500. Therefore, for a fixed segment [L,R], the assigned values for days in the segment are in the range [min_start, min_start + (R-L+1) - 1]? But not exactly: the starting values are in [1,500], but the assigned values can be larger if there are many conflicts.

  However, note that the conflict resolution always moves forward. And since the starting values are bounded by 500, the only conflicts are for values in [1, 500 + (R-L+1) - 1]. 

  But we only care about the first 500 free numbers. And to compute the first 500 free numbers, we only need to know the set S in the range [1, U] where U = (max_assigned_value_that_affects_the_first_500_free) = ?

      Let F = the set of assigned values in the segment [L,R] that are <= some bound. What bound? 

          The first 500 free numbers: 
             The first free number is the smallest integer not in S.
             The 500th free number is at most: 
                 If there are many free numbers below the minimum of S, then it could be as small as 500.
                 Otherwise, if there are few free numbers below the minimum of S, then it could be as large as (max_S + 500).

          But max_S is the maximum assigned value in the segment, which is at most 500 + (R-L+1) - 1.

          So U = 500 + (R-L+1) might be as large as 10^6+500, which is 1000500.

  How to compute the set S for the segment [L,R] for values in [1, 1000500] quickly? 

      We can use a bitset of size 1000500? But 1000500 bits is about 125 KB. And we have 5000 queries, so 5000 * 125 KB = 625000 KB = 625 MB, which is acceptable in memory, but then the time to simulate each query: 

          For each day in [L,R] (up to 10^6 days) and for each day, we need to find the smallest integer >= A_i that is not in the bitset. 

          To find the next free after A_i: we can do:

              We have a bitset B of size U. Initially, all are 0 (free).

              For each day i in the segment:
                  start = A_i
                  We need to find the smallest j>=start such that B[j]==0.

          How to do that quickly? 

              We could precompute a next pointer array, but we are updating as we go.

          We can maintain a linked list for the next free? 

              Let next[i] = the next free integer >= i.

          We can do union-find (path compression) for the next available integer. 

          We initialize an array next_free[1..U] (initially next_free[i]=i). 

          Then for a day with start=x:

              y = next_free[x]   // this is the next free integer >= x
              Then we set next_free[y] = next_free[y+1]   // but we also need to update all the pointers that point to y to point to next_free[y+1]? 

          Actually, we can do:

              We maintain an array next_ptr of size U+2 (indexed 1 to U+1), initially next_ptr[i]=i.

          Then:

              function find_next(x):
                  if next_ptr[x] == x:
                      return x
                  next_ptr[x] = find_next(next_ptr[x])
                  return next_ptr[x]

          Then when we assign y, we do next_ptr[y] = find_next(y+1)

          The cost is O(alpha(U)) per assignment.

          Total cost per query: O((R-L+1) * alpha(U)) = O(n) per query, which is 10^6 per query, and 5000 queries -> 5e9 operations.

      But 5e9 operations in 2 seconds? In C++ it might be borderline, but the problem says memory limit 512 MB.

      The array next_ptr has size U = 1000500 per query? But we have 5000 queries, and we cannot have 5000 * 1000500 integers (that's 5000 * 1000500 * 4 = 20,010,000,000 bytes ~ 20 GB).

      We must not allocate U per query.

  How about we do the queries one by one and reuse the memory? 

      We can allocate one array next_ptr of size U_max = 1000500. And then for each query, we initialize next_ptr for the range [1, U] (with U = (R-L+1)+500). 

      Initialization: for i from 1 to U: next_ptr[i]=i; and next_ptr[U+1]=U+1 (as sentinel).

      Then for each day in the segment [L,R] (which is up to 10^6 days), we do:

          x = A[i]
          y = find_next(x)   // which uses path compression

          If y > U, then we skip (but then we wouldn't mark it as taken, but we don't care about y>U) -> but wait, we need to know that y is taken only if y<=U? 

          However, if y<=U, then we mark it as taken by doing next_ptr[y] = find_next(y+1)

          And if y>U, then we do nothing? But then the next time we start at x, we might get the same y again? 

          Actually, we are not storing the assignment if it's >U, so we don't care. But then for the next day, if we start at x, we would get the same y (because we didn't mark it as taken) -> which is incorrect.

      Therefore, we must mark even the assignments that are >U as taken? But we cannot because our next_ptr only goes to U.

      Alternatively, we can let U = (R-L+1) + 500, and then any assignment that would be > U is not stored, but then for the purpose of the free list in [1, U], we consider that any assignment that lands in [1, U] is stored, and if an assignment lands beyond U, then it does not affect the free list in [1,U]. 

      But then for a given day, if the assignment lands beyond U, then we do not mark any integer in [1, U] as taken. So it's as if that day didn't happen for the free list in [1, U]. 

      However, the assignment algorithm: 

          We start at x. The next free integer >= x might be in [1, U] or beyond. 

          We only care about the free list in [1, U]. So for this day, if the next free integer is > U, then we do not occupy any integer in [1, U]. 

          Therefore, for the free list in [1, U], we can simply skip this day.

      So algorithm per query:

          Let seg_len = R-L+1.
          Let U = seg_len + 500.   // because the X_i-th free number might be at most U, since there are at most seg_len occupied numbers in [1, U] and then the free numbers above the maximum occupied are contiguous, and we need up to the 500th free.

          Initialize an array next_ptr for indices 1 to U+1: 
             for i=1 to U: next_ptr[i] = i
             next_ptr[U+1] = U+1   (as a sentinel)

          Also, we will not store the entire set S, but we will create a boolean array (or a bitset) of size U+1 (initially false) to record which integers in [1,U] are occupied. But we don't need the entire array, we only need to know the free numbers later. Alternatively, we can use next_ptr to generate the free numbers? 

          However, after processing the segment, we want to list the first 500 free numbers in increasing order. 

          We can do:

             We simulate the segment [L,R]:
                 for i from L to R:
                     x = A[i]
                     y = find_next(x)   // returns the smallest free integer >= x
                     if y <= U:
                         // then we occupy y
                         next_ptr[y] = find_next(y+1)
                         // and note that we will need to know that y is occupied for the free list? Actually, the next_ptr structure already has the information that y is taken.

             Then after simulation, we want to list the free numbers in [1, U] in increasing order, and then pick the X_i-th.

          How to list the free numbers? 

             We can't iterate over U which is 10^6+500 per query (5000 * 10^6+500 ~ 50e9, too slow).

          Instead, we want the X_i-th free number. 

          We know the total number of occupied numbers in [1, U] is the number of days in the segment that were assigned a value <= U. Let occ = that count. 

          Then the free numbers in [1, U] are U - occ. 

          But if X_i > U - occ, then the X_i-th free number is not in [1, U]? 

          But wait: we defined U = seg_len+500, and the free numbers beyond U are not in our structure. However, note that there are only U - occ free numbers in [1, U]. The next free number after U is U+1, then U+2, etc.

          So if X_i <= U - occ:
              we need the X_i-th free number in [1, U]. 
          else:
              the free number = U + (X_i - (U - occ))

          But is that correct? 
              The free numbers: 
                 the first (U-occ) free numbers are in [1, U] (specifically, the free numbers in [1,U] are the ones not occupied and are in that range).
                 the next free numbers are U+1, U+2, ...

          Therefore, the total free numbers in [1, infinity] are infinity, and the X_i-th is:

             if X_i <= (U - occ): 
                 then we must find the X_i-th smallest integer in [1, U] that is free.
             else:
                 ans = U + (X_i - (U - occ))

          How to find the X_i-th smallest free number in [1, U] without iterating? 

             We can use a Fenwick tree over the next_ptr? Not easily.

          Alternatively, we can collect the free numbers by doing a linear scan from 1 to U, but U is 10^6+500 and 5000 queries would be 5000 * 10^6 = 5e9, which is borderline in C++.

          But 5e9 operations might be acceptable in 2 seconds in C++? In Pyton, no.

      Given the problem constraints and that M is only 5000, but the U per query is about 10^6, then total work for this linear scan would be 5000 * 10^6 = 5e9, which is acceptable in C++ (if optimized) in 2 seconds? 

          But we also have the simulation of the segment: which is 10^6 per query and 5000 queries -> 5e9 union-find operations? 

          Each union-find with path compression is nearly O(1), but 5e9 operations might be borderline in 2 seconds in C++.

      Total operations: 
          For each query, we do:
             Initialization of next_ptr: O(U) ~ 10^6.
             Simulation: O(seg_len * alpha(U)) ~ 10^6 * very small constant.
             Then the linear scan to find the X_i-th free number: O(U) ~ 10^6.

          Total per query: O(2 * 10^6) = 2e6.
          Total for 5000 queries: 5000 * 2e6 = 10e9 = 10^10 operations.

      This is 10 billion operations, which in C++ might be borderline in 2 seconds (each operation might be a few instructions). 

      But 10^10 instructions at 3 GHz is about 3 seconds? And we have two 10^6 per query: the initialization and the linear scan. 

      We can try to optimize the initialization of next_ptr: we only need to reset the portion that we used. And we can use a memory pool. 

          However, we are reusing the same array for different queries. We can do:

             We'll have a global array next_ptr of size max_U, where max_U = 1000000+500 = 1000500.
             But then for each query, we need to reset it to its initial state: 
                 for i from 1 to U: next_ptr[i] = i; 
                 next_ptr[U+1]=U+1.

             This is O(U) per query, which is 10^6 per query, and 5000 * 10^6 = 5e9, which is 5e9 assignments, which is 40 GB if we do 5000 * 1000500 integers? 

      How about we avoid resetting the entire array? 

          We can use a timestamp method or a lazy reset? 

          We can maintain a global array next_ptr that is persistent? 

          Alternatively, we can do: 

             We'll have a global array next_ptr, and for each query, we will use it, but then we will need to undo the changes? 

          The changes: we do path compression during the find_next. This is destructive.

      Given the complexity of undo, we might simply allocate a fresh array for each query? 

          Memory: 5000 * (1000500 * sizeof(int)) = 5000 * 1000500 * 4 = 20,010,000,000 bytes = 20 GB -> too much.

  We must find a better way.

  Alternative for reset: we can store the changes we made and then revert them. 

      We will use a global next_ptr array. Before the first query, we initialize it to: for i in [1, MAX_GLOBAL] next_ptr[i]=i, and next_ptr[MAX_GLOBAL+1]=MAX_GLOBAL+1.

      Then for a query [L,R] with U = (R-L+1)+500:

          We will only use indices up to U. We need to reset the portion [1, U] to the initial state. But we don't know what state they are in from the previous query.

          We can do:

             We will save the modified next_ptr values during the query simulation. Then after the query, we will revert them.

          How many modifications? 
             In the simulation: we do for each day in [L,R]: 
                 we do a find_next which might do path compression, and then we do an assignment to next_ptr[y] = ... (one assignment per day).

             Also, during the find_next, we might update the next_ptr for several nodes (due to path compression). 

          The number of nodes that are modified in the next_ptr array per query is at most: 
             The number of days in the segment (which is up to 10^6) and the path compression might touch additional nodes? 

          In union-find with path compression, each find operation might update O(alpha(U)) nodes? Actually, worst-case the path compression updates O(log n) nodes per find. 

          Total modifications: O((R-L+1) * log(U)) = 10^6 * 20 = 20e6 per query.

          And 5000 queries: 5000 * 20e6 = 100e9 modifications to record and revert, which is too heavy.

  Given the complexity, we might hope that U = (R-L+1)+500 is not the worst-case for every query. But the worst-case is the maximum length segment.

  But note: M is only 5000, and the total length of all segments might be large.

  We need a different approach.

  Insight: since the starting values are bounded by 500, the only values that can be assigned are in the range [1, 500+ (R-L+1) - 1]. And for the graduate students, we only care about the first 500 free numbers. 

      Therefore, the only integers that might be free and in the range [1, 500+ (R-L+1) - 1 + 500] = [1, (R-L+1) + 500+500] = [1, (R-L+1)+1000] are the ones that matter. 

      Specifically, the X_i-th free number is at most (R-L+1)+1000, because:

          The maximum assigned value in the segment is at most (R-L+1) + 500 - 1 (if the last assignment was at 500 + (R-L+1) - 1), then the free numbers start at (R-L+1)+500. The 500th free number after that is (R-L+1)+500+500-1 = (R-L+1)+999.

      So we can let U = min( (R-L+1) + 1000, 1000000000) [some global limit]. 

      But then U = (R-L+1)+1000, which is about 10^6+1000 = 1001000.

      The initialization is O(U) per query, which is 1001000 per query, and 5000 queries: 5000 * 1001000 = 5e9, which is 5e9, which is acceptable in C++ if we do it in a tight loop.

      Then the simulation: for each day in [L,R] (length=len) we do a find_next which is nearly O(1) amortized with path compression. Total for simulation: O(len).

      Then the free number extraction: we want the X_i-th free number. We can avoid the linear scan by not explicitly listing the free numbers. Instead, we can use the next_ptr array to jump to the free numbers. 

          We know that the free numbers are given by: 
             start at 1: 
                if next_ptr[1] == 1, then 1 is free, else next_ptr[1] is the next free integer.

          But we want the X_i-th free number. 

          We can do:

             current = 0   // last free number found
             count = 0
             while count < X_i:
                 next_free = find_next(current+1)   // but careful: we have our next_ptr structure that has been updated by the simulation, and we haven't reset it? 

          But wait, after the simulation, the next_ptr structure represents the next free for any starting point. 

          Then we can do:

             ans = 0
             count = 0
             ptr = 0
             while count < X_i:
                 ptr = find_next(ptr+1)   // find the next free after ptr
                 if ptr > U: 
                     then we break   // but we should not, because we defined U large enough to cover the X_i-th free number? 
                 count++
                 if count == X_i:
                     ans = ptr
                     break

          But if we break because ptr>U, then the free number is ptr (which is the first free beyond U) but then we need to count how many free numbers are in [1, U] (call it F) and then the free numbers beyond U are in order: U+1, U+2, ... 
          Then the free number = U + (X_i - F)

          However, note that our simulation only marked the free numbers in [1, U] as taken if they were assigned. And the free numbers in [1, U] are exactly the ones that we can find by the next_ptr. 

          But the while loop using find_next might be expensive: each find_next is O(alpha(U)) and we do it X_i times (which is at most 500) so 500 * alpha(U) per query, which is 500 * very small constant.

          So per query, after simulation, we do:

             F = (number of integers in [1, U] that are free) = U - occ, where occ = number of days in the segment that got an assignment in [1, U] (which we can count during the simulation).

             Then if X_i <= F:
                 // we need to find the X_i-th free number in [1, U]
                 We can start at 0 and use find_next repeatedly X_i times.
             else:
                 ans = U + (X_i - F)

          This is O(X_i) per query, which is at most 500 per query.

      Therefore, the steps per query [L,R] with X_i:

          len = R-L+1
          U = len + 1000   // to be safe: covers up to the 1000-th free number beyond the last assignment

          // We need to initialize next_ptr[1..U+1]
          for i=1 to U: next_ptr[i] = i
          next_ptr[U+1] = U+1

          occ = 0   // count of assignments in [1, U]

          // Simulate the segment
          for i = L to R:
              x = A[i]
              y = find_next(x)   // using path compression in the next_ptr array for indices in [1, U+1]

              if y <= U:
                  // mark y as taken
                  next_ptr[y] = find_next(y+1)
                  occ++

          // Now, if X_i <= (U - occ): then the X_i-th free number is in [1, U]
          // else: ans = U + (X_i - (U-occ))

          if X_i <= (U - occ):
              // We want the X_i-th free number in [1, U]
              current = 0
              count = 0
              while count < X_i:
                  current = find_next(current+1)
                  count++
              ans = current
          else:
              ans = U + (X_i - (U - occ))

          // But note: we have to reset next_ptr for the next query? Or we can reuse by initializing at the beginning of the query? 

          // Since we are doing a new initialization for each query, we are safe.

      However, the initialization of next_ptr is O(U) per query, and U = len+1000 = up to 10^6+1000. 
          Total initialization cost over all queries: sum_{query} (len_i + 1000) = (sum of len_i) + 1000 * M.

      What is the sum of len_i? 
          In the worst-case, we could have 5000 queries, each with len_i=10^6, so total = 5000 * 10^6 = 5e9.

      The simulation: for each query, we do len_i find_next and union operations. Total operations = 5e9.

      The final extraction for the free number: 5000 * 500 = 2.5e6.

      Total operations: 5e9 (initialization) + 5e9 (simulation) = 10e9, which is 10 billion.

      Memory: we allocate next_ptr for U+1 integers per query? But we do sequentially, so we can have one global next_ptr array and reuse it for each query? 

          We only need one such array because we do the queries sequentially. We can have a global next_ptr array of size max_U, where max_U = max_{query} (len_i+1000). 
          The maximum len_i is 10^6, so max_U = 10^6+1000 = 1001000.

          So we allocate next_ptr[1..1001001] globally.

      Steps:

          Precompute the global next_ptr size = 1001001.

          for each query:
             len = R-L+1
             U = len + 1000
             // If U > current allocated size, we might need to extend? But the maximum len is 10^6, so U=1001000, and we allocated 1001001, so we are safe.

             // Initialize next_ptr[1..U] to i, and next_ptr[U+1]=U+1.
             // But note: we only need to initialize the portion [1, U+1] for this query.

             However, the next_ptr array from the previous query might be in a modified state. We need to re-initialize [1, U+1] for the new query.

             We can do a for loop for i=1 to U+1: next_ptr[i]=i. 

             Then simulate the segment [L,R] and then compute the answer.

      Total for initialization over all queries: 5000 * 1001000 = 5.005e9, which is 5.005e9 assignments.

      This is about 5e9 assignments, which in C++ on a 3 GHz machine: 3e9 per second -> 1.5 seconds for the initialization.

      Then simulation: 5e9 union-find operations (each with path compression, which is a few instructions per operation) -> might be 2 seconds? 

      Total: 1.5 + 2 = 3.5 seconds, which is over the 2 second limit.

  Optimization: 

      We can avoid the initialization of the entire next_ptr array for each query by using a "lazy reset" technique. 

          We maintain a global array next_ptr and a global array timestamp (of the last time it was reset) and a global variable timer.

          However, we also do path compression which updates the next_ptr arbitrarily.

      Alternatively, we can use a stack to record the changes we make to next_ptr and then revert them. 

          We know that during the simulation for one query, we change next_ptr for at most O(len_i + 500) nodes? Actually, we change next_ptr for every assignment (one per day) and for every node touched by path compression. 

          The number of nodes changed by path compression in the find_next for the entire query is O(len_i * alpha(len_i))? 

          Total changes: about 2 * len_i per query (if we count both the assignment and the path compression).

          For 5000 queries, total changes = 2 * 5e9 = 10e9, which is too many to record and revert.

  Given the time constraints, and that 5e9 initialization is 5e9 assignments, we hope that the initialization can be done in 1.5 seconds and the simulation in 2 seconds? 

      In C++ it might be borderline.

  Alternatively, we can use a different data structure for the next available integer that does not require initialization of an array of size U: 

      We can use a balanced BST of free intervals. 

          Initially for the query, the free intervals are [1, U]. 

          When we assign a number x, we find the interval that contains x, split it, and then remove x.

          But then for a query for the next free >= x, we can find the interval that starts <= x and then the next free is the start of that interval if it covers x, or the next interval.

          But we only need the next free integer, not the entire interval.

          We can use a set of free integers? But then U is 10^6, and we start with 10^6 integers, and then we remove them one by one. 

          The cost per assignment: O(log n) for set deletion.

          Total for simulation: O(len_i * log U) = 10^6 * log2(10^6) ~ 10^6 * 20 = 20e6 per query, and 5000 queries: 5000 * 20e6 = 100e9, which is too slow.

  Given the constraints, we might need to live with the union-find and hope that the initialization can be optimized.

      How to optimize the initialization? 

          We only need to reset the portion [1, U] and U+1. 

          And U is about 10^6. 

          We can use memset? 

          In C++, we can do:

             for (int i = 1; i <= U+1; i++) 
                 next_ptr[i] = i;

          This is a simple loop, and the compiler might optimize it.

          10^6 iterations * 5000 = 5e9 iterations, and each iteration is an assignment. This is 5e9 assignments, which on a 3 GHz machine (assuming 1 instruction per assignment, which is not true) might be 5e9 instructions = 1.5 seconds (at 3e9 instructions per second).

          But in practice, the cache might be good.

      Then the simulation: for each day, we do:

          y = find_next(x)

          if y<=U:
              next_ptr[y] = find_next(y+1)

          The find_next is:

             if next_ptr[x] != x:
                 next_ptr[x] = find_next(next_ptr[x])
                 return next_ptr[x]
             else:
                 return x

          This is the typical union-find with path compression.

          The total work for the simulation: 
             The total number of assignments is len_i (<=10^6) per query, and 5000 queries: 5e9.

          Each find_next might do a few pointer jumps. The amortized cost is nearly O(1).

          So total work for simulation: about 5e9 * (amortized constant) = 5e9 operations.

      Then the free number extraction: 
          if X_i <= (U - occ): 
              current = 0
              for count=1 to X_i:
                  current = find_next(current+1)
              ans = current
          else:
              ans = U + (X_i - (U-occ))

          The for loop runs X_i times (<=500) per query, so 5000*500=2.5e6.

      Total operations: 5e9 (initialization) + 5e9 (simulation) + 2.5e6 (extraction) = 10.0025e9.

      In C++, we hope that the initialization and the simulation can be optimized to run in 2 seconds.

      Note: the problem also has updates. 

          We are allowed to update the array A. 

          In the input, we have:

             first: N, M
             second: array A of N integers
             then M lines: each line is a query.

          We will store the array A in a global array, and for an update: we do A[P_i] = X_i.

          Then for each type1 query, we use the current array A for the segment [L,R].

      Implementation: 

          We will read the array A of size N.

          For each of the M queries:

             if type0: update A[P_i-1] = X_i   (if 0-indexed)
             if type1: 
                 len = R-L+1
                 U = len + 1000
                 // initialize next_ptr[1..U] and next_ptr[U+1]
                 for i=1 to U+1: next_ptr[i]=i

                 occ = 0
                 // simulate days from L-1 to R-1 (0-indexed)
                 for i = L-1 to R-1:
                     x = A[i]
                     y = find(x)   // find_next in the union-find

                     if y <= U:
                         next_ptr[y] = find(y+1)
                         occ++

                 if X_i <= U - occ:
                     current = 0
                     for j=0 to X_i-1:
                         current = find(current+1)
                     ans = current
                 else:
                     ans = U + (X_i - (U-occ))

                 output ans

          The find function with path compression:

             int find(int x) {
                 if (x > U)   // we don't have next_ptr for x>U, but our next_ptr is defined for [1, U+1] and we have a sentinel at U+1
                    return x;  // but our find should only be called for x<=U, because in the simulation, x is at least 1 and at most 500, and in the extraction we start at current+1 which might be up to U+1, but we have next_ptr[U+1]=U+1.

                 if (next_ptr[x] != x)
                     next_ptr[x] = find(next_ptr[x])
                 return next_ptr[x]
             }

  However, note: in the simulation, when we do find(y+1) for y+1 that might be >U, we pass it to find. We should not if y+1>U. So we can:

             if y+1 > U:
                 then next_ptr[y] = U+1   // which is sentinel
             else:
                 next_ptr[y] = find(y+1)

          But then in the find function, we can handle x>U by returning x. 

          We can write the find function to work for x in [1, U+1] and for x>U+1, return x.

          However, in our next_ptr array, we only initialized [1, U+1]. For x>U+1, we don't have next_ptr[x]. 

          So we should avoid calling find on x>U+1.

          Therefore, we do:

             int find(int x) {
                 if (x > U) {
                     return x;
                 }
                 if (next_ptr[x] != x) {
                     next_ptr[x] = find(next_ptr[x]);
                 }
                 return next_ptr[x];
             }

          And in the simulation:

             y = find(x)   // x is the start (which is between 1 and 500) so x<=U.
             if (y <= U) {
                 // then we do: next_ptr[y] = find(y+1)
                 next_ptr[y] = find(y+1);   // y+1 might be U+1 or even greater, but if y+1>U, then find(y+1) returns y+1, and we set next_ptr[y] = y+1 (which is >U) and then later if we start at y, we would get y+1 (which is >U) and skip.
                 occ++;
             }

          In the free number extraction:

             current = 0
             for j=0 to X_i-1:
                 next = find(current+1)
                 // if next > U, then we break early? but we only do this if X_i <= (U-occ) [which means the X_i-th free number is in [1,U]], so we should not break? 

             But note: we only enter the for loop if X_i<= (U-occ). And (U-occ) is the number of free numbers in [1,U]. Therefore, the X_i-th free number is in [1,U] and we will find it within the for loop.

          So in the for loop, we call find(current+1) for current+1 in [1, U] (because current starts at 0, then becomes a free number in [1, U-1]). The last call: current might be the (X_i-1)-th free number, and then we call find(current+1) which might be the next free which is within [1,U]. 

          We don't need to worry about beyond U in the extraction for this branch.

  Let's hope.

  But note: the initialization for next_ptr for the entire range [1, U+1] is O(U) per query, and U is about 10^6, and 5000 queries: 5000 * 10^6 = 5e9, which is 5e9.

  And the simulation: 5e9 union-find operations.

  Total 10e9 operations.

  In C++, we might be able to do 10e9 simple operations in 10 seconds? Not in 2 seconds.

  Therefore, we must optimize the initialization.

      Idea: we can use a temporary array that is global and then use a flag to avoid initializing the entire array. 

          We maintain a global next_ptr array of size MAX_U = 1001000.

          We also maintain an array "timestamp" of the same size, and a global integer "now".

          Initially: now=0, and we don need to initialize next_ptr.

          For each query:

             now++;
             // then during the find, if timestamp[x] != now, then we consider it as never modified in this query, and then next_ptr[x] = x (initial state) and timestamp[x] = now.

          Then we don't need to reset. 

          The find function:

             int find(int x) {
                 if (x > U) {
                     return x;
                 }
                 if (timestamp[x] != now) {
                     // this means it is in initial state: next_ptr[x] should be x, and we set timestamp[x]=now.
                     timestamp[x] = now;
                     next_ptr[x] = x;
                 }
                 if (next_ptr[x] != x) {
                     next_ptr[x] = find(next_ptr[x]);
                 }
                 return next_ptr[x];
             }

          But note: next_ptr[x] might be set by a previous query and we want to reset to x for this query. 

          However, if timestamp[x] != now, then we set next_ptr[x] = x and timestamp[x]=now.

          Then proceed.

          This is as if we have a lazy initialization.

          The cost: only the nodes that are touched are initialized. 

          How many nodes are touched per query? 

             In the simulation: 
                 For each day, we do a find(x) (which might initialize x and then the path compression might initialize some nodes in the path) and then if we assign y, we do find(y+1) which might initialize y+1 and the path from y+1.

             Additionally, in the free number extraction, we do up to X_i (<=500) find operations, each starting at current+1, which might initialize a few nodes.

          The total number of nodes touched per query: 
             The number of days: len_i (<=10^6)
             The number of nodes touched by the finds: each find might do a path of length O(alpha) in the worst-case amortized? 

          But note: our union-find is on a linear array. The path might be long. However, the amortized cost is O(alpha), and the total number of nodes touched is O(len_i * alpha(len_i)), which is O(len_i).

          So total nodes touched per query: O(len_i) = 10^6.

          And 5000 queries: 5000 * 10^6 = 5e9 nodes touched, and for each node touched, we do a constant amount of work (check timestamp, then set to now and next_ptr to x).

          This is 5e9 operations for the initialization (lazy) and then 5e9 for the union-find.

          Total 10e9.

      But this is the same amount of work. However, the advantage is that we avoid the 5e9 initialization loop (which was also 5e9) and now we do 5e9 for the lazy initialization and 5e9 for the union-find.

      It is the same total operations.

  We need a faster method.

  Given the constraints on the values of A_i and X_i, we might use a different approach.

      Since the starting values are at most 500, the simulation for a fixed segment [L,R] can be done by only caring about the assignments for values in [1, 500+ (R-L+1)] but note that the only conflicts are for values in [1, 500+ (R-L+1)].

      We can use a bitset of size 500+ (R-L+1) = len+500. 

          Let size = len+500.

          We will simulate the assignment for the segment [L,R] and for each day, we compute the assigned value and then mark it in the bitset.

          How to compute the assigned value without union-find? 

             We can use a next_available array but only for the range [1, size] and then use a global next_ptr array for this range, and we reset it quickly with a vector of indices that we touched in the previous query? 

          Or we can use an array "used" of booleans of size "size", initially all false.

          Then for a day with start=x:

             y = x
             while y <= size and used[y] is true: 
                 y++
             if y <= size:
                 used[y] = true
                 occ++

          This is O(conflict_length) per day. In the worst-case, the conflict_length might be the entire used array, so per day O(size) and total O(len * size) = 10^6 * 10^6 = 10^12 per query. 5000 queries: 5e15, too slow.

  We give up and hope that the union-find with lazy initialization might be optimized by the compiler and run on a fast machine.

  But the intended solution might be different.

  After research, a known solution for this problem (which is essentially offline for the assignment in a segment) is to use a DSU that is reset for the segment. Given the constraints on the values (A_i<=500) and the number of queries (5000), the union-find with lazy reset might be the way to go.

  Given the time, we implement as described with the lazy timestamp and hope that the constant factors are low.

  Steps for a type1 query (1 Li Ri Xi):

      len = Ri - Li + 1
      U = len + 1000   // we use this as the bound for the assignment and for the free number extraction

      // Use a global timestamp array and a global next_ptr array of size MAX_U = 1000000+1000 = 1001000 (or even larger to MAX_N=1000000, then U can be 1000000+1000, so we need 1001000)
      // Global: 
      //   int next_ptr[MAX_U+2];   // we don't initialize it at the beginning of the program, we use lazy timestamp.
      //   int timestamp[MAX_U+2];  // initialize to 0
      //   int now = 0;   // global for the entire program

      now++   // for this query

      occ = 0

      // For i from Li to Ri:
      for i = Li-1 to Ri-1 (0-indexed in the array A):
          x = A[i]   // which is between 1 and 500

          y = find(x, U)   // we also pass U to the find function

          if (y <= U):
              // do next_ptr[y] = find(y+1, U)
              next_ptr[y] = find(y+1, U)
              occ++

      // Then compute the free count in [1, U]: free_count = U - occ
      if (Xi <= free_count):
          current = 0
          for j in range(Xi):
              current = find(current+1, U)
          ans = current
      else:
          ans = U + (Xi - free_count)

      Output ans

  The find function:

      function find(x, U):
          if (x > U):
              return x
          if (timestamp[x] != now):
              // this is the first time in this query: reset to initial state
              timestamp[x] = now
              next_ptr[x] = x   // initially pointing to itself

          if (next_ptr[x] != x):
              next_ptr[x] = find(next_ptr[x], U)
              return next_ptr[x]
          else:
              return x

  Note: the path compression might update the next_ptr[x] to a value beyond U? But we only care about within U.

  Let's hope it works.

  Sample: [1,3] for the array A=[1,1,2] (after update: A[2] becomes 1)

          Query: [1,3] with X_i=2.

          U = 3+1000 = 1003

          now = some value.

          Day1: i=0, x=1
              y = find(1,1003) 
                 timestamp[1]!=now -> set to now and next_ptr[1]=1 -> return 1.
              then since y=1<=1003, 
                 next_ptr[1] = find(2,1003)
                    for 2: timestamp[2]!=now -> set to now and next_ptr[2]=2 -> return 2.
                 so next_ptr[1]=2.
              occ=1.

          Day2: i=1, x=1
              y = find(1,1003)
                 now next_ptr[1]=2, which is not 1 -> so next_ptr[1] = find(2,1003)
                 for 2: next_ptr[2]=2, so return 2.
                 then next_ptr[1]=2 (unchanged) -> return 2.
              y=2<=1003, so 
                 next_ptr[2]=find(3,1003)=3   (because timestamp[3] is not now -> initialize to 3)
                 so next_ptr[2]=3.
              occ=2.

          Day3: i=2, x=2
              y = find(2,1003)
                 next_ptr[2]=3, so find(3,1003)=3 (as above) -> so next_ptr[2]=3, return 3.
              y=3<=1003, so 
                 next_ptr[3]=find(4,1003)=4 (since 4>1003? no, 4<=1003 -> initialize 4 to 4, so return 4)
                 so next_ptr[3]=4.
              occ=3.

          Then free_count = U-occ = 1003-3=1000.
          We want the 2nd free number: since 2<=1000, we do:

              j=0: current = find(1,1003) -> 
                  1: next_ptr[1]=2 -> then find(2,1003) -> next_ptr[2]=3 -> find(3,1003) -> next_ptr[3]=4 -> find(4,1003) returns 4? 
                  But wait, is 1 free? 
                  In our simulation, we assigned 1,2,3. So 1 is not free.

                  The next_ptr[1]=2, and then we do find(1) which goes to 4? 

              This is because our next_ptr for 1 points to 2, 2 to 3, 3 to 4, and 4 to 4? 
              So find(1) = 4.

              Then j=0: current=4.

              j=1: current = find(5,1003) -> 5 (since not initialized in this query? we initialize it to 5) -> so find(5)=5.

              Then we return 5.

          Which is the sample output.

  But note: the free numbers are 4,5,6,... 
          The 1st free number is 4, the 2nd is 5.

  So it works.

  However, the extraction of the free numbers: we are not resetting the next_ptr for the extraction? 

          In the extraction, we call find(current+1) and this will use the next_ptr and timestamp. 

          For the extraction, we use the same query (now) and the next_ptr and timestamp are already set by the simulation. 

          For example, for the extraction after the simulation, the next_ptr[1] is 2, next_ptr[2]=3, next_ptr[3]=4, and next_ptr[4]=4? 

          When we do find(1), we get 4 (because: 
             find(1): next_ptr[1]=2 -> then find(2)=3 (because next_ptr[2]=3) -> then find(3)=4 (because next_ptr[3]=4) -> then find(4)=4? 
          But 4 is free? In the simulation, we did not assign 4? 

          We assigned 1,2,3. And then we set next_ptr[3]=4 (meaning that 3 is taken and the next free after 3 is 4). 

          But then when we do find(1), we are asking for the next free >=1, which should be 4. 

          And then for find(5): we get 5.

          So it is correct.

  Therefore, the plan is to use a global next_ptr array and a global timestamp array, and a global now. 

      We'll set MAX_U = 1000000 + 1000 = 1001000.

      Initially: 
          now = 0
          timestamp[1..MAX_U] = 0   // we can use memset once at the beginning

      For each query:

          now++ (if now overflows, we reset everything: but now up to 5000, so no problem)

          len = R-L+1
          U = len + 1000   // we'll use this as the bound for the find function

          occ = 0
          // Simulate the segment [L,R]:
          for i = L-1 to R-1 (0-indexed):
              x = A[i]
              y = find(x, U)   // using the function above

              if y <= U:
                  next_ptr[y] = find(y+1, U)   // note: even if y+1>U, the find will return y+1 and we store it, but then we won't use it in the next assignment? 
                  occ++

          free_count = U - occ

          if Xi <= free_count:
              current = 0
              for j in range(Xi):
                  current = find(current+1, U)
              ans = current
          else:
              ans = U + (Xi - free_count)

          print ans

      For update: we simply update the array A.

  Complexity: 
      The work in the simulation: 
          The for loop runs len times.
          In each iteration, the find is amortized O(alpha(U)) per call, and there are two find calls (one for x, and one for y+1).

          Total for simulation: O(len * alpha(U)) per query, and 5000 queries: total simulation work = 5000 * (10^6) * (small constant) = 5e9.

      The extraction: O(Xi) per query, which is 5000*500=2.5e6.

      The lazy initialization: 
          We only initialize a node when it is first accessed. In the simulation, each day might access a few nodes. The total nodes accessed in the simulation is about 2 * len per query (one for the x and one for the y+1, and then the path compressions) -> 2 * 10^6 per query, and 5000 queries: 10e9 nodes.

      So total work: 10e9 for the lazy initialization and 5e9 for the union-find in the simulation, and 2.5e6 for the extraction.

      Total: 15e9 + 2.5e6.

  This is 15e9, which in C++ might be borderline in 2 seconds.

  But the problem says: memory limit 512 MB.

      The arrays: 
          next_ptr: int[1001000] -> 1001000 * 4 = 4.004e6 bytes.
          timestamp: int[1001000] -> same.

          A: int[1000000] -> 4e6 bytes.

          Total: about 12e6 bytes.

  So we are well within the memory.

  We will hope that the constant factors in the union-find and the lazy initialization are low.

  Note: the path compression might not be called on every access, and the amortized bound is good.

  We also note that the starting values are small, so the first find(x) will often be a short path.

  Given the constraints, we output the solution as described.

  Why is it correct?
      The simulation for the segment [L,R] using the DSU with lazy reset and bounded by U = len+1000 correctly computes the set of assigned values in the range [1, U]. 
      Then the extraction of the X_i-th free number: 
          If there are at least X_i free numbers in [1, U], we get it by calling find in the DSU (which gives the next free number) repeatedly.
          Otherwise, the free number is U + (X_i - free_count), which is correct because the free numbers in [1, U] are free_count, and then the next free numbers are U+1, U+2, ...

  Let's run the sample: 
      "1 2 4 2" -> the segment: days 2,3,4: A[2,3,4] = [3,2,4] (0-indexed: indices 1,2,3)

        len=3, U=3+1000=1003.

        now = 1.

        Day1 (index1): x=3.
            find(3): not touched -> set timestamp[3]=1, next_ptr[3]=3 -> return 3.
            since 3<=1003, then next_ptr[3] = find(4) -> 
                find(4): not touched -> set timestamp[4]=1, next_ptr[4]=4 -> return 4.
            so next_ptr[3]=4, occ=1.

        Day2 (index2): x=2.
            find(2): not touched -> set to 2, return 2.
            then next_ptr[2] = find(3)=? 
                find(3): next_ptr[3]=4, which is not 3 -> so find(4) (which is 4) -> so next_ptr[3]=4 -> return 4.
            so next_ptr[2]=4, occ=2.

        Day3 (index3): x=4.
            find(4): returns 4 (because next_ptr[4]=4) -> 
            then next_ptr[4] = find(5)=5 (because 5>4, and not touched -> set to 5, but wait: 
                find(5): 5<=1003, so check: not touched -> set to 5, then return 5.
            so next_ptr[4]=5, occ=3.

        free_count = 1003-3=1000.

        Then for the extraction: X_i=2 -> we do:

            current=0
            j=0: find(1) -> 
                1: not touched -> set to 1, then return 1? 
                But wait, we never assigned 1, so it should be free.

            However, in the simulation, we did not assign 1, but we also did not initialize it? 

            In the simulation, we only initialized 2,3,4,5. But in the extraction, we do find(1) and it is not touched in this query, so we set next_ptr[1]=1 and return 1.

            So current=1.

            j=1: find(2) -> 
                2: next_ptr[2]=4 (from the simulation) -> then we find(4): next_ptr[4]=5 -> then we find(5): next_ptr[5]=5? -> so we return 5.

            So ans=5.

        Which is the sample output.

  But note: the free numbers are 1,5,6,7,... 
        The 1st free is 1, the 2nd free is 5.

  So it is correct.

  The sample update: "0 2 1" -> update A[2-1] = A[1] = 1.

        Then the next query: [1,3] (days1,2,3) with X_i=2.

        After update: A = [1, 1, 2, 4, 5]

        Query: [1,3] -> A[0]=1, A[1]=1, A[2]=2.

        U=3+1000=1003.

        Day1: x=1 -> find(1): set to 1, return 1. Then next_ptr[1]=find(2): set to 2, return 2. So next_ptr[1]=2, occ=1.
        Day2: x=1 -> find(1): next_ptr[1]=2 -> then find(2) (set to 2? it was set in the previous find(2) call? 
                Actually, in the first day we did find(2) and set it. 
                So next_ptr[1]=2, then we do find(2): next_ptr[2] is 2? no, because we haven't set next_ptr[2] yet? 

        How? 
            In the first day, we did for x=1: 
                y=1 -> then next_ptr[1]=find(2) -> which initialized 2 to 2 and returned 2. So next_ptr[1]=2.
            But we did not change next_ptr[2] at that time? 

        Then for the second day: 
            find(1): 
                next_ptr[1]=2, so then we do next_ptr[1] = find(2) -> 
                find(2): not touched? -> set to 2, return 2.
            so we return 2.

            Then we do: next_ptr[2] = find(3) -> set 3 to 3, return 3. 
            occ=2.

        Day3: x=2 -> 
            find(2): next_ptr[2]=3 (because set in day2) -> then find(3): returns 3. 
            then next_ptr[3] = find(4) = 4.

        occ=3.

        free_count = 1003-3=1000.

        Then extraction: 
            find(1): returns 1? -> no, because we haven't touched 1 in the simulation? 
                But we did: on day1, we set next_ptr[1]=2, but we never reset it? 
                In the extraction, we call find(1): 
                    if timestamp[1]!=now? -> we are in the same query? 
                But in the simulation we accessed 1 and 2 and 3 and 4. So timestamp[1]=now.

                next_ptr[1]=2, then find(2): next_ptr[2]=3, then find(3): next_ptr[3]=4, then find(4): returns 4.

            So the first free number is 1? -> no, we got 4.

        This is not correct.

  Why? The free numbers: 
        The assigned values: 
            day1: 1 -> then we set next_ptr[1]=2 (meaning that 1 is taken and the next free after 1 is 2, but then we assigned 2 on day2, and 3 on day3). 
            So the set of assigned values: {1,2,3}. 
            The free numbers: 4,5,6,... -> the first free is 4.

        In the extraction, we did:
            find(1): returns 4 (because the next_ptr chain: 1->2, 2->3, 3->4, 4->4? -> then 4 is free? -> but we set next_ptr[4] to 4, so it is free.

        So the first free is 4, not 1.

        But wait, 1 is assigned so it is taken. So the first free number should be 4.

        Why did we think it should be 1? 

        The simulation: 
            The intended for day1 was 1, and it was free, so it was assigned 1.
            Then the intended for day2 was 1, but 1 is taken, so it goes to 2 (free), assigned 2.
            Then the intended for day3 was 2, but 2 is taken, so it goes to 3 (free), assigned 3.

            So the free numbers are 4,5,6,...

        The extraction: 
            We do find(1): 
                We are in the same query, and next_ptr[1]=2 (set in day1) -> then find(2)=3, then find(3)=4, then find(4)=4 -> so we return 4.

            Then the first free number is 4.

            Then we do find(5) for the second free number? -> no, we do:

            j=0: current=4
            j=1: find(5) -> returns 5.

        So the second free number is 5.

        But the sample output is 5.

        So it is correct.

  Therefore, we code accordingly.

  Summary:

      We use a global next_ptr[1..MAX_U] and timestamp[1..MAX_U] and a global now=0.

      MAX_U = 1000000 + 1000 = 1001000.

      For the entire program:

          Read N, M and the array A of size N.

          For each query (total M queries):

             if type0: 
                 update A[Pi-1] = Xi   (0-indexed)

             if type1:
                 Li, Ri, Xi = input
                 len = Ri - Li + 1
                 U = len + 1000   // bound for this query

                 now = now + 1   // a new timestamp

                 occ = 0
                 // simulate the segment: for i from Li-1 to Ri-1 (0-indexed indices in A)
                 for index = Li-1; index <= Ri-1; index++:
                     x = A[index]
                     y = find(x, U)   // the function defined below

                     if y <= U:
                         next_ptr[y] = find(y+1, U)
                         occ = occ + 1

                 free_count = U - occ
                 if Xi <= free_count:
                     current = 0
                     for j in range(Xi):
                         current = find(current+1, U)
                     ans = current
                 else:
                     ans = U + (Xi - free_count)

                 print ans

      The find function (with path compression and lazy initialization for the current query):

          function find(x, U):
              if x > U:
                  return x
              if timestamp[x] != now:
                  timestamp[x] = now
                  next_ptr[x] = x
              if next_ptr[x] != x:
                  next_ptr[x] = find(next_ptr[x], U)
                  return next_ptr[x]
              else:
                  return x

  Note: the initialization for the entire program: 
        We only need to allocate next_ptr and timestamp of size MAX_U+1 (indexed from 1 to MAX_U) and initialize timestamp to 0.

  This meets the memory limit.

  Time: 15e9 operations, which in C++ might be borderline in 2 seconds, but note that M is only 5000 and the average len is 10^6, and we do two finds per day (each amortized constant) and the lazy initialization per node is constant.

  We hope that the constant factors are low.

  Given the sample, it works.

  Let's run the last sample: [1,5] with X_i=10.

      len=5, U=5+1000=1005.

      A = [1,1,2,4,5]

      now = ...

      Day0: x=1 -> y=find(1)=1 -> next_ptr[1]=find(2)=2 -> so next_ptr[1]=2, occ=1.
      Day1: x=1 -> y=find(1)=find(2) (because next_ptr[1]=2) -> then find(2) (which is not touched? so set to 2, then next_ptr[1] is still 2, but we then do next_ptr[1]=2? and then we do for this x=1: y=2 -> then next_ptr[2]=find(3)=3, occ=2.
      Day2: x=2 -> y=find(2)=find(3) (because next_ptr[2]=3) -> then find(3) (set to 3) -> then next_ptr[3]=find(4)=4, occ=3.
      Day3: x=4 -> y=4 -> next_ptr[4]=find(5)=5, occ=4.
      Day4: x=5 -> y=5 -> next_ptr[5]=find(6)=6, occ=5.

      free_count = 1005 - 5 = 1000.

      Then we want the 10th free number: 
          free_count=1000, 10<=1000 -> so we do:

          j0: current = find(1) = find(2) (from next_ptr[1]=2) -> then find(2) leads to 3 (because next_ptr[2]=3), then 3->4, then 4->5, then 5->6, then 6 is free? 
          So the first free number is 6? 

          But the free numbers: 
             1,2,3,4,5 are taken -> free numbers: 6,7,8,..., so the 10th is 15.

          How do we get 15?

          We do:

              j0: current = find(1)=6 -> then we want to find the next free after 6? 
              j1: current = find(7) -> 7 (because 7 is free) -> but wait, we haven't assigned 7, so it should be free.

          But in our simulation, we only assigned up to 5. How did we get 6 for find(1)? 

          The find(1) in the extraction:

             1: next_ptr[1]=2 -> then 2: next_ptr[2]=3 -> 3: next_ptr[3]=4 -> 4: next_ptr[4]=5 -> 5: next_ptr[5]=6 -> 6: next_ptr[6] is not set in the simulation? 

          In the extraction, when we call find(1), we will:

             for 1: timestamp[1] is set (by the simulation) so we use next_ptr[1]=2.
             for 2: set by simulation -> next_ptr[2]=3.
             for 3: next_ptr[3]=4.
             for 4: next_ptr[4]=5.
             for 5: next_ptr[5]=6.
             for 6: in the extraction, it is the first time? -> then we set next_ptr[6]=6 and return 6.

          So the first free number is 6.

          Then j0: current=6.

          j1: find(7) -> 7 is not touched -> set to 7, return 7.
          j2: find(8) -> 8
          ...
          j9: find(15) -> 15

          So we return 15.

      Therefore, it is correct.

  We'll implement accordingly.

  Note: the bound U = len+1000 is enough because the assigned values are at most len, and then the free numbers we care about are the ones up to the (at most 500th) free number, and the 500th free number is at most len+500. We use len+1000 to be safe.

  Code accordingly.

  We note that the worst-case might be a query with len=10^6, and we do 10^6 * (amortized constant) for the simulation and then 500 for the extraction, which is acceptable if the constant is small.

  Given the constraints, we output the solution.

  (Note: if we run into performance issues, we might try to reduce the constant factor, but we hope it passes.)

  Let's code.