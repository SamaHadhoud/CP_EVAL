 We note the constraints: 
   N up to 1e6, M up to 5000, but the values of A_i and X_i are at most 500.
   However, the update queries are only 5000, but the query type 1 requires processing a contiguous segment of days.

 But note: the sample solution in C++ uses a DSU-like structure with lazy reset and a bounded range (len+1000). 
 However, the problem states that the starting icebergs (A_i) are at most 500, and the queries ask for the X_i-th piplup (with X_i at most 500).

 Important insight: 
   The migration process: 
        We have an initial set of occupied positions? Actually, we start from an empty set.
        For each day i in [L, R]: we start at A_i and then move to the next available position.

   Since the starting positions are at most 500, and the segment length can be large (up to 10^6), but note that the next available position might go beyond 500. However, we are only going to have at most (R-L+1) = len piplups. The positions they occupy will be in the range [min_value, ...] but the maximum occupied position might be as large as len + 500? 

   But note: the problem says that the X_i-th piplup we are to find is the position of that piplup. And X_i is at most 500. However, we are not just looking for the first 500 free positions? Actually, we are to find the X_i-th smallest unoccupied position? 

   Actually, the query is: 
        We are to output the position of the X_i-th piplup in the group? 

   However, note the problem: 
        "they wish to determine the furthest iceburg (i.e., the largest numbered iceburg) that any of them will end up at with the information they have" 
        But then the sample output: 
            Query: [2,4] (days 2 to 4) with A[2]=3, A[3]=2, A[4]=4 -> then the piplups end up at positions: 
                day2: starts at 3 -> gets 3 (since initially empty)
                day3: starts at 2 -> gets 2
                day4: starts at 4 -> gets 4? 
            Then the positions occupied are {2,3,4}. Now the group of X_i=2 piplups: they start at iceburg 1. 
                First piplup: starts at 1 -> gets 1? 
                Second piplup: starts at 1 -> but 1 is taken, then 2 is taken, then 3 is taken, then 4 is taken, then 5 is free -> so gets 5.

        So the answer for the first query is 5.

   How to interpret the group? 
        The students are a group of X_i Piplups, all heading for iceburg 1. They leave one by one. 
        The process: 
            The first student goes to the smallest available iceberg starting at 1 -> which is the smallest iceberg not in the set S (where S is the set of icebergs occupied by the recorded migrations from days L to R).
            The second student then goes to the next available iceberg starting at 1, which will be the next smallest not in S and not taken by the first student.

        So the set of icebergs occupied by the recorded migrations is fixed (say T). Then the group of X_i piplups will occupy the first X_i icebergs that are not in T.

        Therefore, the k-th student will end up at the k-th smallest integer not in T.

        And the problem asks for the position of the X_i-th piplup, which is the X_i-th smallest integer not in T.

        However, note: the sample second query: 
            After update: A becomes [1, 1, 2, 4, 5] 
            Query: [1,3] -> days 1,2,3: 
                Day1: A1=1 -> gets 1
                Day2: A2=1 -> 1 taken, so goes to 2
                Day3: A3=2 -> 2 taken, so goes to 3
            Then T = {1,2,3}. 
            The group of 2: 
                The first piplup: smallest not in T is 4? 
                The second: 5? 
            But the sample output is 5.

        However, the problem states: 
            "there must still be at most one piplup per iceburg"

        But note: the group of students are added AFTER the recorded migrations. So T is the set of icebergs taken by the recorded migrations. Then the students start at 1 and take the smallest available. 
            The smallest available is the smallest integer not in T -> which is 4? Then next is 5. 
            Therefore, the second piplup is at 5.

        So the answer for the query is the X_i-th smallest natural number (starting at 1) that is not in T.

        Therefore, the problem reduces to: 
            For a segment [L, R], we have a set T = the set of icebergs that are occupied by the migrations in that segment. 
            Then we want to compute the X_i-th smallest natural number not in T.

        How to compute T? 
            We cannot store T explicitly because the segment [L,R] might be large (up to 10^6) and the numbers might be large (but note: the starting A_i are bounded by 500, but the occupied positions might extend to around 500 + (number of migrations) which is 10^6?).

        However, note that we are only going to have at most (R-L+1) distinct occupied positions? Actually, no: because the migration might cause consecutive positions to be taken? 

        But the problem: 
            We are to simulate: 
                We have an empty set. 
                For each migration i in [L, R]:
                    x = A_i
                    while x is in the set: x = x+1
                    then add x to the set.

        The set T is the set of all such x.

        How to compute T efficiently? 
            We can use a union-find (disjoint set) to jump to the next available. But note: we are only going to have at most len = (R-L+1) elements. The positions we touch are at most from the minimum starting value to ...? 

        Since the starting values are at most 500 and the segment length is len, the maximum occupied position we might get is at most 500 + len? 

        However, worst-case: if all start at 1, then the positions occupied are 1,2,...,len. 
        But if they start at 500, then worst-case we get 500, 501, ... 500+len-1.

        Therefore, the occupied positions are in the range [min_start, min_start+len-1] or even lower? Actually, if we start at 1 and then 1 is taken, the next one goes to 2, then 3, etc. So the maximum occupied position is at most (max_start + len) but max_start is at most 500, and len up to 10^6 -> so maximum position 10^6+500? 

        But note: we have M up to 5000 queries. We cannot simulate each query by iterating over each day and then doing a union-find that goes over 10^6 steps per query -> worst-case 5000 * 10^6 = 50e9 operations.

        We need a better approach.

        Alternative approach: 
          The problem constraints: 
            The starting values A_i are at most 500? Actually, the problem says: "0 <= A_i <= N-1", but then in the limits: "1 <= X_i, A_i <= 500" -> so actually A_i is in [1,500]? 

          And the updates: they set A_{P_i} = X_i, and X_i is also at most 500? 

          So we can assume that all A_i and update values are in [1,500].

        Therefore, the starting values are small. Then the occupied positions T are going to be a contiguous set? Not necessarily: because if we have two starting values: 1 and 3, then we get {1,3}? Then the next one starting at 1: becomes 2 -> so T = {1,2,3}. 

        However, note that the process: 
            We are starting at a value in [1,500]. Then we move until we find an unoccupied. 

        Since the starting values are bounded by 500, the set T must be contained in the interval [1, 500 + len]. Why? 
            Because if we have len migrations, the maximum displacement for a piplup is at most len? 

        Actually, worst-case: if all start at 1, then the first takes 1, the next takes 2, ... the last takes len. 
        But if one starts at 500 and then the next one also starts at 500, then that one goes to 501, then the next 502, etc. So the maximum occupied position is 500 + (number of migrations that start at 500 or above) - 1? 

        Actually, the maximum occupied position is at most 500 + len.

        Since len = R-L+1 <= 10^6, the maximum position we might see is 10^6+500, which is 1000500. 

        But note: the query then asks for the X_i-th smallest natural number not in T. 
            The natural numbers are 1,2,3,...

        How to compute the k-th smallest natural number not in T? 
            We can generate the entire set T? But |T| = len, which is up to 10^6. And we have 5000 queries -> worst-case 5000 * 10^6 = 50e9 which is too slow.

        We need to avoid generating T for each query.

        Alternative idea: 
          We note that T is a set of integers. The k-th smallest integer not in T is the smallest integer x such that the number of integers <= x that are not in T is at least k. 
          But note: the number of integers <= x that are not in T is: x - |T ∩ [1, x]|.

          So we want the smallest x such that: 
              x - |T ∩ [1, x]| >= k.

          We can binary search on x. But how to compute |T ∩ [1, x]| quickly? 

        However, note that T is the set of occupied positions from the migration. We have a way to simulate the migration and we know that T is contained in [1, 500+len]. But if we want to do a binary search for each query, and we have to compute |T ∩ [1, x]| for many x, and we don't have T stored.

        How to compute |T ∩ [1, x]|? 
          We can simulate the migration and for each migration we get a value y. Then we count how many y are <= x. But note: we have to do that for each x in the binary search? And we have to do that for each query? 

        That would be O(len * log(max_x)) per query. With len up to 10^6 and log(max_x) about 20, that is 20e6 per query, and 5000 queries -> 100e9 operations -> too slow.

        We need a different approach.

        Since the starting values are small (only 500 distinct starting points) and the migrations in the segment are many, we can try to use a sweep-line of the starting positions and then use a data structure to merge the chains? 

        Actually, we can use the idea of the union-find with a bounded range [1, U_bound] where U_bound = len + 1000 (as in the sample C++ solution). 

        Steps for a query:
          We set U_bound = len + 1000. (Because the maximum occupied position we might get is at most len + 500, so adding 1000 is safe to capture the first (len+1000) natural numbers? But we are also going to need to know the k-th free position. If k is within the first (U_bound) numbers, then we can simulate. Otherwise, we know that beyond U_bound, all numbers are free? 

          Specifically: 
            The set T (the occupied positions) is a subset of [1, U_bound]. 
            The free numbers in [1, U_bound] are U_bound - |T| = U_bound - len (because we have len migrations). 
            If k <= (U_bound - len), then the k-th free number is in [1, U_bound]. 
            Otherwise, the k-th free number is U_bound + (k - (U_bound - len)).

          Why? 
            The free numbers in [1, U_bound] are (U_bound - len). 
            Then the next free numbers beyond U_bound are consecutive: U_bound+1, U_bound+2, etc.

          Therefore, if k > (U_bound - len), then the k-th free number is: 
                U_bound + (k - (U_bound - len)) 
                          = len + k

          But wait: 
              total free numbers in [1, U_bound] = U_bound - len.
              Then the next free numbers are U_bound+1, U_bound+2, ... 
              The first free number beyond U_bound is the (U_bound - len + 1)-th free number? 
              Then the j-th free number beyond U_bound is U_bound + j.

          Therefore, the k-th free number: 
              If k <= (U_bound - len): we need to find the k-th free number in [1, U_bound].
              Else: the (k - (U_bound - len))-th free number beyond U_bound -> which is U_bound + (k - (U_bound - len)).

          However, note: the free numbers in [1, U_bound] are not contiguous? But we can use a union-find to mark the occupied and then traverse the free ones? 

          But how to get the k-th free number? 
            We can simulate the entire set T for the segment? But we are going to simulate the migrations to build the DSU for the bounded range [1, U_bound] anyway.

          The sample C++ solution does:
            It uses a DSU (with lazy reset) for the bounded range [0, U_bound] (actually they start at 0? but we note the natural numbers start at 1). 
            Steps:
                Initialize the DSU for the current query (using a timestamp to avoid resetting the entire array) for the bounded range [1, U_bound] (and beyond? but beyond we don't store in the DSU).

            For each day in the segment [L, R]:
                x = A_i (which is in [1,500])
                y = find(x)   -> which returns the smallest available position >= x in the bounded range? 
                If y <= U_bound: 
                    then we mark y as occupied, and set next_ptr[y] = find(y+1) (so that next time we jump to the next available).

            Then after processing the migrations, we have the set T (which is the set of occupied positions in [1, U_bound]).

            Then to get the k-th free number (where k = X_i):
                If k <= (U_bound - len): 
                    Then we traverse the free numbers by starting at 0 and then repeatedly find the next available: 
                        start = 0
                        for i in range(k):
                            start = find(start+1)   -> but note: find returns the next available, which might be beyond U_bound? 
                    However, we know there are at least k free numbers in [1, U_bound], so we can do:

                But note: the DSU structure we built for the bounded range: 
                    The find operation for a starting point x: 
                         if x is not occupied, returns x.
                         if occupied, returns the next available.

                However, we have already processed the migrations. The DSU now has the entire set T for [1, U_bound] marked as occupied? 

                How to get the k-th free number? 
                    We can start at 0 and then:
                         p1 = find(1) -> the first free number.
                         Then we mark p1 as occupied? But we don't want to change the state for the migration set T.

                Actually, we don't want to alter T. We can use the DSU for T to skip the occupied positions. But we want to know the free positions.

                Alternatively, we can note that the free positions in [1, U_bound] are the complement of T. How to get the k-th smallest? 

                We can simulate by starting at 1 and then using the DSU to jump: 
                    Let current = 0.
                    For j in range(k):
                         current = find(current+1)   -> which gives the next free position.

                    But note: if k is large and we do k steps, k can be up to U_bound - len (which is 1000) so at most 1000 steps per query? 

                However, worst-case k can be up to 500 (as per constraints: X_i <= 500). So we can safely do k steps (500 steps per query).

                But wait: what if k is 500 and we do 500 steps? 

                Therefore, we do:
                    if k <= (U_bound - len): 
                         current = 0
                         for j in range(k):
                             current = find(current+1, U_bound)   -> the DSU we built for the bounded range [1, U_bound] (and beyond: if beyond, we return the number itself because we didn't store state beyond U_bound, so we assume beyond is free?).

                    However, note: our DSU structure: 
                         For x > U_bound: we return x (because we haven't stored state for them, and we assume they are free).

                But what about if current becomes beyond U_bound? 
                    We only do k steps, and k is at most 500, and we start at 0 -> then the first step: current+1=1 -> if 1 is free then current=1; next step: 2, ... 
                    But worst-case, the free numbers in [1, U_bound] are the first k numbers? Then we get current=k. 

                However, note: the set T might break the contiguous free numbers. But since we are using the DSU, it will jump over the occupied ones.

                But worst-case: we have to jump over many occupied? But the DSU find is O(α) per step? 

                But note: we are doing k steps (at most 500) and each step is O(α) so it's acceptable.

            But there is a catch: if k > (U_bound - len), then we don't need to simulate the DSU beyond U_bound. We can compute: 
                ans = U_bound + (k - (U_bound - len))

          Why is that? 
                The free numbers in [1, U_bound] are (U_bound - len). 
                Then beyond U_bound, the numbers are consecutive and free: 
                    The free numbers are: 
                        1st free: ... the last free in [1,U_bound] is the (U_bound-len)-th free.
                        Then the next free number is U_bound+1 (which is the (U_bound-len+1)-th free)
                        Then U_bound+2, ...

          Therefore, the k-th free number: 
                if k <= (U_bound - len): we simulate the DSU to get the k-th free number in [1, U_bound].
                else: we return U_bound + (k - (U_bound - len))

        However, note: the DSU we built already has the entire set T for the bounded range? So we can use the same DSU to traverse the free numbers? 

        But we have to be careful: the DSU we built for the bounded range [1, U_bound] has stored the next available for numbers in that range. For numbers beyond U_bound, we haven't stored anything. 

        How do we write find(x, U_bound) in Python? 
            We can do: 
                if x > U_bound: 
                    return x
                else:
                    if timestamp[x] != now: 
                         initialize it: next_ptr[x] = x, timestamp[x] = now
                    if next_ptr[x] != x:
                         next_ptr[x] = find(next_ptr[x], U_bound)
                    return next_ptr[x]

        Then when we do:
            For each migration: 
                x = A_i
                y = find(x, U_bound)
                if y <= U_bound: 
                    then we set next_ptr[y] = find(y+1, U_bound)   -> which is the next available after y.

        Then to get the k-th free number: 
            We do:
                current = 0
                for i in range(k):
                    current = find(current+1, U_bound)

            But note: if during this traversal we hit an occupied number, the DSU will jump to the next available. 

            However, we have already built the DSU for T (the migrations). The free numbers we are traversing now are the complement of T? 
                Yes, because we built the DSU to skip the occupied (by the migrations). 

            But then we are about to alter the DSU? 
                When we do find(current+1, U_bound), we are just querying. However, if we do not update the DSU, the path compression might be done? 

            We must note: the DSU is built for the migrations. Now we are going to use the same DSU to traverse the free numbers. However, we do not want to alter the state of the DSU for the migrations? 

            Actually, we are not altering the set T. We are only reading the state. The find operation for the free numbers traversal will do path compression? 

            But that's okay: because we are going to reset the DSU for the next query (by incrementing the timestamp). 

        However, we are going to use the same DSU for the free numbers traversal? And we are going to do:
            current = 0
            for i in range(k):
                current = find(current+1, U_bound)

            This will set next_ptr for some free numbers? But we don't care because we reset by timestamp for the next query.

        But note: we are not updating the DSU when we do the free numbers traversal? We are only querying? 
            Actually, the find operation with path compression does update the next_ptr. But that's okay because we are not using the DSU for anything else in this query.

        However, we are going to use the same DSU for the entire query? 

        Steps for a query:
            now++   (to reset the DSU for the current query)
            U_bound = len + 1000
            For each day in the segment [L, R]:
                x = A_i
                y = find(x, U_bound)   -> this might update the DSU (with path compression and union with the next available)
            Then we compute:
                free_in_bound = U_bound - len   (because we have len occupied in [1, U_bound]? But note: it is possible that some migration went beyond U_bound? 

            How do we know how many migrations ended up in [1, U_bound]? 
                Actually, we only mark an occupied if y <= U_bound. 
                But note: we start with x in [1,500] and then we do find(x) -> which returns the next available. 
                Since U_bound = len+1000, and len can be large, but the starting x is small, so the next available might be beyond U_bound? 
                Example: if we have a migration that would require going beyond U_bound? 

            The problem: 
                For a migration: 
                    x = A_i
                    y = find(x, U_bound) 
                    if y <= U_bound: 
                        we mark it as occupied and set next_ptr[y] = find(y+1, U_bound)
                    else:
                        we don't mark it? 

            Then the migration that ends beyond U_bound is not recorded. 

            But note: the set T we care about for the free numbers? 
                Actually, the migration that goes beyond U_bound does not occupy a number in [1, U_bound]. So it doesn't affect the free numbers in [1, U_bound]. 

            Therefore, the number of occupied positions in [1, U_bound] is the count of migrations that found a position in [1, U_bound]. 

            How many migrations are there? len. 
            How many of them are beyond U_bound? 
                Since we set U_bound = len+1000, and the maximum displacement from a starting point x (which is at least 1) is at most len? 
                So the maximum occupied position we can get is 1 + (len-1) = len? (if all start at 1) -> then len <= len+1000 -> so they are all in [1, U_bound]. 

            Actually, worst-case: if we have a migration that starts at 500, and there are already 500+len-1 occupied? 
                Then the next migration starting at 500 would go to 500+len? which is beyond U_bound? 

            But note: we set U_bound = len + 1000 -> so 500+len-1 <= len+1000? 
                500+len-1 = len+499 <= len+1000 -> so it is within. 

            Actually, the maximum starting point is 500. Then the maximum occupied position we can get is 500 + (number of migrations that start at 500 or above) - 1? 
                Actually, the maximum occupied position is at most: 
                    max_start + (number of migrations that start at or above the same value) - 1.

            But the worst-case: 
                All migrations start at 500. 
                Then the first migration: 500
                second: 501
                ... 
                last: 500 + len - 1.

            Then 500+len-1 <= len+1000? 
                Since 500+len-1 = len+499, and 499 < 1000 -> so it is within.

            Therefore, every migration will be placed in [1, U_bound] because 500+len-1 = len+499 <= len+1000.

            Therefore, the count of occupied in [1, U_bound] is exactly len.

            Then free_in_bound = U_bound - len.

            Then if k <= free_in_bound: 
                 we traverse the free numbers by starting at 0 and then k times: 
                     current = find(current+1, U_bound)
                 and output current.

            Else: 
                 ans = U_bound + (k - free_in_bound)

        However, note: the free numbers beyond U_bound are consecutive and free. 

        But what if k is 10? 
            free_in_bound = (len+1000) - len = 1000.
            Then if k=10: we do the traversal.

        But note: k is at most 500 (as per constraints: X_i <= 500). 
            So k is always <= 1000? because free_in_bound = 1000 -> so we never go to the else branch? 

            Actually: 
                k <= 500, and free_in_bound = 1000, so k<=1000 -> so we always do the traversal.

        Therefore, we can simply do the traversal for k steps (since k<=500) without worrying about the else branch? 

        But wait: what if len=0? then U_bound = 1000, free_in_bound=1000, and k=500 -> we traverse 500 steps. 

        However, what if we set U_bound = len+1000 and len=1000000, then free_in_bound=1000, and k=500: we traverse 500 steps -> acceptable.

        But note: the problem says X_i (the k) is at most 500. So we can always do the traversal without the else branch? 

        Actually, the problem says: "1 <= X_i, A_i <= 500" -> so X_i is at most 500. 

        Therefore, we can skip the else branch? 

        Then we can do:

            U_bound = len + 1000   (to ensure that all migrations are placed within [1, U_bound])

            now += 1
            # simulate the migrations in the segment [L, R]
            for i in range(L-1, R):
                x = A[i]
                y = find(x, U_bound)
                # Since we know y<=U_bound, we update the DSU for y
                if y <= U_bound:
                    # Mark y as occupied: set next_ptr[y] = find(y+1, U_bound)
                    # But note: if y is beyond U_bound, we skip? but we know y<=U_bound.
                    next_ptr[y] = find(y+1, U_bound)

            # Now, we want to get the k-th free number (k = X_i) in the entire natural numbers? 
            # But note: we have marked the occupied positions in [1, U_bound] in the DSU. 
            # Then we do:
            current = 0
            for j in range(X_i):
                current = find(current+1, U_bound)

            Then output current.

        Why is this the k-th free number? 
            The find(current+1, U_bound) returns the smallest available number >= current+1. 
            We start at 0, then we get the first free number: find(1, U_bound) -> which is the smallest available >=1. 
            Then we get the second: starting at the next after that, etc.

        But note: we are not marking these free numbers as occupied? 
            We are only using the DSU to skip the occupied by the migrations. The free numbers we are traversing are not being added to T? 
            And that's correct: because the students are a separate group. We are just querying the free numbers.

        However, the DSU structure we built for the migrations is still valid? and we are using it to skip the occupied positions (by the migrations). 

        But note: the free numbers traversal does path compression? 
            For example, when we do find(1, U_bound): 
                if 1 is free, then we return 1, and then we don't update next_ptr[1]? 
                Actually, we do: 
                    if timestamp[1] != now: then we set next_ptr[1]=1 -> then we return 1.
                But if 1 is free, then we don't have an entry for 1 in the DSU? Actually, we haven't processed any migration that touched 1? 

            Then we initialize next_ptr[1]=1 and then we return 1. 

            Then when we do next_ptr[1] = 1? 

            Then when we do find(2, U_bound) later? 

            But note: in the migrations, we did not set next_ptr for 1? 

            However, the free numbers traversal does not alter the state for the free numbers? It just initializes the state for the free numbers that were not initialized by the migration simulation? 

            Then when we do find(1, U_bound) for the free traversal: 
                we set next_ptr[1]=1 -> and then we return 1.

            Then when we do find(2, U_bound): 
                we set next_ptr[2]=2 -> and return 2.

            So it's as if we are building the DSU for the entire bounded range on the fly? 

            But we already processed the migrations: for the migrations, we only set the next_ptr for the occupied positions? 
                For an occupied position y, we set next_ptr[y] = the next available after y (which might be beyond U_bound? but we set it to the return value of find(y+1, U_bound)).

            Then when we do the free traversal, we will initialize the free positions that were not touched by the migration simulation? 

            That's acceptable.

        But note: if a free position was not touched by the migration simulation, then we initialize it to point to itself. Then the find operation for that free position returns itself? 

        Therefore, the algorithm:

            We maintain a global next_ptr array and a global timestamp array, and a global variable now (current time).

            For each query of type 1 (L, R, X_i):
                len = R - L + 1
                U_bound = len + 1000   # to cover all migrations

                now += 1   # reset the DSU for the current query

                # Process the migrations in [L, R]:
                for idx in range(L-1, R):
                    x = A[idx]
                    y = find(x, U_bound)   # this will initialize if not present and do path compression
                    # We know y <= U_bound? because we set U_bound = len+1000 and the maximum we can get is len+499, so yes.
                    # Mark y as occupied: we set next_ptr[y] = find(y+1, U_bound)
                    next_ptr[y] = find(y+1, U_bound)

                # Now, to find the X_i-th free number (which is the position of the X_i-th student)
                current = 0
                for j in range(X_i):
                    current = find(current+1, U_bound)

                print(current)

            For update: 
                simply update A[P_i-1] = X_i

        However, note: the constraints: 
            M (number of queries) up to 5000, and the worst-case for a query is O(len + X_i) = O(len + 500). 
            And the total len over all queries? 
                The worst-case total len = sum_{query} (R_i-L_i+1) can be: 
                    worst-case: each query is [1, N] -> 10^6 per query, 5000 queries -> 5e9 -> too slow.

        We must optimize the per-query processing time.

        How to optimize? 
            We note that the starting values A_i are small (in [1,500]). Therefore, we can precompute something? 

        Alternative approach: 
          We note that the migration process is deterministic. The set T (the occupied positions) for a segment [L,R] is the set of the final positions. 

          We can use a Mo's algorithm? But M up to 5000 and N up to 10^6 -> Mo's is O(N * sqrt(N)) which is about 10^6 * 1000 = 10^9 -> too heavy.

        We need an offline method? 

        But note: the updates are interleaved with queries. 

        However, the constraints: 
            M = 5000, and the segment length might be large (10^6) but the total over all queries might be too high.

        We must look for a different idea.

        Insight: 
          Since the starting values are small (at most 500) and the updates only change the starting values to another value in [1,500], we can try to use a segment tree that aggregates the set of starting values? 

        Actually, we note that the entire migration process can be simulated by a greedy algorithm that groups the migrations by starting value. 

          Let f(x) = the number of migrations in the segment that start at x.

          Then the process: 
            The migrations that start at 1: they will occupy the first f(1) numbers starting at 1: 1,2,...,f(1)
            Then the migrations that start at 2: 
                They will try to occupy 2, but if 2 is already taken by a migration that started at 1? 
                Actually, the migrations are in the order of the days? 

          But the problem: the migrations are in the order of the days. So we cannot reorder by starting value.

        However, there is a known offline algorithm for the "mex" problem? 

        But note: we are to compute the entire set T? and then the k-th free number.

        Alternatively, we can use a Fenwick tree to count the final positions? 

        Known: 
          The final position for a migration starting at x is x + (number of migrations that ended up in the range [x, x+ ...])? 

          Actually, we can use a Fenwick tree to simulate the migration? 
            We maintain an array that marks occupied positions. 
            For a migration starting at x: 
                we want to find the smallest y>=x such that the prefix sum [x,y] has no occupied? -> but we are building it.

          But we are processing a segment of days. 

        Given the complexity, and the fact that M is only 5000 but the segment length can be 10^6, we need an approach that is independent of the segment length? 

        But note: the starting values are small (500) and the final positions are bounded by len+500, and k is small (500). 

        However, worst-case segment length 10^6 per query and 5000 queries -> 5e9 which is too high in Python.

        We need to optimize the simulation of the migration for the segment.

        We can use a DSU that is optimized for the entire array? but we cannot build a DSU for the entire natural numbers.

        But note: we are only interested in the bounded range [1, len+1000]. We can simulate the segment in O(len) per query? 

        And worst-case total len over all queries: 
            The worst-case total is the sum of (R_i - L_i + 1) for each query. 
            The worst-case is when every query is the entire array: 10^6 * 5000 = 5e9 -> too slow in Python.

        Therefore, we must avoid simulating each migration one by one.

        Alternative approach: 
          We note that the migrations in the segment are independent? but the DSU we built for the bounded range is efficient per migration? 
          The DSU with path compression is nearly O(1) per migration? 

          So the entire simulation for a segment of length L is O(L * α(L))? 

          Then worst-case total L: 5000 * (10^6) = 5e9 -> which is too high in Python.

        We must use the fact that the starting values are small. 

        Idea: 
          We can precompute the entire set T for the entire array? But there are updates. 

        How about storing the starting values in a segment tree and then using a global DSU? 

        Actually, we note that the migration process for a given starting value x: 
            It will end up at x + (number of migrations that have already occupied a position in the range [x, ...])? 

        But the dependency is on the entire segment.

        Known: 
          We can compute the final position of a migration starting at x on a set of existing migrations as the smallest y>=x that is not in the set. 

          And the set is the union of the migrations that come before? 

        We can use a Fenwick tree to count the number of migrations that have ended in a particular position? 

        But then we have to update as we go? 

        However, we are processing a segment [L,R]. 

        We can use offline queries? 

        Given the constraints, and that M is only 5000, we can try to use a DSU that is reset for each query, but we hope that the DSU with path compression is fast? 
          Worst-case total operations: 5000 * (10^6) = 5e9 -> too high in Python.

        We need a better approach.

        Insight: 
          Since the starting values are at most 500, we can simulate the entire segment by maintaining 500 buckets? 

          Let freq[x] = count of migrations in the segment that start at x.

          Then the final set T can be computed by:

            We start with an empty set.
            We consider x from 1 to 500 (or from 0 to 500? but the problem says 1<=A_i<=500) -> so x in [1,500].

            For each x, the freq[x] migrations starting at x: 
                They will occupy a contiguous block? Not necessarily: because there might be migrations with starting value < x that have occupied positions above x? 

          Actually, the final positions for migrations starting at x are in the range [x, x + (some offset)].

          How to compute the offset? 
            Let F(x) = the number of migrations that have occupied a position in the range [1, x] (from migrations with starting value < x) -> then the first migration starting at x will go to: 
                y0 = x + (number of migrations that have a final position in [x, x0])? 

          This is complex.

        Alternatively, we can use a sweep from low starting value to high:

            Let T be the set of final positions (which we want to compute).
            We know the starting values. 

            We can do:

              arr = [0]*(max_position+1)   # but max_position = len+1000 -> 10^6+1000, which is 1e6+1000, acceptable per query? 
              But then we have to do 5000 queries, each with an array of 1e6+1000 -> 5000*1.1e6 = 5.5e9 integers -> too much memory.

        Given the complexity, and the constraints on the starting values and k, there is a known efficient method:

          We can use a Fenwick tree or segment tree to simulate the entire segment quickly? 

          Steps for a query [L,R] with small starting values:

            We want to compute the final positions for each migration in the segment. 

            We can do:

                Let max_pos = len + 1000   (where len = R-L+1)
                We create an array mark of size max_pos+1 (but we don't want to initialize the entire array per query? that's 10^6 per query * 5000 -> 5e9, too much)

            Instead, we can use a DSU that is lazy in the bounded range, but we hope that the DSU path compression will make it efficient? 
                The total number of DSU operations per query is 2*len (because for each migration we do one find, and then we update one next_ptr, and then we do the free numbers: 500 more).

            The worst-case total operations over all queries: 
                total_len = sum_{queries} (R_i-L_i+1)

                worst-case: 5000 * (10^6) = 5e9 operations -> too many in Python.

        Therefore, we must avoid processing each migration individually.

        New idea: 
          We note that the migrations with the same starting value can be processed together? 

          But the days are in order. 

        However, the final position of a migration depends on the exact order? 

          Example: 
            [1,1] -> first migration:1, second:2.
            If we reverse: still first:1, second:2.

          Actually, the final set T is the same as if we processed the migrations in increasing order of starting value? 
            Because if two migrations start at x and y with x<y, then the one with x will not be affected by the one with y? 
            But if they are in different orders? 

          However, consider: 
            Day1: starts at 2 -> gets 2.
            Day2: starts at 1 -> gets 1? 
            Then the set T={1,2}.

          If we swap:
            Day1: starts at 1 -> gets 1.
            Day2: starts at 2 -> gets 2.

          So the set T is the same. 

          But note: 
            Day1: starts at 1 -> gets 1.
            Day2: starts at 1 -> gets 2.

          And if we swap the days for the same value? 
            It doesn't matter.

          Therefore, the set T is independent of the order of the migrations? It only depends on the multiset of starting values.

          Then we can process the segment [L,R] by:

            Count frequency for each starting value: freq[x] for x in range(1,501)

          Then how to compute T from the frequency counts? 

          We can simulate greedily by increasing x:

            Let next_available = 1   # but we have to consider starting value
            Instead, for x from 1 to 500:
                For the freq[x] migrations that start at x, they will occupy the first freq[x] available positions that are >= x.

            How to find these available positions? 

          We maintain an array (or a Fenwick tree) of available positions? But we are not iterating position by position.

          We can use a greedy pointer for each x? 

          Alternatively, we know that the available positions are the ones that are not occupied by a migration with starting value < x. 

          Let F = sorted list of occupied positions from migrations with starting value < x. 
          But we don't have that.

          Instead, we can do:

            Let T = empty set.
            For x from 1 to 500:
                Let count = freq[x]
                Let current = x
                While count > 0:
                    if current not in T:
                        add current to T
                        count -= 1
                        current += 1
                    else:
                        current += 1

          But this is O(total_occupied) = O(len), and len can be 10^6, and we do it for 500 values of x -> 500 * 10^6 = 50e6 per query -> 5000 * 50e6 = 250e9 -> too slow.

        We need to speed up the while loop.

        We can use a DSU for the available positions across x? 

          We maintain a DSU for the range [1, U_bound] (U_bound = len+1000) for the entire frequency simulation? 

          Steps:

            now += 1   # reset for the query
            U_bound = len+1000

            # Instead of processing the days, we process the frequencies
            for x in range(1, 501):
                count = freq[x]
                # We need to find the next available position starting from x, and take count consecutive available positions? 
                # But note: it's not consecutive, because the available positions might be non-continuous? 
                # Actually, we want to find the next count available positions starting at x.

                # We can do:
                current = x-1   # we'll start from x
                for c in range(count):
                    current = find(current+1, U_bound)
                    # But then we mark current as occupied: 
                    next_ptr[current] = find(current+1, U_bound)

            Then the set T is the set of all 'current' we found.

          Then the total number of DSU operations is the total frequency = len. 

          But then we still have to do the free numbers traversal for the students: k=500 steps.

          How to compute freq[x] for the segment [L,R]? 
            We can have an array for the entire array A, and then for each query, we iterate x from 1 to 500 to get freq[x] = number of indices in [L,R] with A[i]==x.

          How to get that quickly? 
            We can precompute an array for each x in [1,500] the list of indices where A[i]==x? 
            But then for a query [L,R] and a given x, we can do a binary search to get the count. 

          We have 500 values of x, and 5000 queries: 5000*500=2.5e6 -> acceptable.

          Steps:

            Precomputation:
                Create a list of lists: positions[1..500]
                For i in range(N):
                    if A[i] is in [1,500]:
                        positions[A[i]].append(i)

            For a query [L,R] (0-indexed L-1 to R-1):
                for x in range(1,501):
                    # count the number of indices in [L-1, R-1] in positions[x]
                    # use bisect

            Then we have freq[x] for x in [1,500]

          Then we simulate the DSU for the bounded range [1, U_bound] (U_bound = len+1000) by:
                now += 1
                for x in range(1,501):
                    count = freq[x]
                    for c in range(count):
                        y = find(x, U_bound)   # but wait, we are starting at x? but then next time we want the next available after the last occupied?
                        # Actually, we should start from the last known available? 

                But note: the DSU naturally does that: 
                    The first call for x: find(x) returns the smallest available >=x.
                    Then we mark it as occupied: next_ptr[y] = find(y+1, U_bound)
                    Then the next call for the same x: find(x) will return the next available after y.

                But that's correct.

          However, we are doing two finds per migration: one for the current x and one for next_ptr[y]. So total 2 * len.

          Then after that, we do the free numbers traversal for the students: 500 steps.

          Total operations per query: 2 * len + 500.

          Total over all queries: 
                sum_{query} (2 * len + 500) 
                = 2 * (total_len_over_queries) + 500 * (number of queries)

          The worst-case total_len_over_queries is 5000 * 10^6 = 5e9, and 5000*500=2.5e6, so total operations 10e9 + 2.5e6 -> 10e9 operations.

          In Python, 10e9 operations might be borderline in PyPy/C++ but in Python it is likely to be too slow.

        But note: the DSU find is with path compression. The amortized time per find is nearly O(1). However, the constant factors and the sheer 10e9 operations in Python might be too slow.

        We must hope that the path compression will reduce the number of steps? 

        Alternatively, we can avoid the inner loop for a fixed x by doing:

            We want to find the next count available positions starting from x.

            We can do:

                start = x
                while count > 0:
                    next_avail = find(start, U_bound)
                    if next_avail > U_bound:
                        break   # but we know count should be 0 because U_bound is large enough, so we won't break
                    # We want to find a contiguous block of available positions starting at next_avail? 
                    # But they might not be contiguous: because we haven't processed higher x? 
                    # Actually, we are processing x in increasing order, so the available positions we see are indeed contiguous from next_avail until the next occupied by a lower x? 
                    # But we have processed all lower x. So the available positions are contiguous until we hit an occupied by a higher x? -> no, we haven't processed higher x, but they don't occupy anything yet.

                    # Therefore, from next_avail, we can take a contiguous segment of free positions? 
                    # How many free positions are there from next_avail to the next occupied? 
                    # The next occupied might be at next_avail + 1? -> but we don't know because we haven't processed it.

                    # Actually, the DSU will tell us the next available after next_avail: it is find(next_avail+1, U_bound)

                    # Let end = find(next_avail, U_bound)   # but that's next_avail.
                    Let next_next = find(next_avail+1, U_bound)   # the next available after next_avail.

                    The contiguous free block from next_avail is only one if next_next != next_avail+1.

                    Therefore, we cannot assume contiguous.

            So we have to take count positions one by one.

        Given the time constraints, and that worst-case total operations might be 10e9 in Python (which is too slow), we must abandon the frequency method if the total length is large.

        Or we can hope that in practice the DSU path compression will be fast? 

        But 10e9 operations in Python is about 100 seconds in PyPy/C++ but in Python it could be minutes.

        Therefore, we must use the first method (processing each day in the segment) for small segments and the frequency method for large segments? 

        But the frequency method is for large segments? 

        Actually, the frequency method does 2 * len operations, same as the first method. 

        So both are O(len) per query. 

        Then we are stuck with O(total_len) = 5e9 in the worst-case.

        But note: the problem constraints for the entire input: 
            N = 10^6, M = 5000, but the worst-case total_len = 5000 * 10^6 = 5e9.

        However, the problem also says: 
            "0 Pi Xi" updates are interleaved. 
            And there are M queries in total, which include both updates and queries. 

        The number of type 1 queries is at most 5000.

        And the sum of lengths of the segments in type 1 queries might be up to 5000 * 10^6 = 5e9.

        But 5e9 might be borderline in C++ but in Python it is likely too slow.

        Therefore, we must hope that the segments in the queries are short? 

        But the problem does not guarantee that. 

        Alternatively, we can use a more efficient offline method? 

        Given the complexity of the problem and the constraints, and the fact that the starting values are small, we can try to use a different approach for the entire array with a Fenwick tree that is updated for the entire array, and then we can reset for each query? 

        However, the updates are there.

        Or we can use a segment tree that for each node, stores the final set T for that node? but then the merge of two sets is not easy.

        Given the time, and since M is only 5000, and the worst-case might be borderline in PyPy/C++ but in Python we need a miracle, 
        but the sample solution in C++ is provided and it is O(total_len) for the queries, and it passed because M is only 5000 and the worst-case total_len is 5000*(10^6)=5e9, which is acceptable in C++ (2 seconds for 5e9 operations? not in Python).

        Therefore, we must implement the frequency method in Python and hope that the average length is short or that the path compression makes it faster.

        But note: the worst-case is a ( len + 1000) for the bounded range, and the path compression will make the find nearly O(1). 

        However, the total number of find operations is 2 * total_len_over_queries, which is 10e9.

        In Python, 10e9 iterations might be 100 seconds, which is acceptable for the sample (2 seconds for C++ but not for Python) -> we need a faster method.

        Final idea: 

          We note that the set T is the union of the final positions, and the final positions for migrations with starting value x are exactly the set of integers in the interval [x, x + f(x) - 1] might be not if there are overlaps.

          Actually, the final positions for migrations with starting value x are the f(x) smallest integers >= x that are not in the final positions of migrations with starting value < x.

          This is the greedy algorithm in increasing order of x:

            Let T = set()
            For x from 1 to 500:
                Let A_x = the f(x) smallest integers >= x that are not in T.
                Then T = T ∪ A_x.

          Then the entire set T for the segment [L,R] is the union of A_x for x in [1,500].

          How to compute A_x quickly? 
            Let F(x) = the number of integers in T (from x' < x) that are < x. 
            This doesn't help.

          Instead, we can maintain the whole set T as a bitset? but the size is len+1000, which is 10^6+1000, so we can allocate a bitset of size 1.1e6 per query? 
            Memory: 5000 * 1.1e6/8 = 5000 * 137.5KB = about 687.5 MB -> acceptable.

          Then for a query:

            len = R-L+1
            U_bound = len+1000
            bitset = bytearray((U_bound+7)//8)   # initialized to 0

            # But then we have to update the bitset for the migrations by increasing x:
            for x in range(1,501):
                count = freq[x]
                # starting at x, find the next count free positions in the bitset
                ptr = x
                while count > 0 and ptr <= U_bound:
                    if ptr is not set in the bitset:
                        set the bitset for ptr
                        count -= 1
                    ptr += 1

            Then the free numbers are the unset bits.
            Then we want the X_i-th unset bit in the bitset? 

          The while loop: worst-case ptr might go up to U_bound for each x, so total 500 * (U_bound) = 500 * (len+1000) = 500 * (10^6+1000) = 500e6 per query -> 5000 * 500e6 = 2500e9 -> too slow.

        We must use a DSU-like structure for the bitset? 

          We can use a Fenwick tree or segment tree to get the next free position quickly? 

          But we are within the bitset for one query. 

          Alternatively, we can use a DSU for the entire bitset? 

          We can maintain an array next_free[0..U_bound] for the next free position. 
            Initially, next_free[i] = i.
            When we set a bit at i, we do: next_free[i] = next_free[i+1]

          But then we have to update all positions? 

          Or we can use a union-find for the free positions within the bitset? 

          Steps:

            next_ptr = list(range(U_bound+2))   # next_ptr[i] = next free position >= i
            for i in range(U_bound+1):
                next_ptr[i] = i

            Then for x in range(1,501):
                count = freq[x]
                while count > 0:
                    y = next_ptr[x]
                    if y > U_bound:
                        break
                    # then we set next_ptr[y] = next_ptr[y+1]
                    # and then we do union: we want to merge y with y+1? 
                    # But we can do: 
                    next_ptr[y] = next_ptr[y+1]   # this is not enough for future finds.

                    # We also need to update all positions that point to y to point to next_ptr[y+1]? 
                    # That is expensive.

            Instead, we can use a stack. 

          Given the complexity, and the time, we return to the DSU per query with the hope that the average len is small or that the path compression in the bounded range is fast enough.

        Given the constraints in the problem that the starting values are small and the k is small, and that the total worst-case total_len is 5e9, we may need to use PyPy and hope, or in C++.

        But the problem memory limit is 512 MB.

        We will implement the frequency method with the DSU in the bounded range, and hope that the test cases are not the worst-case.

        Steps for a type 1 query (1 Li Ri Xi):

            len = Ri - Li + 1
            U_bound = len + 1000   # to cover all migrations

            # Reset the DSU for the range [0, U_bound] for the current query by incrementing 'now'
            now += 1
            # But note: we won't initialize the arrays, we will initialize on the fly.

            # Compute freq[x] for x in range(1,501) for the segment [Li, Ri]
            # We have precomputed for each x in [1,500] the sorted list of indices (the positions in the array A where the value is x)
            freq = [0]*501   # 1..500
            for x in range(1,501):
                # in the list for x, count the number of indices in [Li-1, Ri-1]
                # let's say the list is `positions[x]`
                # use bisect_left and bisect_right
                left_index = bisect.bisect_left(positions[x], Li-1)
                right_index = bisect.bisect_right(positions[x], Ri-1) - 1
                if left_index <= right_index:
                    freq[x] = right_index - left_index + 1
                else:
                    freq[x] = 0

            # Then for x from 1 to 500:
            for x in range(1,501):
                count = freq[x]
                for c in range(count):
                    y = find(x, U_bound)   # find the next available >= x
                    # We know y is in [x, U_bound] because we set U_bound large enough.
                    next_ptr[y] = find(y+1, U_bound)   # mark y as occupied: next_ptr[y] becomes the next available after y

            # Then compute the answer for the students: the Xi-th free number
            current = 0
            for j in range(Xi):
                current = find(current+1, U_bound)
            print(current)

        For update: 
            type 0 P X: 
                We are changing A[P-1] to X.
                We have to update the positions: remove P-1 from the list for the old value (if the old value is in [1,500]) and add it to the list for the new value (if the new value is in [1,500]).

            But note: the old value might be something we have in our positions[old] list.

            Steps for update:
                old_value = A[P-1]
                if 1<= old_value <=500:
                    # find the index in positions[old_value] and remove it (using bisect)
                    # but removal from a sorted list is O(n) -> we might use a balanced BST? 
                    # but M updates is 5000, and the total number of removal/insertion is 5000, and the total size of these lists is 10^6, so removal is O(n) per update -> worst-case 10^6 per update * 5000 = 5e9.

            Alternatively, we can use a Fenwick tree for each x? 

            Or we can avoid storing the lists and use a Fenwick tree for each x? 

            But we only need to do a range count for the segment [L,R] for a fixed x. 
                We can use a 2D structure, but x only has 500 values, so we can have a 2D array of size 500 * N? -> 500 * 10^6 = 50e6 integers -> 200 MB, and then updates are point updates.

            We can have a fenwick tree for each x in [1,500] is an overkill.

            Instead, we can maintain an array for each x in [1,500] not as a list of positions, but as a Fenwick tree over the array length? 

            Specifically, let arr_x[i] = 1 if A[i]==x, else 0.
            Then the frequency for x in [L,R] is the sum of arr_x[L..R].

            We can maintain for each x in [1,500] a Fenwick tree for the array arr_x.

            But then we have 500 Fenwick trees, and each update: 
                We update the old_value: set to 0 at position i, and the new_value: set to 1 at position i.

            Then for a query, we can get freq[x] = fenw[x].range_sum(L-1, R-1)

            Memory: 500 * (N) integers -> 500 * 10^6 = 50e6 integers = 200 MB, which is acceptable.

            Time per update: O(500 * log(N))? -> no, per update we only update two trees: the tree for the old_value and the tree for the new_value, so O(2 * log(N)) per update.

            Time per query: to get freq[x] for x in [1,500] is 500 * O(log(N)) = 500 * 20 = 10000 per query.

            Total updates: up to 5000, so total for updates: 5000 * 2 * log(N) = 5000 * 2 * 20 = 200000.
            Total for queries: 5000 * 500 * 20 = 5000 * 10000 = 50e6, which is acceptable.

        Therefore, the plan:

            Precomputation:
                N, M = ...
                A = list of length N

                # Initialize 500 Fenwick trees (for values 1..500)
                fenw = [FenwTree(N) for _ in range(501)]   # we'll use 1..500

                # Initially, for i in range(N):
                for i in range(N):
                    if 1 <= A[i] <= 500:
                        fenw[A[i]].update(i, 1)

            For an update of type 0: (0 P X)
                i = P-1
                old_val = A[i]
                new_val = X
                A[i] = new_val

                if 1 <= old_val <= 500:
                    fenw[old_val].update(i, -1)
                if 1 <= new_val <= 500:
                    fenw[new_val].update(i, 1)

            For a query of type 1: (1 L R X)
                len = R-L+1
                U_bound = len+1000

                now += 1   # for DSU reset

                # Get freq[x] for x in range(1,501): 
                freq = [0]*501
                for x in range(1,501):
                    freq[x] = fenw[x].range_sum(L-1, R-1)

                # Then simulate the DSU for the bounded range [0, U_bound] for the frequency counts
                for x in range(1,501):
                    count = freq[x]
                    for c in range(count):
                        y = find(x, U_bound)
                        next_ptr[y] = find(y+1, U_bound)

                current = 0
                for j in range(X):
                    current = find(current+1, U_bound)
                print(current)

        We need to implement Fenwick tree.

        Fenwick tree: 
            class FenwTree:
                def __init__(self, size):
                    self.n = size
                    self.tree = [0]*(self.n+1)

                def update(self, index, delta):
                    i = index+1
                    while i <= self.n:
                        self.tree[i] += delta
                        i += i & -i

                def query(self, index):
                    # prefix sum [0, index]
                    i = index+1
                    s = 0
                    while i:
                        s += self.tree[i]
                        i -= i & -i
                    return s

                def range_sum(self, l, r):
                    if l>r: return 0
                    if l==0:
                        return self.query(r)
                    return self.query(r) - self.query(l-1)

            But note: our indices are 0-indexed.

        However, the range_sum: we have to do:
            if l==0: then prefix[r]
            else: prefix[r]-prefix[l-1]

        But our query(i) returns prefix sum [0,i]. So:
            range_sum(l, r) = self.query(r) - (self.query(l-1) if l-1>=0 else 0)

        Alternatively, we can do:

            def range_sum(self, l, r):
                if l>r: return 0
                if l==0:
                    return self.query(r)
                else:
                    return self.query(r) - self.query(l-1)

        But wait, what if l-1<0? 
            Then we should do: 
                if l==0: return self.query(r)
                else: ...

        But our FenwTree supports query(-1)? no.

        Better: 
            range_sum(l, r) = self.query(r) - self.query(l-1)   [where we define query(-1)=0]

        So:

            if l==0:
                part1 = 0
            else:
                part1 = self.query(l-1)
            part2 = self.query(r)
            return part2 - part1

        Or we can change the query to be 1-indexed and handle the boundaries.

        Let's implement:

            def query(self, index):
                # if index <0, return 0
                if index < 0:
                    return 0
                i = index+1
                s = 0
                while i:
                    s += self.tree[i]
                    i -= i & -i
                return s

            def range_sum(self, l, r):
                if l>r: return 0
                return self.query(r) - self.query(l-1)

        Now, note: the total work in the query for the DSU part: 
            total_ops = 0
            for x in range(1,501):
                total_ops += freq[x]   # which is the number of migrations in the segment with starting value x

            But note: the total migrations in the segment is len = R-L+1.
            So total_ops = 2 * len   (because each migration we do two find: one for the current and one for next_ptr[y] = find(y+1))

        Therefore, the total work in the D part is O(len) per query.

        And the Fenwick tree part is 500 * O(logN) per query.

        The worst-case total work over all queries for the DSU part is the sum of (2 * len) for each query = 2 * (sum of len over queries) = 2 * (at most 5000 * (10^6)) = 10e9, which in Python is likely to be too slow.

        But we must hope that the average len is not 10^6.

        Or we can avoid the DSU for the bounded range by the bitset method with a DSU-like free list? 

        Given the time, we go with the above.

        Let's hope that the test cases have small average query length.

        Alternatively, we can use a global DSU structure that is reset by a lazy array and hope that the constants are low.

        We will implement the DSU with a list for next_ptr and a list for timestamp, and a global now.

        We will make the find function as efficient as possible.

        Note: we are using a bounded range [0, U_bound] (U_bound = len+1000). The size of the DSU array is U_bound+1, but we are not preinitializing, we are initializing on the fly with 'now'.

        We'll use:

            MAX = 0
            We will not predefine a fixed MAX, but we know that the maximum U_bound is 10^6+1000+ (because the maximum len is 10^6) -> so about 10^6+1000.

            But there are 5000 queries, and the total memory for next_ptr and timestamp might be 2 * (10^6+1000) per query? 
            But we are reusing the same arrays for next_ptr and timestamp for all queries, and we are indexing by the number (the position). The position can be as large as 10^6+1000, and we have 5000 queries, but we cannot allocate 5000 * (10^6+1000) arrays.

        Instead, we use two global arrays next_ptr and timestamp of size MAX_U = (maximum U_bound we might see) + 1.

        What is the maximum U_bound? 
            len = up to 10^6, so U_bound = 10^6+1000 = 1001000.

        Then we set MAX_U = 1001000.

        Then next_ptr = [0]*(MAX_U+2)   # for positions 0 to MAX_U, and beyond? 
        But for a position > MAX_U, our find function returns the position itself.

        We'll do:

            next_ptr = list(range(MAX_U+2))   # initially, we can set next_ptr[i]=i for all i, but then we reset by 'now'
            timestamp = [0]*(MAX_U+2)
            now = 1   # we start at 1

        But then for each query, we don't want to reset the entire next_ptr array. We use the timestamp to lazy initialize.

        How does the find function work for a given x and U_bound (which is at most 1001000) for the current query?
            if x > U_bound: 
                return x
            if timestamp[x] != now:
                # initialize the node x: next_ptr[x] = x, and timestamp[x]=now
                next_ptr[x] = x
                timestamp[x] = now
            if next_ptr[x] != x:
                next_ptr[x] = find(next_ptr[x], U_bound)
            return next_ptr[x]

        But note: next_ptr[x] might be set to a value beyond U_bound? Then in the next time, we will not store that? 

        This is the same as the sample C++ code.

        Now, we implement:

            We'll set MAX_U = 1000000 + 1000 = 1001000

        Steps for the entire solution in Python:

            import sys
            import bisect

            sys.setrecursionlimit(3000000)  # for the DSU find recursion? or we do iterative?

            But the find function might recurse up to the depth of the chain? 
                We have path compression, but the recursion depth might be the length of the chain? 
                The chain in the DSU might be of length 1001000? -> recursion depth 1001000, which is too much.

            We must avoid recursion.

        We will do iterative path compression? 

            def find(x, U_bound):
                stack = []
                while x <= U_bound and timestamp[x] == now and next_ptr[x] != x:
                    stack.append(x)
                    x = next_ptr[x]
                if x > U_bound:
                    root = x
                else:
                    if timestamp[x] != now:
                        timestamp[x] = now
                        next_ptr[x] = x
                        root = x
                    else:
                        root = x
                # Now, we do path compression: set next_ptr for all in stack to root
                for y in stack:
                    next_ptr[y] = root
                return root

        But wait, what if the next_ptr[x] points to a node that is also not root? 

        Alternatively, we can do:

            def find(x, U_bound):
                # find the root and do path compression
                start = x
                stack = []
                while x <= U_bound:
                    if timestamp[x] != now:
                        break
                    if next_ptr[x] == x:
                        break
                    stack.append(x)
                    x = next_ptr[x]
                if x > U_bound:
                    # return x
                    for y in stack:
                        next_ptr[y] = x
                    return x
                if timestamp[x] != now:
                    # initialize x
                    timestamp[x] = now
                    next_ptr[x] = x
                root = x
                # Now, traverse the stack backwards? 
                for y in stack:
                    next_ptr[y] = root
                return root

        This doesn't work for x>U_bound.

        Better: 

            def find(x, U_bound):
                if x > U_bound:
                    return x
                stack = []
                while True:
                    if timestamp[x] != now:
                        break
                    if next_ptr[x] == x:
                        break
                    stack.append(x)
                    x = next_ptr[x]
                    if x > U_bound:
                        break
                if x > U_bound:
                    for y in stack:
                        next_ptr[y] = x
                    return x
                if timestamp[x] != now:
                    timestamp[x] = now
                    next_ptr[x] = x
                root = x
                for y in stack:
                    next_ptr[y] = root
                return root

        But this might not be entirely correct.

        Alternatively, we can use the iterative method that compresses the path without recursion and without a large stack by doing two passes? 

        Given the time, we will use a non-recursive find that might be iterative and not use a stack if we are not doing path compression for the entire chain? 

        Actually, we can do iterative path splitting:

            def find(x, U_bound):
                if x > U_bound:
                    return x
                # if not initialized, initialize
                if timestamp[x] != now:
                    timestamp[x] = now
                    next_ptr[x] = x
                # Start from x, and traverse until we find the root or a node that is not in this query or beyond U_bound.
                cur = x
                while cur <= U_bound and timestamp[cur] == now and next_ptr[cur] != cur:
                    cur = next_ptr[cur]
                if cur > U_bound:
                    # update all nodes in the path to point to cur (which is x > U_bound)
                    # we traverse again to update
                    temp = x
                    while temp != cur and temp <= U_bound:
                        nxt = next_ptr[temp]
                        next_ptr[temp] = cur
                        temp = nxt
                    return cur
                if timestamp[cur] != now:
                    timestamp[cur] = now
                    next_ptr[cur] = cur
                root = cur
                # Path splitting: 
                while next_ptr[x] != root:
                    next = next_ptr[x]
                    next_ptr[x] = root
                    x = next
                return root

        But this is complex.

        Given the time, and since the and the chain length is not expected to be very long because of path compression, but worst-case the chain might be the entire range, we will use a stack for the path.

        We know that the total work is 10e9, and the average chain length might be 1 if we have path compression. 
        But in the worst-case, the first time we might have to traverse the entire chain, but then we compress.

        We'll do the stack version.

        Steps for find in the bounded range for a query:

            stack = []
            start = x
            while x <= U_bound:
                if timestamp[x] != now:
                    break
                if next_ptr[x] == x:  # root
                    break
                stack.append(x)
                x = next_ptr[x]

            if x > U_bound:
                # set all in stack to x
                for y in stack:
                    next_ptr[y] = x
                return x

            if timestamp[x] != now:
                timestamp[x] = now
                next_ptr[x] = x   # root of itself

            root = x
            for y in stack:
                next_ptr[y] = root
            return root

        But note: the first node in the stack is the start, and the last node is the one that is either uninitialized or a root.

        However, what if we break because x<=U_bound and timestamp[x]==now and next_ptr[x]==x? then we set the entire stack to that root.

        This is path compression.

        We'll implement accordingly.

        Let's code accordingly.

        Given the time constraints, we will implement the DSU with a iterative find with a stack.

        Summary of the code structure:

            MAX_U = 1000000 + 1000  # 1001000
            next_ptr = [0] * (MAX_U+2)
            timestamp = [0] * (MAX_U+2)
            now = 0

            # Fenwick tree for each x in [1,500] (501 trees, index0 unused for fenw[0])

            # Precomputation for the Fenwick trees: 
                # ... 

            for each query:
                if type0: 
                    update the Fenwick trees and update the array A.
                else:
                    len = R-L+1
                    U_bound = len+1000   # but if len+1000 > MAX_U, then we need to extend? but MAX_U is 1001000, and len<=10^6, so len+1000<=1001000, so safe.

                    now += 1   # new query for DSU

                    # Get freq[x] for x in range(1,501) using the Fenwick trees.
                    freq = [0]*501
                    for x in range(1,501):
                        cnt = fenw[x].range_sum(L-1, R-1)
                        freq[x] = cnt

                    total_occupied = 0
                    for x in range(1,501):
                        cnt = freq[x]
                        for i in range(cnt):
                            y = find(x, U_bound)
                            if y <= U_bound:
                                # we simulate: mark y as occupied by setting next_ptr[y] = find(y+1, U_bound)
                                next_ptr[y] = find(y+1, U_bound)
                                total_occupied += 1
                            # else: not necessary since we won't use it

                    # Then find the X-th free number:
                    current = 0
                    for j in range(X):
                        current = find(current+1, U_bound)
                    print(current)

        We hope that the inner loops (for the DSU) will be fast.

        But worst-case, the total work is 2 * len (for the two finds per migration) + X (<=500) per query.

        Total work over all queries: 2 * (sum of len over queries) + 500 * (number of queries).

        The worst-case sum of len over queries is 5000 * 10^6 = 5e9, so 10e9 + 2.5e6.

        10e9 iterations in Python is about 1000 seconds.

        We need to optimize the inner find.

        Alternatively, we can hope that the path compression will reduce the average steps to nearly O(1).

        Given the time, we output the code and hope for the best.

        Note: the problem has a time limit of 2 seconds in C++ and we are in Python.

        But the sample input is small.

        We'll try the sample.

        Sample: 
            "5 4"
            A = [1,3,2,4,5]
            Queries:
                "1 2 4 2" -> L=2, R=4, X=2: 
                    len = 3, U_bound=3+1000=1003
                    freq[1] = 0, freq[2]=1, freq[3]=1, freq[4]=1, freq[5]=0 (because the segment is A[1:4] (0-indexed indices 1,2,3): A[1]=3, A[2]=2, A[3]=4.
                    Then for x=2: count=1 -> 
                         y = find(2,1003): 
                            2: timestamp[2]!=now (which is 1) -> initialize: next_ptr[2]=2, then return 2.
                         then next_ptr[2] = find(3,1003) = 3 (initialize if needed) -> so next_ptr[2]=3.
                    Then for x=3: count=1 ->
                         y = find(3,1003): 
                            3: not initialized -> initialize to 3, return 3.
                         then next_ptr[3] = find(4,1003)=4.
                    Then for x=4: count=1: 
                         y = find(4,1003)=4, then next_ptr[4]=find(5,1003)=5.

                    Then we do the free numbers for X=2:
                         start at 0: 
                            find(1): 
                                1: not initialized -> initialize to 1, return 1.
                         then find(2): 
                               2: next_ptr[2]=3, so we do:
                                   find(3): next_ptr[3]=4 -> then find(4)=5? 
                                   Actually, we do:
                                        find(2): 
                                           2: next_ptr[2]=3, then we do find(3): 
                                                3: next_ptr[3]=4, then find(4): 
                                                      4: next_ptr[4]=5, then find(5): 
                                                           5: not initialized? -> then we set next_ptr[5]=5, return 5.
                         so current=5.

                    Output 5.

                Then update: "0 2 1" -> set A[1] (0-indexed index1) to 1.

                Then query: "1 1 3 2": 
                    len=3, U_bound=1003.
                    A[0]=1, A[1]=1, A[2]=2.
                    freq[1]=2, freq[2]=1, others 0.
                    For x=1: count=2
                         first: y=find(1)=1 -> next_ptr[1]=find(2)=2
                         second: y=find(1): 
                                1: next_ptr[1]=2, then find(2): 
                                      2: initialize? -> no: we will initialize 2 to 2? 
                                      so find(2)=2 -> then next_ptr[1] becomes 2? and then next_ptr[2]=find(3)=3
                         then for the second: y=find(1) -> 
                                1: next_ptr[1]=2, then we go to 2, then next_ptr[2]=3, then find(3)=3, so we return 2? 
                         But then we set next_ptr[2]=3.

                    For x=1: 
                        first: y=1 -> next_ptr[1]=2
                        second: y= find(1) -> goes to 2 (because next_ptr[1]=2, and then we do find(2) which is 2) -> then set next_ptr[2]=3.

                    For x=2: count=1:
                        y=find(2)=3 (because next_ptr[2]=3, then we find(3)=3) -> then next_ptr[3]=4.

                    Then free numbers for X=2:
                         first: find(1) -> 
                            1: not visited in the migration part? 
                                But we did use 1 and 2 in the migration part? 
                                How? 
                                In the migration part, we did:
                                    for x=1: we occupied 1 and then 2.
                                    for x=2: we occupied 3.
                                So the free numbers: 
                                    The first free number: the smallest free is 4? 
                            Let's do:
                                find(1): 
                                    1: was initialized in the migration part? 
                                        During the migration part: 
                                            for the first migration (x=1): we did find(1): we initialized it and set next_ptr[1]=2.
                                        Then for the second migration (x=1): we did find(1) and got 2, then set next_ptr[2]=3.
                                        Then for x=2: we did find(2) and got 3, then set next_ptr[3]=4.
                                    So for the free traversal:
                                        find(1): 
                                           1: timestamp[1]==now? -> yes, and next_ptr[1]=2 -> so we go to 2.
                                           2: next_ptr[2]=3 -> go to 3.
                                           3: next_ptr[3]=4 -> go to 4.
                                        then 4: not initialized -> initialize to 4, and return 4.
                                Then find(2) for the second free number: 
                                        we do find(5) -> 5? 
                                        Actually, we do:
                                            find(2): 
                                                2: next_ptr[2]=3 -> go to 3.
                                                3: next_ptr[3]=4 -> go to 4.
                                                4: next_ptr[4] was not set by the migration part? 
                                                    In the migration part, for x=2: we set next_ptr[3]=4, but not next_ptr[4]. 
                                                But in the migration part, we did not have a migration that started at 4, so we never initialized 4.
                                                So for 4: we initialize and return 4? 
                                        Then we get 4 again? 

                            This is a mess.

                    We must ensure that in the free number traversal, we also use the DSU with the same 'now' and the same next_ptr and timestamp.

                    In the migration part, we occupied 1,2,3. 
                    Then the free numbers should be: 4,5,...
                    The first student: 4, second:5.

                    How to get 4 and then 5?

                    We do:
                        current = 0
                        j=0: current = find(1) -> 
                            1: is occupied? -> next_ptr[1]=2, so we go to 2? 
                            2: next_ptr[2]=3, go to 3.
                            3: next_ptr[3]=4, go to 4.
                            4: not initialized -> set next_ptr[4]=4, return 4.
                        j=1: current = find(5) -> 5: not initialized, set next_ptr[5]=5, return 5.

                    But we did not update next_ptr for 4 to point to 5? 
                        In the free number traversal, we are not marking the free numbers as occupied? 
                        So if we do find(4) again, we would get 4 again? 
                        But the students are occupying the free numbers, so they should be marked as occupied? 
                        However, the problem: the students are not part of the recorded migrations. The set T is fixed. The free numbers are the complement of T. 
                        So we don't want to mark them as occupied in the DSU for the migration set. 

                    In our simulation, we are using the same DSU to skip the migration set T. The free numbers are still free, and we are not altering them. 
                    And that's fine: because the find for the free numbers is only for querying the next free, and we are not updating the DSU for the free numbers (because we don't mark them as occupied). 

                    How then do we skip them? 
                        We don't. The DSU for the migration set T is fixed. The free numbers are not touched by the migration simulation, so they are initialized to next_ptr[x]=x during the free traversal. 

                    So in the free traversal, we are not doing path compression for the free numbers? 

                    But that's acceptable: because the free numbers are not occupied by migrations, so they are not in the DSU as non-roots? 

                    The only non-root nodes are the ones that are occupied by migrations.

                    Therefore, for a free number x, the find(x) will return x immediately.

                    So the free numbers are reported in order.

                    Therefore, in the sample query: 
                        free1: find(1) will be 1? -> but wait, 1 is occupied by a migration -> so we skip 1,2,3 and get 4.
                        free2: find(5) -> 5 is free, so we return 5.

                    But our find(1) in the free traversal: 
                        1: timestamp[1]==now -> yes, and next_ptr[1]=2 -> so we go to 2.
                        2: next_ptr[2]=3 -> go to 3.
                        3: next_ptr[3]=4 -> go to 4.
                        4: not initialized -> we set next_ptr[4]=4 and return 4.

                    Then for the second free number: 
                        find(5): not initialized -> set next_ptr[5]=5, return 5.

                    So we get 4 and then 5.

                    But the sample output for the second query is 5, meaning the second student is at 5.

                    And the third query: 
                        "1 1 5 10"
                        len=5, U_bound=5+1000=1005
                        A = [1,1,2,4,5]  (after update)
                        freq[1]=2, freq[2]=1, freq[4]=1, freq[5]=1.
                        Simulation for migrations:
                            x=1: count=2: 
                                first: find(1)=1 -> next_ptr[1]=find(2)=2
                                second: find(1)=2 (because next_ptr[1]=2) -> then set next_ptr[2]=find(3)=3
                            x=2: count=1: find(2)=3 (because next_ptr[2]=3) -> next_ptr[3]=4
                            x=4: count=1: find(4)=4 -> next_ptr[4]=5
                            x=5: count=1: find(5)=5 -> next_ptr[5]=6
                        Then free numbers for 10:
                            We need the 10th free number.
                            The free numbers: 
                                6,7,8,9,10,11,12,13,14,15,...
                            But our DSU for the free traversal: 
                                We start at 0:
                                find(1) -> goes to 6? no, it will go through the next_ptr chain: 
                                    1->2, 2->3, 3->4, 4->5, 5->6, then 6 is free? 
                                    But we haven't initialized 6, so we return 6.
                                Then find(7) -> free, return 7.
                                ... until the 10th: 15.

                        So output 15.

                    But the sample output is 15.

        Therefore, we output the code as described.

        However, we must be cautious about the time.

        Let's hope that the test cases have short average query length.

        We'll write the code accordingly.

        Note: the sample is now passing.

        We'll now write the code.

        Steps:

            Precomputation:
                MAX_U = 1001000
                next_ptr = [i for i in range(MAX_U+2)]   # not sure if we need to initialize to i, but we will use lazy initialization with 'now'
                timestamp = [0]*(MAX_U+2)
                now = 0

                # Fenwick trees for x in 1..500
                # We'll create a FenwTree class.

            Then read input.

            Precompute the Fenwick trees: for x in range(1,501) we create a FenwTree of size N.

            Initialize the array A, and for each i, if A[i] in [1,500], update fenw[A[i]] at i with 1.

            For each of the M queries:
                if type0: update
                else: query

        We'll implement the iterative find (with a while loop and a stack) to avoid recursion depth issues.

        Let's code accordingly.

        Due to the complexity, we hope for the best.

        Note: in the migration simulation, we do not need to update the DSU for a free number if it is beyond U_bound? 

        We'll implement the iterative find (without a stack, but with a loop that is not recursive) to avoid recursion limits.

        We'll do:

            def find(x, U_bound):
                if x > U_bound:
                    return x
                path = []
                cur = x
                while cur <= U_bound:
                    if timestamp[cur] != now:   # not visited in this query: then we break
                        break
                    if next_ptr[cur] == cur:   # root, then we break
                        break
                    path.append(cur)
                    cur = next_ptr[cur]
                if cur > U_bound:
                    # set all in path to cur
                    for node in path:
                        next_ptr[node] = cur
                    return cur
                if timestamp[cur] != now:
                    timestamp[cur] = now
                    next_ptr[cur] = cur   # root
                # Now, set all in path to cur
                for node in path:
                    next_ptr[node] = cur
                return cur

        But note: it is possible that cur is <= U_bound and we break because it is a root. Then we return the root.

        This is path compression.

        Let's test on the first sample migration for x=1 in the first query:
            x=1, U_bound=1003
            path = []
            cur = 1: 
                timestamp[1] != now? (now=1, and initially timestamp is 0) -> break.
            then we set timestamp[1]=1, next_ptr[1]=1, and return 1.

        Then for the next_ptr[1] = find(2,1003): 
            cur=2: not visited -> break, then set next_ptr[2]=2, return 2.

        Then for x=3: 
            find(3): not visited, set to 3, then next_ptr[3]=find(4)=4 (which is not visited, set to 4).

        Then for the free traversal: 
            find(1): 
               1: visited, and next_ptr[1]=2 (so we go to 2)
               path = [1], then cur=2: 
                   2: visited, next_ptr[2] = 2? -> no, in the migration part we set next_ptr[1]=2, but did we change next_ptr[2]? 
                         We set next_ptr[2]=2 initially, and then when we set next_ptr[1]=2, but then for the migration part for x=1, after the first migration we set next_ptr[1]=2, but then for the second migration for x=1: 
                            find(1): 
                               1: visited, next_ptr[1]=2, then we go to 2: 
                                   2: visited, next_ptr[2] = 2? -> then we break? 
               So the path = [1] and cur=2 is a root? -> then we set next_ptr[1]=2 and return 2.

            But that's not what we want: we want to get to 4 for the first free number.

        We see the error: 
            In the migration part for the second migration for x=1 (in the first sample there was only one migration for x=1? 
            In the first query: the segment is [2,4] (days 2,3,4) -> so only one migration for x=1? 

            But in the second query: 
                for x=1: count=2, so we do two migrations.

            In the second query: 
                first migration for x=1: 
                    y = find(1) -> 1 (free, so we return 1)
                    then next_ptr[1] = find(2) = 2.
                second migration for x=1: 
                    y = find(1): 
                        1: visited, next_ptr[1]=2, so we do:
                            find(2): 
                               2: not visited: then we set next_ptr[2]=2, and return 2.
                    then next_ptr[2] = find(3)=3.

                Then for the free traversal: 
                    find(1): 
                        1: visited, next_ptr[1]=2 -> so we go to 2.
                        2: visited, next_ptr[2]=3 -> so we go to 3.
                        3: not visited? -> then we initialize and return 3.

                But the expected free number should be 4.

            What's the occupied set? 
                migrations: 1,2 -> and then for x=2: 
                    y = find(2) -> we did find(2) in the second migration for x=1, and we set next_ptr[2]=3, but we haven't marked 2 as occupied by a migration? 

            In the simulation for the second migration for x=1, we assigned the migration to position 2. 
            Then when we do the migration for x=2: 
                we do find(2) -> 
                    2: visited, next_ptr[2]=3, then we go to 3.
                    then we set next_ptr[3] = find(4)=4.

            So the occupied positions: 1,2,3.

            Therefore, the first free number is 4.

            Why in the free traversal we got 3 for the first free number? 
                Because when we did find(1) in the free traversal, we followed:
                    1->2->3, and 3 is free (because we haven't set next_ptr[3] to point to 4 in the DSU for the free traversal? 
                But we set next_ptr[3]=4 during the migration part for x=2.

            In the free traversal: 
                for node 3: 
                    next_ptr[3] = 4   (from the migration part)
                so find(1) in the free traversal: 
                    1: visited, next_ptr[1]=2 -> so we append 1 and go to 2.
                    2: visited, next_ptr[2]=3 -> append 2 and go to 3.
                    3: visited, next_ptr[3]=4 -> append 3 and go to 4.
                    4: not visited: break, then set next_ptr[4]=4 and return 4.

            So the first free number is 4.

            The error in the second migration for x=1 was that we returned 2 for the second migration, and then we updated next_ptr[2] to 3, but that's correct.

        Therefore, the find function for the free traversal does use the next_ptr set by the migration part.

        The algorithm is then: use the same next_ptr and timestamp for the entire query.

        We'll implement accordingly.

        We hope it will be fast enough.

        Let's code.