We are given n, m, k and n strings of length m.
Two students a and b are similar if there exists a set of at least k questions such that:
  - For every question j in the set, both students a and b have answered j (i.e., their j-th character is not '.')
  - For every question j in the set, the answers of a and b for j are the same.

We are to find the pair (a, b) with:
  - smallest b (if multiple pairs exist, then among those with the same smallest b, we take the largest a)
  - if no such pair, output -1.

Constraints:
  n: [2, 5000]
  m: [1, 3000]
  k: [1, 5]

Note: k is at most 5, which is small.

Approach:

A naive approach would be to check every pair (a, b) (with a < b) and for each pair, check over the m questions to see if they have at least k common questions where they both answered and the answers are the same. However, the total number of pairs is O(n^2) and each check is O(m). In the worst case, n=5000, m=3000, that would be about 5000*4999/2 * 3000 ~ 37.5e9 operations, which is too slow.

We need a more efficient method.

Observation 1: Since k is at most 5, we can think about the following: 
  For a pair (a, b) to be similar, they must have at least k common positions j where both have non-dot and the character is the same.

But note: The problem requires the pair with smallest b and then largest a. So we can iterate by b from 2 to n, and for each b, iterate a from b-1 down to 1, and check if (a,b) is similar. Then we can break as soon as we find a pair for this b? However, the problem says: if multiple pairs for the same b, we want the largest a. So for fixed b, we want the largest a (with a < b) such that (a,b) is similar. Then we take the first b (smallest b) for which such an a exists.

But the naive check per pair is O(m) and worst-case n=5000, then total operations is about (5000 * 4999 / 2) * 3000 which is too high.

We need to precompute or use a faster way to check a pair.

Observation 2: Instead of iterating over pairs, we can try to represent each student by the set of questions they answered? But note: we care about the common questions with same answer.

Alternatively, note that k is small (at most 5). We can try to use the following idea:

  For each question j, we can consider it as a feature. But we are not just interested in the presence, but also the exact character.

But note: if a student didn't answer a question, we ignore that question for him.

We are to count, for a pair (a, b), the number of questions j for which:
   S_a[j] != '.' and S_b[j] != '.' and S_a[j] == S_b[j].

We need this count to be at least k.

We can precompute for each pair of students? That would be O(n^2 * m) which is too high.

Another idea: iterate by the student b (from 2 to n) and then for each previous student a (from b-1 down to 1) we want to quickly compute the number of common positions where both answered and the same.

But we cannot iterate over m for each pair.

Alternative approach: For each student, we can record the positions where they answered (non-dot). Then, for two students, we can merge the lists of positions that both have non-dot? But the positions are up to 3000, and then we have to check the character. And the number of pairs is O(n^2). The worst-case total number of non-dot positions per student is m=3000, and if every student has all 3000, then merging two lists of size 3000 for each pair would be 3000 * (n choose 2) = 3000 * 12.5e6 = 37.5e9, which is too high.

We need a better idea.

Observation 3: We are only required to know if the count is at least k. We don't need the exact count if it's more than k. So we can break early: for a fixed pair (a,b), we traverse the positions and count the matches. If we reach k, we can stop. However, worst-case if k=5 and we break after 5 matches, then we might do 5 * (n choose 2) which is 5 * 12.5e6 = 62.5e6, which is acceptable? But note: worst-case m=3000, and if we break early only when we have found k, then worst-case when k=5, we might have to check until we find 5 matches. However, if two students have many matches, we break after 5. But if they have less than k, we have to check all positions? Then worst-case when no pair is similar, we do 3000 * (n choose 2) = 3000 * 12.5e6 = 37.5e9 which is too high.

But note: the constraints say that n can be 5000, and m 3000. 37.5e9 operations (each operation a character comparison) is too high in a 3s time limit (in C++ maybe, but in Pyton, no). We must design an efficient solution.

Alternative idea: we can precompute for each student a bitmask or a set of (position, character) but we cannot iterate over sets for each pair.

Another idea: since k is small, we can use the following: 
  For each pair (a, b) we are only concerned with the positions j that are answered by both. And we want to know if the number of positions j for which the character is the same is at least k.

But note: we can also note that if we have a position j, then if either a or b has a dot at j, we skip. Otherwise, we compare.

But we want to avoid iterating over all m for each pair.

How about we precompute an array for each student: a list of the indices j where they have non-dot? Then, for a pair (a,b), we can iterate over the common positions? But the common positions are the intersection of the two sets. The size of each set is at most m, and the intersection could be large. However, we can break early: we only need k matches. 

But worst-case, if we have two students with many common positions but no k matches? Then we have to check all common positions. The total over all pairs might be high.

Alternatively, we can use an indexing for each position? 

Let me try: 
  Precompute for each question j, the list of students that answered it (with non-dot) and the character they wrote.

Then, for a fixed pair (a,b), we want to count the number of j such that both a and b are in the set of students that answered j and they have the same character.

But we cannot iterate over all j for each pair.

Another idea: use hashing? 

But note: we are not aggregating over all pairs, we are searching for a pair that meets a condition.

We can try to use the following: 
  Since k is small, we can consider that a pair is similar if they share k common positions with the same character. 

We can use an inverted index: for each (j, c) [meaning at position j, the character c], we have a list of students that have c at position j.

Then, for each j, and for each character c that appears at j, we can record the students that have c at j.

Now, if two students a and b have the same character c at a position j, then we can say that the pair (a,b) gets a point from j.

We want to count for each pair (a,b) the number of positions j that contribute a point. And then we want the first pair (by our criteria) that has at least k.

But the number of pairs might be huge, we cannot iterate over all pairs.

Alternatively, we can use a 2D array to count? That would be O(n^2) memory which is 25e6 integers (about 100 MB) but n=5000 -> 25e6 pairs, which is acceptable? But then we have to update for each (j,c): for each pair of students in the list for (j,c), we increment the count for that pair. Then we scan the pairs.

But the worst-case: for a particular (j,c), if the list has size L, then we do O(L^2) to update every pair. The total over all (j,c) could be very high. For example, if one question j has all n students with the same character, then L = n, and we do O(n^2) for that j. Then for m=3000, worst-case if every j has all n students, then total operations would be 3000 * O(n^2) = 3000 * 25e6 = 75e9, which is too high.

We need a more efficient method to aggregate the counts.

Note: We don't need the exact count for every pair. We only need to know if a pair has at least k. And we are also only interested in the pair with smallest b and then largest a.

We can do:

  We iterate b from 1 to n (but we want a < b, so for each new b we can check with previous a's). For each new student b, we want to update the common count for each previous student a. How?

  We can maintain an array `common_count` of size n (for the previous students) such that `common_count[a]` is the current number of positions j (that we have processed) where both a and b have the same non-dot character.

But we have to process the new student b: for each position j where b has a non-dot character, say c, then we can look at which previous students a have a non-dot character at j and that character is also c. Then we can increment `common_count[a]` for each such a.

But then we have to iterate over all j for b, and for each j, we iterate over the list of previous students that have a non-dot at j and with character c. Then we update `common_count[a]`. Then we check: for a from b-1 down to 1, if `common_count[a] >= k`, then we have a candidate: (a, b). Since we iterate a from b-1 down to 1, the first a we meet that satisfies the condition is the largest a? Actually, we want the largest a for this fixed b. So we can iterate a from b-1 down to 1 and break as soon as we find one that has common_count[a] >= k? But note: we update the counts as we process the positions of b. So we can do:

  Initialize an array `cnt` of length n (for indices 0 to n-1) to zeros. (We index students from 0 to n-1, but note: the problem uses 1-indexed.)

  For b from 0 to n-1 (representing the b-th student, 0-indexed; then the student index in output is b+1):
      For each position j in [0, m-1]:
          if the b-th student has a non-dot at j, then:
              Let c = S_b[j]
              Then, for each previous student a (with index a in [0, b-1]) that has at position j the same non-dot character c, we do:
                  cnt[a] += 1
                  // If after this update, cnt[a] >= k, then we note that (a, b) is a candidate? But we want the largest a for the current b. However, we are updating a's in arbitrary order.

      Then, after processing all j for this b, we check for a from b-1 down to 0: 
          if cnt[a] >= k, then we have found a candidate (a, b). Since we are iterating from the highest a (which is b-1) downward, the first candidate we meet is the largest a? Actually, we iterate from b-1 down to 0: we start at a = b-1, then a = b-2, ... until we find one. Then we break. Since we want the largest a, we start at the largest a (which is b-1) and go backwards until we find one.

But note: we are updating the counts for a's arbitrarily as we iterate j. So the order of j does not matter? The count for a student a is the total number of positions j (so far in the current b) that both a and b have the same non-dot character.

However, we must consider: we are processing all j for the current student b. Then after that, we check the counts for all a's.

But the problem: for each j, we need to know which previous students a have the same non-dot character at j. How to do that efficiently?

We can precompute for each j and each character c, the list of student indices (that we have seen so far, i.e., with index < b) that have c at j. Then for a fixed b and j, we can get the list of a's that we need to update.

But then, for each j in the current student b, we iterate over the list for (j, c) and update the count for each a in that list.

How much time does it take? 

  Total work over all b and j: 
      For each student b, we iterate over each j (m positions). For each j, if the student b has a non-dot character c, then we iterate over the list L_{j,c} (the list of previous students that have character c at j) and update the count for each a in that list.

  The total work is the sum over j and c of the number of times we process a student with (j,c) multiplied by the size of the list L_{j,c} at the time.

But note: each time we process a student b with (j,c), we add b to the list for (j,c) after processing? Actually, we are iterating b from 0 to n-1. We want the list for (j,c) to contain only students with index < b. So we can build the list as we go.

The total work is the same as: for each (j,c), we add each student that has (j,c) to the list, and when we process a new student b with (j,c), we iterate over the entire current list (which is the list of all previous students with (j,c)). Then the total work for a fixed (j,c) is O(n^2) in the worst-case (if all n students have (j,c), then the work for that (j,c) is 0 + 1 + 2 + ... + (n-1) = O(n^2)). 

Then over all (j,c): worst-case, if for one j, all students have the same character c, then we do O(n^2) for that j. Then for m=3000 and one j, that is 3000 * O(n^2) = 3000 * (5000^2) = 3000 * 25e6 = 75e9, which is too high.

But note: we have m positions and each position has at most 26 characters? Actually, for a fixed j, we have one list per character. But if all students have the same character at j, then we have one list for that j. So the worst-case total over all j is the sum_{j} [ (number of students that have non-dot at j) * (number of students that have non-dot at j and same character? Actually, we break by character) ].

But worst-case, if for a particular j, all students have the same character, then the list for (j, c) will be of size the number of students that have non-dot at j (which might be n). Then the total work for that j is O(n^2). And if every j is like that, then total work is m * O(n^2) = 3000 * 25e6 = 75e9 operations, which is too high.

We need to avoid this.

Alternative approach: 

  We want to know for each pair (a,b) the number of j where both answered and same character, and we want to know if it's >= k.

  Since k is small (at most 5), we can use the following: 
      If two students have at least k common positions with same character, then there must exist a set of k positions that are common and same. 

  We can try to use a randomized hashing? But the problem requires deterministic and we want the first pair by the criteria.

Another idea: 
  We can precompute a signature for each student that allows fast comparison? 

  However, note: we don't need the exact count, we only need to know if the count is at least k. 

  We can use the following: 
      For each student, we represent the set of (position, character) pairs. Then, for two students, we want the size of the intersection of their sets. 

  But we cannot do set intersection for each pair.

  We can use min-hashing? But that is probabilistic and we need exact.

Alternative idea: we use the fact that k is small. We can try to sample k positions? But we don't know which ones.

But note: we are iterating by b, and we want to check with all previous a's. 

  We can precompute for each student a list of the positions they answered. Then, for each previous student a, we can use a bitset? But m is 3000, which is too big for bitset of 3000 bits for each student? We have n=5000, so 5000 * 3000/8 = about 1.875e6 bytes, which is acceptable. Then we can do:

      For each student, we create a bitset of m bits: where bit j is 1 if the student answered j and the character is ...? But we cannot use the same bit for different characters.

  Actually, we want to count the positions j where both a and b have the same character. But the character matters. So we cannot ignore the character.

  We can create for each student a vector of m characters, but then to compute the common positions we can do:

      common = (bitset_a & bitset_b)   ... but then we also have to check the character? 

  Alternatively, we can create a separate bitset for each character? Then for 26 characters, we would have 26 * n bitsets. The memory would be 26 * n * (m/8) = 26 * 5000 * 375 ~ 26 * 1.875e6 = 48.75e6 bytes, which is about 50 MB, acceptable.

  Then for a pair (a,b): 
      total = 0
      for each character c in 'A' to 'Z':
          total += (bitset_a[c] & bitset_b[c]).count()
          if total >= k: break out of the character loop.

  Then if total>=k, the pair is similar.

  However, worst-case: if k=5, we might break early. But worst-case when the pair is not similar, we do 26 bitset ANDs and counts. The count operation for a bitset of size 3000 is O(m) but with machine words, it is O(m / word_size). Typically word_size=64, so 3000/64 ~ 47 operations per bitset. Then 26 * 47 ~ 1200 operations per pair? Then total operations over all pairs: 12.5e6 * 1200 = 15e9, which might be acceptable in C++ but in Python? The problem time limit is 3s. But 15e9 operations in C++ might be borderline (with optimization), but in Python it is too slow.

But note: we are iterating b from 0 to n-1, and for each b, we want to check previous a's from b-1 down to 0 until we find one that satisfies. We hope that we find one quickly? But worst-case, we might have to check all previous a's for each b. So worst-case we do 15e9 operations.

But 15e9 operations in 3s is 5e9 operations per second, which is challenging even in C++. We need a faster method.

We can try to break early in the character loop: we do:

      total = 0
      for each character c:
          temp = (bitset_a[c] & bitset_b[c]).count()   # this is an O(m) operation? Actually, bitset count is O(m/word_size) but that is fixed per bitset.

But note: we cannot break inside the count operation. We have to compute the entire count for one character at a time.

Alternatively, we can avoid the inner loop over characters by having one big bitset for each student that is the union of answered positions? But then we lose the character.

Another idea: we can precompute for each student a list of the positions they answered (the indices). Then for two students, we can iterate over the common positions by merging the two lists? The size of the list for a student is the number of non-dot answers, say d_i. The worst-case d_i = m = 3000. Then for two students, the intersection can be computed in O(d_i + d_j). Worst-case 6000 per pair. Then total 12.5e6 * 6000 = 75e9, too high.

We need to break early: we only need to know if the common count is at least k. So we can iterate over the shorter list and count the common positions. When the count reaches k, we break. How to iterate? We can do:

      common = 0
      for each position j in the list of student a (or b, we take the shorter one) and check if the other student has the same character at j? But note: we stored the list of positions for the student, but we don't store the character? Actually, we do: we can store for each student a vector of (j, c) for each non-dot. But then when we iterate over the list of a, we can check: for a position j, we look at the character of a at j and then we check the character of b at j? But we don't have b's entire string stored? We do, but we need random access to b's character at j? We can store the entire grid.

  Actually, we store the grid: we have an array of n strings, each of length m.

  Then for a pair (a,b), we can iterate over the set of positions that a answered (which is at most m) and for each such position j, if b also answered j (i.e., not dot) and S_b[j]==S_a[j], then we increment the count. And if we reach k, break.

  Similarly, we can iterate over the student with fewer non-dot answers. The worst-case for one pair is min(d_a, d_b). The total work over all pairs is the sum_{a<b} min(d_a, d_b).

  What is the worst-case for this sum? If every student has all m=3000 non-dots, then min(d_a,d_b)=3000 for every pair. Then total work = 3000 * n(n-1)/2 = 3000 * 12.5e6 = 37.5e9, which is too high.

But note: k is at most 5. We can break as soon as we find k matches. So for a pair, we iterate until we have found k matches. The worst-case for a pair is O(min(d_a, d_b)) but if k=5, then we break after 5 matches. So per pair, we do at most min(d_a, d_b, 5) * (cost per position) but actually we break after k matches. So per pair, we do at most k * (cost to find one match? not exactly) because we might check more than k if we don't break immediately? Actually, we break as soon as we have found k matches. But we might check positions that are not common? 

Actually, we iterate over the non-dot positions of one student, and for each such position, we check the other student. But if the other student has a dot or a different character, we skip. So we only count the common matches. And we break after we have counted k.

So per pair, we do at most min(d_a, d_b) checks, but if the pair has at least k matches, we break after k matches. If the pair has less than k, we do min(d_a, d_b) checks.

Worst-case overall: if there is no similar pair, then we do for every pair (a,b) min(d_a, d_b) checks. And if every student has m non-dots, then min(d_a, d_b)=m=3000 per pair, and total 12.5e6 * 3000 = 37.5e9 checks.

But 37.5e9 character comparisons in 3s? In C++, one comparison is a few cycles. 37.5e9 operations might be borderline in C++ (if we can do 1e9 per second, then 37.5 seconds). But the problem says time limit 3s.

We must optimize further.

How to reduce the work in the worst-case (no similar pair)?

  We can try to skip pairs that obviously cannot have k common matches? 

  Note: if the total number of non-dot positions in student a is less than k, then we can skip a entirely? Similarly for b. But worst-case, if every student has m non-dots (which is at least k, since k<=5), then we don't skip.

  Alternatively, we can precompute the total number of non-dot positions for each student. Then if for a pair (a,b), min(|a|, |b|) < k, then they cannot have k common matches? Actually, no, because common matches are limited by min(|a|,|b|). So if min(|a|,|b|) < k, then we skip the pair.

  This might help in some cases, but worst-case (all have m non-dots, m=3000>=k) then we don't skip.

Another idea: we can use indexing per student by the positions they answered. But then to compute the common positions quickly, we want to know the common ones without iterating over all positions.

But we only need to find k common positions. 

  We can store for each student the set of non-dot positions. Then for a pair (a,b), the common positions are the intersection of the two sets. We can iterate over the smaller set and check if the character matches? But that is what we are doing.

  And if the two students have many common positions but no k matches? We still have to check until we find k or exhaust.

But worst-case no similar pair, and we do min(d_a,d_b) per pair.

Total work: sum_{a<b} min(d_a, d_b).

What is the worst-case of this sum? It is maximized when all d_i = m. Then it is O(n^2 * m) = 12.5e6 * 3000 = 37.5e9.

But we must do better.

We can change the order: iterate by b, and for each b, we iterate over the non-dot positions of b, and for each non-dot position j, we iterate over the students a that have a non-dot at j and have the same character as b at j, and then update the count for the pair (a,b). Then we can break for a fixed b as soon as we find a previous student a that has reached count k.

But note: we want for fixed b the largest a (with a < b) such that the count>=k. So we have to check all a's for the current b? Not necessarily: we can update an array `cnt` for each a as described earlier, and then after processing all j for b, we check from a=b-1 downto 0 for the first a with cnt[a]>=k.

But the work for one b is: for each non-dot position j in b, we iterate over the list of previous students a that have non-dot at j with the same character. The work for one b is the sum_{j in positions of b} |L_{j,c}|, where L_{j,c} is the list of previous students (with index < b) that have non-dot at j and character c (which is the same as b's c).

Then the total work over all b is the sum_{j} sum_{c} [ |L_{j,c}| * (number of times a student with (j,c) appears after the ones in L_{j,c}) ].

But note: for a fixed (j,c), the list L_{j,c} grows. When we process a student b with (j,c), we will iterate over the entire current list. Then later we add b to the list. 

The total work for a fixed (j,c) is: 0 + 1 + 2 + ... + (n_{j,c}-1) = O(n_{j,c}^2), where n_{j,c} is the number of students that have (j,c).

Then the overall work is sum_{j} sum_{c} [ n_{j,c}*(n_{j,c}-1)/2 ].

In the worst-case, for one j, one character c, n_{j,c} = n (all students have the same character at j), then the work for that (j,c) is O(n^2). Then for m=3000, worst-case total work is 3000 * O(n^2) = 3000 * 25e6 = 75e9, which is too high.

But note: k is only up to 5. Do we need to update every pair? We only care about the count per pair (a,b) and we only care about if it reaches k. We can break out of the update for a fixed b as soon as we find that for one a, the count reaches k? But then we have to continue to find the largest a? 

Actually, no: even if one a reaches k, we want the largest a. So we have to process all j for b, because a larger a might get more matches.

But note: we are processing all j for b, and then after that we scan a from b-1 downto 0. So we must update the counts for all a's.

However, if we find during the updates that one a has reached at least k, we cannot break the j-loop because a larger a might appear later in the lists? Actually, we are iterating over j, not over a. And the lists for different j are independent. We have to update for every a in the lists.

But we can try to break after we have processed all j and then find the largest a that has>=k. 

So the work for the updates is fixed per (j,c) and student b.

We need a method that is output-sensitive in k.

Alternative approach meeting the constraints:

  Since k is small (at most 5), we can try to find a witness of k common matches.

  For a pair (a,b) to be similar, there exists a set of k positions j1, j2, ..., jk such that for each i, S_a[ji] = S_b[ji] and both are non-dot.

  We can try to use hashing to generate a fingerprint for each student for every subset of positions of size up to k? But the number of subsets is C(m, k) which is huge.

  Or we can use the following: 
      For each student, and for each question j, we can ignore the question or not. But we want at least k.

  Another idea from the fact that k is small: we can use the inclusion of a student a in the same "groups" defined by a particular set of k matches.

  But then we are indexing by groups, which are defined by (j1, j2, ..., jk) and then the characters at those positions. The number of groups is O(m^k * 26^k) which is 3000^5 * 26^5, which is astronomical.

 We cannot do that.

Given the constraints (n=5000, m=3000, k<=5) and the difficulty, we might need to use the following:

  We iterate b from 1 to n (i.e., the second student in the pair), and for each b, we iterate a from b-1 down to 1, and check if (a,b) has at least k common matches. But we hope that in practice, we find a candidate early for b. But the problem requires the smallest b and then largest a, so if we find for b=2 a pair (1,2) that is similar, we output (1,2) and stop. But the problem does not say we can stop. Because there might be a pair (1,2) and (1,3) and (2,3), and we want the pair with smallest b. The smallest b is 2, and then for b=2, the only a is 1. So we output (1,2). But Sample Input #2: 
       3 3 1
       BBC
       ..C
       .BC
  Output: 1 2.

  So for b=2, student1 and student2: 
        S1 = "BBC", S2 = "..C"
        common positions: only j=2 (0-indexed) with 'C'. That is 1 common match. k=1 -> valid.
  Then we output (1,2) and we don't care about b=3.

  Therefore, we can do:

      for b from 1 to n:
          for a from b-1 downto 1:
              if the pair (a,b) has at least k common matches, then output (a,b) and return.

      if no pair found, output -1.

  But then the worst-case is when no pair is found, we do O(n^2 * m) or O(n^2 * min(d_a, d_b)) work, which is 12.5e6 * 3000 = 37.5e9.

  However, we break per pair as soon as we find k common matches. In the worst-case (no pair is found), we do for every pair the full min(d_a, d_b) work.

  But 37.5e9 operations is too high.

But note: the worst-case might be when there is no similar pair, and every student has the maximum number of non-dots, and we have to do 3000 * 12.5e6 = 37.5e9 operations.

In C++ this might be borderline: 37.5e9 operations might take around 37.5 seconds on a machine that does 1e9 operations per second. But we might optimize by using a break when the remaining positions in the smaller list is not enough to reach k (i.e., if the smaller list has size L and we have already found x matches, and L - (current index) < k - x, then break). This might help a little, but in the worst-case where no matches, we still do L work per pair.

Alternatively, we can hope that the data is not worst-case.

But the problem says n up to 5000 and m up to 3000, so we must handle worst-case.

Another idea: for each student, we can store the non-dot positions in a compressed manner, and then for the pair (a,b), we can iterate only over the positions that are non-dot in both? 

 We can try to create for each student a list of non-dot positions. Then for two students, the common positions are the intersection of two sorted lists. We can iterate in merge-like fashion. The size of the intersection might be the minimum of the two lists, but the merge takes O(|list_a| + |list_b|). 

 But then, if we only need to find k matches, we can break after k. However, the merge might not have to go over the entire lists if we find k matches early. But if there are no matches, we go over the entire lists.

 The work per pair is O(|list_a| + |list_b|). In the worst-case, each list has m=3000, so 6000 per pair. Total 12.5e6 * 6000 = 75e9, which is even worse.

 Therefore, it is better to iterate on the smaller list.

 How to do: 
   if |list_a| < |list_b|, then iterate on list_a: for each position j in list_a, check if j is in list_b and if the character at j for a and b is the same. 
   else, iterate on list_b.

 But checking if j is in list_b: if list_b is sorted, we can binary search? Then for each j in list_a, we do a binary search in list_b to see if j is there, and if so, then compare the characters.

 Then the work per pair is O(|smaller_list| * log(|larger_list|)).

 In the worst-case, the smaller list has size 3000, then per pair: 3000 * log2(3000) ~ 3000 * 12 = 36000 operations.

 Total work: 12.5e6 * 36000 = 450e9, which is way too high.

 So we cannot do that.

Therefore, we return to the method of iterating over the smaller list and checking the character in the other string by direct access. 

  Work per pair: O(|smaller_list|) = O(min(d_a, d_b)).

  Total work: sum_{a<b} min(d_a, d_b).

  In the worst-case, all d_i = m = 3000, then sum_{a<b} 3000 = 3000 * (n*(n-1)/2) = 3000 * 12.5e6 = 37.5e9.

  In C++, if each comparison is a single operation, then 37.5e9 operations might be borderline in 3 seconds on a fast machine (which can do 1e9 operations per second, then 37.5 seconds) -> too slow.

 We must optimize further.

One optimization: 
   if min(d_a, d_b) < k, skip the pair.

   And if during the iteration we have: 
        count = 0
        remaining = min(d_a, d_b)
        if count + remaining < k: then break   [but we are iterating, so we can break if the number of positions left is less than k - count]

   This might help in some cases, but in the worst-case (no match and min(d_a,d_b)=3000) we still do 3000 work per pair.

Another optimization: 
   We iterate in an order that might find the matches faster. For example, if we reindex the positions randomly, then matches might be found earlier. But worst-case still exists.

Given the constraints and the fact that k is only up to 5, we might rely on the fact that in practice the students have sparse non-dot answers.

But the problem does not guarantee sparsity.

However, note that m=3000 is not too large, and n=5000, and the worst-case 37.5e9 operations might be acceptable in C++ if optimizations are applied (like cache-friendly access). But in Pyton, we cannot.

Since this is an editorial for a competitive programming problem, we assume a fast language like C++.

But the problem does not specify language, so we must design an algorithm that is theoretically efficient.

We return to the idea of inverting the index and updating a count array for each b.

  Total work is sum_{j} sum_{c} [ n_{j,c} * (n_{j,c}-1) / 2 ].

  In the worst-case, this is O(m * n^2) = 3000 * 25e6 = 75e9, which is worse than the pairwise min(d_a,d_b) method.

But note: if the answers are not worst-case, it might be faster.

Alternatively, we can use a vector for each a to store the current common count with b, and for each b, we initialize a new vector of length b (for a=0 to b-1) to 0. Then for each non-dot position j in b, for each a that has the same non-dot character at j, we ++ to common_count[a]. The work for b is sum_{j in b} |L_{j,c}|.

  The total work is sum_{j} (n_{j} * frequency of students at j) )? Actually, for each (j,c), the number of times we iterate over the list is the number of students that have (j,c) (because for each such student, when it is b, we iterate over the list for (j,c) which is the previous students with (j,c)). Therefore, the total work is sum_{j,c} [ n_{j,c} * (n_{j,c}-1)/2 ].

  This is the same as before.

  But note: if we only want to know if the count for a reaches k, we can break out of the entire b loop as soon as we have found for the current b an a (with the largest a) with count>=k. But the work for b is fixed: we have to go through all non-dot positions and update all a's.

  Alternatively, we can use a different strategy for updating: 
        for each non-dot position j in b, and for each a in the list L_{j,c} (previous students with the same (j,c)), we do:
             count[a]++
             if count[a] >= k, then candidate = max(candidate, a)

        then after the position j, we can check if there is any candidate and then after all j, we choose the largest candidate for b.

  Then for b, if we found a candidate, we can use the largest a that became>=k at any update. But note: it is possible that a1 becomes>=k at an early j, and then a2 (which is larger than a1) becomes>=k at a later j. So we cannot break early in the j-loop.

  Therefore, we have to process all j for b.

  Then the work for b is the sum_{j in b} |L_{j,c}|.

  The total work is sum_{j,c} n_{j,c} * (number of times a student with (j,c) is used as b) = sum_{j,c} [ n_{j,c} * (n_{j,c} - 1) / 2 ] because each pair (a,b) with a<b and both in (j,c) will be counted once for b and a in the list.

  This is the same as before.

Given the time, we must choose the method that has a good chance in practice.

 I have an idea: since k is small, we can stop the update for an a as soon as its count>=k, because we only care about if it's>=k. But then we can remove a from all future lists? 

  For example, we maintain for each (j,c) a list of students a that have (j,c) and that currently have count<k. When updating for a new student b with (j,c), we iterate over the list for (j,c) and update count[a] for each a in the list. If an a's count becomes>=k, then we remove a from all lists? 

  But the lists are per (j,c) and there are m*26 lists. When a student a count becomes>=k, then for this a, we remove a from every list (j,c) that a is in. This is expensive: a might be in many lists.

  Alternatively, we can mark a as 'finished' and then in the future when we process a (j,c) and see a in the list, we skip. But then the lists are not physically reduced, but we can use a flag for a. Then the work for b is still the same: for a (j,c), we iterate over the entire list, but then for each a in the list, we check the flag. If the flag is set (count>=k), then we skip. This might reduce the work for future b's, but for the current b, we do the same work.

  This helps in later b's, but for the current b, we do the same.

  And in the worst-case, no a reaches k, then we do the same as before.

  Also, the work to remove a from all lists is not done.

 Given the complexity of the removal, and the fact that the worst-case work is still O(m*n^2) for the first method, we must choose the pairwise method with min(d_a, d_b) and break when found>=k.

 In C++, 37.5e9 is borderline. We can try to optimize by:

   - Using contiguous memory: store the grid in a 2D array of char, and store for each student the list of non-dot positions as a vector of int (positions) and also store the compressed representation of the non-dot positions for each student.

   - For a student a, store: 
          int da = number of non-dot positions.
          int[] positions = sorted positions (or not sorted? we don't care) of the non-dot indices.

   - For each pair (a,b) (with a<b), we do:
        if min(da, db) < k: skip.
        if da <= db:
             for each position j in the positions of a:
                 if b has a non-dot at j and grid[b][j] == grid[a][j]:
                     count++;
                     if count>=k: break out of the loop and mark this pair as valid.
        else:
             similarly with a and b swapped.

   - We iterate b from 2 to n, and for each b, iterate a from b-1 down to 1, and if we find a valid pair, then we have the largest a for this b (because we are iterating a from b-1 downto 1, so we find the largest a first that is valid).

   - Then we output (a,b) and return.

   - If we don't find for this b, we go to next b.

   - If no b yields a pair, output -1.

 In the worst-case (no valid pair), the work is sum_{a<b} min(da, db).

 We can only hope that in average the min(da, db) is small.

 But the problem says the students might have many non-dots.

 However, note the sample inputs have around m=3 to 12.

 If in worst-case m=3000 and n=5000, then 37.5e9 might be too high in Pyton, but in C++ with compiler optimizations and if we do it in a tight loop, it might pass in 3 seconds on a fast judge machine? 

 Let me calculate: 37.5e9 iterations. Each iteration: 
        - Access to the list of positions for a: not if we do for the student with the smaller number of non-dots, we iterate on its non-dot positions.
        - For each position j, we access: 
              grid[b][j] and grid[a][j]

   This is two array accesses.

   In C++, a typical array access is nanoseconds.

   If we can do 1e9 iterations per second, then 37.5e9 would take 37.5 seconds.

 We need to speed up by a factor of 12.5.

 But note: the cache performance. If the grid is stored by rows (each row contiguous), then for fixed a and b, we access for a: a fixed row (cache friendly) and for b: also a fixed row (cache friendly). But when iterating a for fixed b, the a changes, so we have to bring new rows for a. This might cache thrash.

 We can iterate a from b-1 down to 0, and for each a, we bring the entire row of a and the row of b. If the row is 3000 chars, then 3000*sizeof(char) = 3000 bytes, which is 3KB, and cache might hold many of these.

 Alternatively, if we store the entire grid in a 2D array in row-major, then the rows for the students are in contiguous memory, and we might have good cache locality for the a's that are close in index.

 But we are iterating a from b-1 downto 0: the a's are in increasing index in memory? Our array for students is by index, so student0, student1,..., student_{n-1}. When we access a = b-1, then a = b-2, etc., we are going backwards in memory, which is not a problem for cache.

 But the rows for a and b might be far apart? For a and b, we access two random rows. If the grid is stored as a vector of vectors, then the rows might be non-contiguous.

  Better to store as a single vector of size n*m, and then use student i at positions [i*m, i*m+m-1].

  Then for student a: pointer = base + a*m, and for student b: base + b*m.

  When iterating a for fixed b, the accesses for a are to a row that is (a-b) * m away from b's row. This might cache miss.

  Alternatively, we can precompute for each student the list of non-dot positions and the characters at those positions stored in a compact array. Then for a student, we only iterate over the non-dot positions, and for each non-dot position j, we need to get the character for the other student at j. To get the character for student b at j, if we have the grid, it is grid[b][j]. This is one access per non-dot position.

  The memory access for b's row might be cached for fixed b, because we are checking the same b for many a's. But for each a, we might have to bring a new row for a.

 Given the time constraints, and since the problem has been given with 3s time limit in C++, we assume that the intended solution is to hope that in many pairs we find k matches quickly or that the min(da, db) is small.

 In fact, the sample input #4 has only 4 students and 12 questions, and we are to find (2,3) with k=2.

 The intended solution might be the pairwise method with break when found>=k.

 We will implement in C++ and hope that worst-case is rare or that the constant factors are low.

 But note: the constraints: n=5000, m=3000, and in the worst-case (no match) we do 37.5e9 iterations. This is borderline in C++.

 We can try to use SIMD? not manually.

 Alternatively, we can use a different method for students that have many non-dots: 
        if min(da, db) is large, then we might use a temporary array to

 but we not.

 Given the complexity, we output the following solution:

   - Precompute for each student i, a vector `v[i]` of the positions j where the student has a non-dot.
   - Also, store the grid in a 2D array `grid` (0-indexed: student i, position j).

   - For b from 1 to n-1 (0-indexed student index for the larger student in the pair), 
        for a from b-1 down to 0:
            int count = 0;
            if (v[a].size() <= v[b].size()) {
                for (int j : v[a]) {
                    if (grid[b][j] != '.' && grid[a][j] == grid[b][j]) {
                        count++;
                        if (count >= k) break;
                    }
                }
            } else {
                for (int j : v[b]) {
                    if (grid[a][j] != '.' && grid[a][j] == grid[b][j]) {
                        count++;
                        if (count >= k) break;
                    }
                }
            }
            if (count >= k) {
                // we found a pair (a,b) with a<b (0-indexed: a and b, but output a+1, b+1)
                // and because we are iterating a from b-1 down to 0, this a is the largest a for this b.
                cout << a+1 << " " << b+1 << endl;
                return 0;
            }

   - If no pair found, output -1.

 But note: the problem says: if there is more than one pair, find the one with the smallest b. In our iteration, we are iterating b from 0 to n-1 (0-indexed) and for each b from smallest to largest, and for a from largest (b-1) down to smallest, and we return the first pair we find. This yields the smallest b and then the largest a for that b.

  Because if for a small b we find a valid a (which is the largest a for that b), then we output it and it has the smallest b possible.

  So this meets the criteria.

  Let's test with the samples.

  Sample 1: 
        n=3, m=3, k=2, 
        S0 = "BBC", 
        S1 = "..C", 
        S2 = ".BC"

        b=1 (student1, 0-indexed student1 is the second student; in the problem, student index1 is the first string, so careful: our array:
            student0: "BBC"
            student1: "..C"
            student2: ".BC"

        We iterate b=1 (which is the second student, so in 0-indexed, the output will be a+1 and b+1, and b+1=2).
        a=0: 
            v0: positions: [0,1,2] (because no dot)
            v1: positions: [2] 
            min: size=1 -> skip because 1<k (k=2) -> then we use the condition: if min(da,db) < k, skip? Actually, in our code we do not skip, because we might find>=k in the larger set? 

        But wait: if min(da,db) < k, then it is impossible to have>=k common matches. So we can skip the pair.

        However, in the code above, we do not skip, we iterate over the smaller list. In this case for a=0 and b=1, the smaller list is v1 (size1). We iterate j in v1: j=2.
            grid0[2] = 'C', grid1[2] = 'C' -> count=1, not>=2.
            then count=1<2, so not valid.

        Then b=2 (third student)
        a=1: 
            v1: [2], v2: [1,2] -> smaller is v1 (size1). Iterate j=2 in v1:
                grid2[2] = 'C' and grid1[2]=='C' -> count=1<2 -> not valid.
        a=0:
            v0: [0,1,2], v2: [1,2] -> min size=2. Iterate over v2 (because.size()=2<3) -> for j in [1,2]:
                j=1: grid0[1]='B', grid2[1]='B' -> count=1.
                j=2: grid0[2]='C', grid2[2]='C' -> count=2>=2 -> valid.
            output (0,2) -> (0+1,2+1) = (1,3)

        This matches sample1.

        But sample2: k=1.
        b=1: a=0: use the smaller list v1 ([2]): 
               j=2: count=1 -> valid.
               then output (0,1) -> (1,2)

        sample3: k=3: no pair.

        sample4: 
            4 12 2
            GOOD.LUCK.IN
            WINNING.ICPC
            ASIA.PACIFIC
            CHAMPIONSHIP

        We are to output (2,3) meaning the second and third students.

        In our indexing:
            student0: "GOOD.LUCK.IN"
            student1: "WINNING.ICPC"
            student2: "ASIA.PACIFIC"
            student3: "CHAMPIONSHIP"

        We iterate b from 1 to 3.

        b=1 (student1: "WINNING.ICPC")
            a=0: 
                non-dot for0: all except positions 4 and 8 (which are '.') -> 10 non-dots.
                non-dot for1: all except positions: let's see: "WINNING.ICPC" -> positions with dot: the '.' at index7? (0-indexed: 
                     0:W, 1:I, 2:N, 3:N, 4:I, 5:N, 6:G, 7:., 8:I, 9:C, 10:P, 11:C -> so non-dots: 11 (only one dot at7)
                min=10, so iterate over the non-dots of a0 (10) might be faster to iterate over a1? because 10<11, so we iterate over a0's non-dots.
                But then we count the common matches. We need 2.
                We break when count>=2.
                We find at least two matches? 
                    For example, position0: 'G' vs 'W' -> no.
                    position1: 'O' vs 'I' -> no.
                    ... we find none? 
                So count=0.

        b=2 (student2: "ASIA.PACIFIC")
            a=1: 
                non-dot for1: 11
                non-dot for2: "ASIA.PACIFIC" -> dots? 
                    ASIA.PACIFIC: 
                        0:A, 1:S, 2:I, 3:A, 4:., 5:P, 6:A, 7:C, 8:I, 9:F, 10:I, 11:C -> non-dots: 11 (dot at4)
                min=11, so iterate over either, say we iterate over a1 (student1) non-dots: which is all indices except7.
                But at index7, student1 has '.' and student2 has 'C'? -> skip because we only iterate non-dots of a1, and index7 is not in the non-dot list of a1? 
                Actually, we are iterating over the non-dots of the smaller set, but if we choose the set of a1, then we consider positions where a1 is non-dot. For each such position j, we then check if a2 (student2) has a non-dot and the same character.
                For j=0: a1 has 'W', a2 has 'A' -> not match.
                j=1: 'I' vs 'S' -> no.
                j=2: 'N' vs 'I' -> no.
                j=3: 'N' vs 'A' -> no.
                j=4: 'I' -> but a2 has '.' at4? -> skip (because we only require a2 to be non-dot and equal? but if a2 has dot, then skip.
                j=5: 'N' vs 'P' -> no.
                j=6: 'G' vs 'A' -> no.
                j=8: 'I' vs 'I' -> count=1.
                j=9: 'C' vs 'F' -> no.
                j=10: 'P' vs 'I' -> no.
                j=11: 'C' vs 'C' -> count=2 -> valid.

                So we output (1,2) -> (1+1,2+1) = (2,3).

        This matches sample4.

  Therefore, we will use this pairwise method with the following optimizations:

        Let d[i] = the list of non-dot positions for student i.

        for b in range(1, n):   # 0-indexed b from 1 to n-1
            for a in range(b-1, -1, -1):  # a from b-1 down to 0
                if min(len(d[a]), len(d[b])) < k: 
                    continue
                count = 0
                if len(d[a]) <= len(d[b]):
                    for j in d[a]:
                        # if student b has a non-dot at j and the character equals student a at j
                        if grid[b][j] != '.' and grid[a][j] == grid[b][j]:
                            count += 1
                            if count>=k:
                                break
                else:
                    for j in d[b]:
                        if grid[a][j] != '.' and grid[a][j] == grid[b][j]:
                            count += 1
                            if count>=k:
                                break
                if count>=k:
                    print(a+1, b+1)
                    exit(0)

        print(-1)

  We hope that in practice, we often find the k matches quickly.

  Also, we hope that the lists d[i] are not worst-case.

  Worst-case overall is when there is no valid pair and for every student, d[i] = m, then the work is about n^2 * m / 2 = 5000*5000/2 * 3000 = 37.5e9.

  In C++, if we assume 1e9 operations per second, then 37.5 seconds. But we may be able to optimize the inner loop with compiler optimizations and cache. 

  Alternatively, we can try to use the condition: 
        if count + (remaining number of positions in the list) < k: break
  In the loop, we can precompute:

        if we are iterating over a list of size L, and we are at the i-th element, then the remaining is L - i - 1.

        So: if count + (L - i - 1) < k: then break out of the loop.

  This might help when the count is still low and the remaining positions are not enough to reach k.

  We can do:

        if len(d[a]) <= len(d[b]):
            L = len(d[a])
            for index in range(L):
                j = d[a][index]
                ... update count
                if count>=k: break
                if count + (L - index - 1) < k: 
                    break

        similarly for the other case.

  This will reduce the work per pair to at most the entire list if count is always 0, but if there are no matches, we still do the whole list.

  But if there are some matches but not enough, we might break early.

  In the worst-case (no matches), we do the whole list, so it doesn't help.

  But if there are a few matches, then we might break early.

  Given that k is at most 5, if we have found 0 matches and we have 5 positions left, then we can break. So if we are at the L-5-th position and count==0, then we break.

  This reduces the work per pair to at most min(d_a, d_b) but if there are no matches, we do the entire list.

  However, if there are matches, we break as soon as we have k, and if there are not enough matches, we break when the remaining positions are not enough to reach k.

  This can help in the case where there are not enough matches.

  So we will add this.

  Code for the inner loop:

        if len(d[a]) <= len(d[b]):
            L = len(d[a])
            for index in range(L):
                j = d[a][index]
                if grid[b][j] != '.' and grid[a][j] == grid[b][j]:
                    count += 1
                    if count>=k:
                        break
                # Check if it is possible to reach k
                if count + (L - 1 - index) < k:
                    break

        similarly for the other case.

  This is valid because in the worst-case, the remaining positions might not yield any match, but we are assuming they might yield one per position. But we break when even if the rest are matches, we cannot reach k.

  This is correct.

  This reduces the work per pair to at most min(d_a, d_b) and at least the number of positions until the point where the remaining are not enough.

  In the worst-case (no match), we break when we have processed L - k + 1 + 1? Actually, we break when at a position i, we have count=0 and the remaining positions are < k - count = k. So we break when i is such that L - i - 1 < k, i.e., i >= L - k.

  So we only do L - k + 1 iterations. In the worst-case (no match) and L=3000, k=5, we do 3000 - 5 + 1 = 2996 iterations, which saves only 5 iterations.

  Not helpful.

  But if we have found 0 matches and we are at the beginning, we still have to do until L-k? 

  Alternatively, we can break as soon as count + (L - index - 1) < k.

  And initially, if L < k, we skip the pair.

  So in the worst-case (no match), we do the entire list.

  Therefore, this does not help the worst-case.

  But it doesn't hurt.

  We will include it.

  Final solution in pseudocode:

      n, m, k = input()
      grid = [ input() for _ in range(n) ]

      # Precompute d: for each student i, the list of positions j where grid[i][j]!='.'
      d = []
      for i in range(n):
          list_i = []
          for j in range(m):
              if grid[i][j] != '.':
                  list_i.append(j)
          d.append(list_i)

      # Iterate over b from 1 to n-1 (0-indexed), and for a from b-1 down to 0
      for b in range(1, n):
          for a in range(b-1, -1, -1):
              da = len(d[a])
              db = len(d[b])
              if da < k and db < k:  # actually, min(da,db) < k then skip, but if one>=k, we still have to check
                  continue

              count = 0
              # We'll iterate on the smaller of d[a] and d[b]
              if da <= db:
                  L = da
                  for idx in range(L):
                      j = d[a][idx]
                      # If b has a non-dot at j and the character equals a's character at j
                      if grid[b][j] != '.' and grid[a][j] == grid[b][j]:
                          count += 1
                          if count >= k:
                              break
                      # Check if we can break because the remaining are not enough
                      if count + (L - 1 - idx) < k:
                          break
              else:
                  L = db
                  for idx in range(L):
                      j = d[b][idx]
                      if grid[a][j] != '.' and grid[a][j] == grid[b][j]:
                          count += 1
                          if count >= k:
                              break
                      if count + (L - 1 - idx) < k:
                          break

              if count >= k:
                  print(a+1, b+1)
                  exit(0)

      print(-1)

  This should work.

  Let me test with a small worst-case: n=3, m=3, k=3, and no valid pair.
        Students: 
            S0: "ABC"
            S1: "DEF"
            S2: "GHI"

        d[0] = [0,1,2], d[1]=[0,1,2], d[2]=[0,1,2]

        b=1: a=0 -> da=3, db=3: iterate over d[0] (smallest not, we do d[0]).
            j=0: 'A' and 'D' -> not match -> count=0, then 0 + (3-1-0)=3-1=2 <3? -> 0+2<3 -> true, so break inner loop.
        b=2: a=1: similarly break after first j: count=0, then 0+ (3-1)=2<3 -> break.
            a=0: similarly break after first j.

        So we do 1 (for a=0 in b=1) + 2 (for b=2: a=1 and a=0) = 3 iterations of the inner loop.

  This is a improvement.

  Without this optimization, we would have done 3+3+3 = 9 iterations.

  In the worst-case (no match and every student has exactly m non-dots), the work per pair is not m, but only until we can break by the condition, which is at the (m - k + 1)th element? 

  Actually, we break when we are at an index i and we have done i+1 iterations and we have count + (L - i - 1) < k.

  In the worst-case (no matches), count=0, and we break when L - i - 1 < k, i.e., i >= L - k.

  So the number of iterations per pair is min(da, db) - k + 1.

  In the worst-case, if min(da,db)=3000, then we do 3000 - k + 1 ~ 3000 iterations, which is the same as before.

  But wait: if there are no matches, then we break when we reach the (L - k + 1)-th element? 

        We break at the first index i for which: i+1 iterations have been done and the condition is met.

        The condition: count + (L - i - 1) < k.

        Since count=0, we break when L - i - 1 < k, i.e., i >= L - k.

        How many iterations: from i=0 to i = L-k, we do L-k+1 iterations.

        For L=3000, k=5, we do 3000-5+1 = 2996 iterations.

  So we save k-1 iterations per pair.

  But 2996 is not much better than 3000.

  But if k is 5, then we save 4 iterations per pair, which is negligible.

  Therefore, the worst-case 37.5e9 iterations remains.

  We must hope that the judge's machine is fast in C++.

  Or that the average-case is good.

  Or that there is a valid pair early.

  Given the problem constraints and the fact that k is small, and that the sample input #4 has only 4 students and we find in the third student, we hope that in many cases we find the pair early.

  In the worst-case (no valid pair), we might need to optimize further.

  One more idea: 
        If the entire set of students has very few non-dot positions in common, we might skip pairs faster. 

  But without a global index, it is hard.

  Given the time, we output the solution and hope that the worst-case does not happen.

  Alternatively, there is a known efficient solution using bitset for the case of small k? 

  I recall that we can use a randomized approach: 
        For each position j, and for each character c, assign a random number (for example, a 64-bit integer) to the event (j,c). 
        For a student i, compute a hash F(i) = the sum of the random numbers for each (j, grid[i][j]) for each non-dot position j.
        Then for two students a and b, the common positions with the same character will be in both, so the sum will be F(a) + F(b) - [the random numbers for the events where they differ or not both present]? 

  This doesn't work.

  Or we can compute a signature for the student as the set of (j, grid[i][j]), and then the common matches are the intersection. But we only care if the intersection has size>=k.

  We can use a bloom filter? not exact.

  Given the complexity, we output the pairwise method.

  Note: the intended solution might be using the fact that k is small and iterating by the common positions by the following:

      We can for each student, and for each subset of up to k positions that the student has non-dot, we not and then use hashing to find a collision? 

  But the number of subsets per student is C(d_i, k) which can be large.

  We can try to find a witness for a set of k positions: 
        For each student i, we generate all subsets of size k from the non-dot positions of i, and for each subset, we generate a hash of the tuple ( (j1, c1), (j2, c2), ... (jk, ck) ) and also we can generate a hash of the set of j's with the characters.

  Then if two students have the same hash for at least one subset of size k, then they have at least k common matches.

  But the problem: we want to find the pair (a,b) with smallest b and then largest a.

  We can iterate by b, and for each b, and for each subset of size k from the non-dot positions of b, we generate the hash for the event (the set of positions and the characters of b at these positions) and then look for a previous student a (with a < b) that has the same set of positions and the same characters.

  But note: the common matches might be a superset of this set? Actually, if they have this set in common, then they have at least k common matches.

  However, the converse is not true: they might have a different set of k positions with common matches.

  So we have to generate all possible subsets of k common matching positions? 

  The number of subsets per student is C(d_i, k). In the worst-case, d_i = 3000, then C(3000,5) is about 3000^5 / 120 ~ 2.25e16, which is too high.

  Therefore, we cannot do that.

  So we settle for the pairwise method with the min(d_a, d_b) and break when found>=k and also break when the remaining cannot make it.

  We hope that in practice the number of non-dots is not huge or that there is a valid pair early.

  Or we hope that the worst-case of 37.5e9 is acceptable in C++ on the judge.

  Given that the problem has a time limit of 3 seconds, and that 37.5e9 might be acceptable in C++ if we optimise the inner loop and the memory access, and use Ofast, we try it.

  We note: 
      - The grid is stored in a vector of strings, contiguous memory for each string, but the strings are together.
      - The lists d[i] are stored as vectors of int.

  In C++, we can do:

        vector<string> grid;
        vector<vector<int>> d;

        // Input and compute d

        for (int b = 1; b < n; b++) {
            for (int a = b-1; a >=0; a--) {
                int da = d[a].size();
                int db = d[b].size();
                if (da < k && db <k) continue; // both less than k, skip
                int count = 0;
                if (da <= db) {
                    int L = da;
                    for (int idx=0; idx<L; idx++) {
                        int j = d[a][idx];
                        if (grid[b][j] != '.' && grid[a][j] == grid[b][j]) {
                            count++;
                            if (count>=k) break;
                        }
                        if (count + (L - 1 - idx) < k) break; // the condition
                    }
                } else {
                    // similar
                }
                if (count>=k) {
                    cout << a+1 << " " << b+1 << endl;
                    return 0;
                }
            }
        }
        cout << -1 << endl;

  We hope this works.

  Given the sample tests, it works.

  For the worst-case, we hope that the judge's machine is fast or that the worst-case does not occur.

  Or if the worst-case occurs, we hope that the condition (count + (L - 1 - idx) < k) helps for the future pairs? In the worst-case (no valid pair) it doesn't reduce the work enough.

  But if the average-case is good, we try.

  This is the solution.

  Let's code in C++ for the judge.

  However, the problem does not require code in the editorial, so we stop here.

  Summary for the reader:

      We precompute for each student i a list of the positions where they have non-dot answers.

      We then iterate for b from 1 to n-1 (0-indexed) (which represents the larger student index in the pair), and for each b, we iterate a from b-1 down to 0.

      For each pair (a,b), we iterate over the list of non-dot positions of the student with the shorter list (between a and b). For each position in that list, we check if the other student also has a non-dot at that position and if the characters match.

      We count the matches, and if we reach k, we output (a+1, b+1) and exit.

      We also include an early break if the number of remaining positions in the list (plus the current count) is less than k.

      If we finish the loops without finding a pair, we output -1.

  This solution has a worst-case time complexity of O(n^2 * m), which is 37.5e9 for n=5000, m=3000, which may be borderline in C++ but is the best we have.

  Given the small k (<=5) and the hope that in practice the lists are short or we find the pair early, we use this.

  It matches the sample cases.