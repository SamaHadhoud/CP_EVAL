We are given:
  n, q
  an array p of n integers: p_1, p_2, ... p_n (with p_i being the parent of i; 0 meaning no parent)

We need to build a rooted tree.

The array L is built as:
  for x from 1 to n:
      for y from 1 to n:
          value = (x-1)*n² + (LCA(x,y)-1)*n + (y-1)
      then we sort the n values for this row (x) and then we merge all rows (so overall sorted array L of size n²)

But note: the problem says: 
      for x = 1 to n
          for y = 1 to n
              append ... to L
          sort L in non-decreasing order   -> but wait, the pseudocode sorts after each row? 
  Actually, the pseudocode does:
      let L be an empty array
      for x from 1 to n:
          for y from 1 to n:
              append ... to L
          sort L in non-decreasing order

But note: that would sort the entire array L (which has x*n elements so far) after each row. However, the problem says "append" and then sort the entire L? 

But reading the problem again: 
      "append ((x - 1) * n * n + (LCA(x, y) - 1) * n + (y - 1)) to L"
      then "sort L in non-decreasing order"

However, the sample output is a sorted array of n² elements. So the entire array L of size n² is sorted. The inner sorting after each row is misleading? 

But note: the sample input has n=5, and the array L has 25 elements. So the entire array L is built by iterating over all pairs (x,y) and then the entire array is sorted.

The problem says: 
      "let L be an empty array
       for x = 1 to n
           for y = 1 to n
               append ... to L
       sort L in non-decreasing order"

So we only sort once at the end? 

But the pseudocode says "sort L in non-decreasing order" inside the x-loop? That is, after each x (so after each row) we sort the entire array? 

Actually, the pseudocode is:

      let L be an empty array
      for x = 1 to n
          for y = 1 to n
              append ((x - 1) * n * n + (LCA(x, y) - 1) * n + (y - 1)) to L
          sort L in non-decreasing order

This means that after the first row (x=1), we have n elements and we sort them. Then we add the next n elements (for x=2) and then sort the entire array (which now has 2n elements). 

But note: the entire array L is of size n². The problem then has q queries: for each query j, we are given k_j and we must find the k_j-th element of the array L (1-indexed).

However, the problem does not require us to build the entire array L (which has 10^10 elements for n=100,000). We must answer each query without building the entire array.

Alternative interpretation: the entire array L is built by the two nested loops and then sorted. But the pseudocode sorts after each row? That would be inefficient and not necessary. 

But note: the problem says "Your friend would like to run the following pseudocode". We are not actually running that pseudocode. We are to answer the q queries about the final sorted array.

The key is: the entire array L is the multiset of values for all pairs (x,y). And we sort the entire array. The pseudocode does:

      L = []
      for x in [1, n]:
          for y in [1, n]:
              L.append( value )
          sort(L)   # sorts the entire L so far (which is the first x rows)

But note: the entire array at the end is the same as if we built the entire array and then sorted. Why? Because the last sort (at x=n) sorts the entire array.

So the array L after the last row is sorted is the same as if we built the entire array and sorted.

Therefore, we are to consider the entire sorted array of n² elements.

We have to answer: for a given k_j, what is the k_j-th element in that sorted array?

How to approach?

The value for a pair (x,y) is: 
   V(x,y) = (x-1)*n² + (LCA(x,y)-1)*n + (y-1)

Note that the term (x-1)*n² is the dominant term. Therefore, the entire array is grouped by x. That is, all pairs with x = 1 come first, then x=2, etc. And within a fixed x, the values are in the range: 
   [(x-1)*n², (x-1)*n² + (n-1)*n + (n-1)] = [(x-1)*n², (x-1)*n² + n² - 1]

But note: the entire array is sorted. So the smallest n² values are for x=1, then the next n² for x=2? Actually, no: because the value for (x,y) is not necessarily increasing with x? Actually, the term (x-1)*n² is so large that indeed:

   For two pairs (x1,y1) and (x2,y2):
        if x1 < x2, then V(x1,y1) < V(x2,y2) because (x1-1)*n² <= (x2-2)*n² + ... and (x2-1)*n² - (x1-1)*n² >= n², which is larger than the maximum inner value (n²-1).

Therefore, the entire sorted array L has:

   First: all pairs with x=1 (in increasing order of their inner value: (LCA(1,y)-1)*n + (y-1))
   Then: all pairs with x=2 (in increasing order of inner value)
   ... 
   Last: all pairs with x=n.

Therefore, we can:

   Step 1: Determine the row x0 that contains the k_j-th element.

        Since each row has exactly n elements, the row index x0 is: 
            x0 = ceil(k_j / n)   -> actually: the k_j-th element is at row = (k_j - 1) // n + 1, and within that row at position = (k_j - 1) % n + 1.

        But note: the entire array has n rows, each of n elements. So the element at global index k_j is the r-th element (0-indexed r = (k_j-1) % n) in the sorted row x0 = (k_j-1)//n + 1.

   Step 2: For the row x0, we need to know the r-th smallest value of the inner expression: 
            inner(x0, y) = (LCA(x0,y)-1)*n + (y-1)   for y in [1, n]

        Then the entire value is: (x0-1)*n² + inner_value.

   So the problem reduces to: for a fixed x0, we want the r-th smallest inner value over y in [1, n].

How to compute the sorted list of inner values for a fixed x0 without iterating over all y?

Observation for fixed x0:

   The inner value is: (z-1)*n + (y-1), where z = LCA(x0,y).

   Note that z must be an ancestor of x0. Why? Because the LCA of x0 and y must be an ancestor of x0.

   Therefore, we can break the set of y by which node z (on the path from root to x0) is the LCA.

   Specifically, for a node z on the path from root to x0, the set of y for which LCA(x0,y)=z is:

        S(z) = { y in the subtree of z } \ { y in the subtree of the child of z that is on the path to x0 }

   (If z is x0 itself, then we exclude nothing? Actually, if z = x0, then the child to exclude would be none because then we are at the leaf of the path? Actually, we break the subtree of z by removing the next node in the path? Or if z=x0, then we don't remove any part? But note: the LCA is x0 when y is in the subtree of x0? Actually, if y is in the subtree of x0, then the LCA is x0? 

   However, if y is in the subtree of a strict descendant of z that is not on the path to x0, then the LCA is still z? Actually, no: if y is in the subtree of a child of z that is not the one leading to x0, then the LCA is z. If y is in the subtree of the child that is on the path to x0, then the LCA is the LCA of x0 and y which might be a node below z? 

   Actually, the set of y for which the LCA(x0,y)=z is:

        The entire subtree of z, excluding the subtree of the next node in the path from z to x0 (if such a child exists). 

   So:

        For the root, we have the entire tree excluding the subtree of the child that leads to x0.
        For an intermediate node z (not x0), we have the subtree of z excluding the subtree of the next child on the path to x0.
        For z = x0, we have the entire subtree of x0.

   Now, note: the inner value for a given y is (z-1)*n + (y-1). Therefore, for a fixed z, the inner values for y in S(z) are:

        base = (z-1)*n
        and then we have the values: base + (y-1) for y in S(z).

   And note: since y is an integer from 1 to n, the expression (y-1) is just the node id minus one.

   Therefore, for a fixed z, the set of inner values is contiguous? No, because S(z) is a set of nodes (which are not contiguous in node id). However, we are going to sort the entire inner values for row x0. 

   We can think: the inner value for a pair (x0,y) is determined by (z, y). And since z is fixed for a set, and the base is (z-1)*n, and then we add (y-1). So the entire inner value is at least (z-1)*n and at most (z-1)*n + (n-1). 

   But note: the values for different z do not overlap? Because the base for z1 and z2 (z1 < z2) would be (z1-1)*n and (z2-1)*n, and if z1 < z2 then (z1-1)*n < (z2-1)*n. So the sets for different z are disjoint.

   Therefore, the entire row x0 is partitioned into disjoint intervals by the z (the LCA). And the values in the set for z lie in the interval [ (z-1)*n, (z-1)*n + (n-1) ].

   And since the bases are increasing with z (which is the node id? but note: the node ids are arbitrary) - wait, the base is (z-1)*n, so it depends on the node id z. However, the nodes on the path from root to x0 are in increasing depth order, but their node ids are arbitrary. 

   But note: the inner value for a pair (x0,y) is (LCA(x0,y)-1)*n + (y-1). The LCA(x0,y) is one of the nodes on the path. And the base (z-1)*n is determined by the node id of z. Therefore, the sets for different z are disjoint only if the node ids are distinct? They are. 

   However, the bases for different z might not be in increasing order of depth? They are in increasing order of the node id? But the problem does not say that node ids are in any particular order. 

   Actually, the base is (z-1)*n, so if z1 < z2 (as integers) then base1 < base2. But the nodes on the path from root to x0 have arbitrary node ids. Therefore, the bases for the different z on the path are not necessarily in increasing order of depth? 

   But note: the value we are sorting by is the inner value, which is a number. And we want the r-th smallest.

   Therefore, for the entire row x0, we have:

        The inner value for a pair (x0,y) is: base_z + (y-1), where z = LCA(x0,y).

   How to collect the entire set? 

        We can iterate over the nodes z on the path from root to x0. For each z, we consider the set S(z) and then the inner values are base_z + (y-1) for each y in S(z). 

   Then the entire set for the row is the union of these sets.

   Since the sets are disjoint (each y belongs to exactly one z) and the inner value for different z are in disjoint intervals (because base_z are multiples of n), we can:

        For each z (in arbitrary order) we know the set of y in S(z). Then the inner value for a particular y is base_z + (y-1). 

   However, to find the r-th smallest inner value, we can do:

        We know the entire set of inner values is the union of several arithmetic progressions? Actually, no: the set of y in S(z) is arbitrary.

   Alternate approach:

        We note that the inner value we are looking for must be in the range [0, n²-1]. We can binary search on the inner value V (from 0 to n²-1) and count the number of pairs (x0,y) such that inner(x0,y) <= V.

        Then we find the smallest V such that the count is at least r.

   How to count for a fixed x0 and V?

        We break V as: 
            base = (z0 - 1) * n   for some z0? 
        Actually, let z0 = floor(V/n) + 1. Then:

            For a node z on the path from root to x0:
                base_z = (z-1)*n.

                If base_z > V: then no y in S(z) can satisfy base_z + (y-1) <= V.

                If base_z <= V: then we require y-1 <= V - base_z, i.e., y <= V - base_z + 1.

                So we need to count the number of y in S(z) such that y <= Y0, where Y0 = V - base_z + 1.

        Therefore, for each z in the path (and base_z <= V), we can:

            count_z = number of y in S(z) such that y <= Y0.

        Then total_count = sum_{z in path, base_z<=V} count_z.

        Then we can binary search V such that total_count >= r.

   How to compute count_z?

        We need to count the nodes in S(z) (which is the subtree of z excluding the subtree of the next child on the path to x0) that are <= Y0.

        We can preprocess the entire tree with a DFS to get in-time and out-time. Then the set S(z) is:

            entire subtree of z: [in_time[z], out_time[z]]
            minus: the subtree of the next child (if exists): [in_time[child], out_time[child]]

        But note: the next child is the one that is on the path to x0. Let that child be c. Then the set S(z) is:

            [in_time[z], in_time[c]-1] U [out_time[c]+1, out_time[z]]

        And we want to count the nodes (by their node id, not DFS time) that have node id <= Y0 and whose DFS time is in that interval? 

        Actually, we want to count nodes y (by the actual node id) that are <= Y0 and that lie in the DFS intervals corresponding to S(z).

        We can precompute a structure for the entire tree that supports:

            Given an interval [L, R] in the DFS array, count the number of nodes in that interval that have node id <= X.

        This is a 2D range counting (by DFS index and node id) but note: the DFS array is an array of node ids. And we want:

            Count { i in [L, R] such that A[i] <= X }

        This is a standard problem that can be solved with a Fenwick tree if we do offline queries, but we need online and we have many queries (q up to 100000) and for each query we do O(depth) steps? (depth up to 100000) so worst-case 100000*100000 is 10^10.

        Alternatively, we can use a Wavelet tree (or a Segment Tree over the DFS array with the array A = the DFS array) for range queries by value.

        We can build a Wavelet tree on the DFS array (which is the array of node ids in DFS order). Then we can quickly count the number of nodes in a contiguous segment of the DFS array that have value (node id) <= X.

   Steps for the entire solution for general trees:

        Preprocessing:

          1. Build the tree from the parent array.
          2. Do a DFS (in-order) to assign in_time and out_time, and build an array A of length n: A[in_time[i]] = i.
          3. Build a Wavelet tree (or a Fenwick tree with coordinate compression? but we need many queries) over the array A.

          4. Precompute the path from the root to each node? Actually, we can compute the path for each x0 on the fly and cache it.

        For each query:

          k_j -> 
            x0 = (k_j-1) // n + 1
            r = (k_j-1) % n + 1   (r from 1 to n)

          Get the path from root to x0 (if we haven't computed it for this x0, compute and cache).

          Then we binary search over V in [0, n²-1] (the inner value) to find the smallest V such that:

                count = 0
                for each z in the path (from root to x0):

                    base = (z-1)*n
                    if base > V: skip.

                    Y0 = V - base + 1   (because we require y <= Y0)

                    If Y0 < 1: then skip.

                    Now, we have the set S(z) = the subtree of z excluding the next child on the path (if exists).

                    How to represent S(z) in DFS intervals?

                        If z is the last node in the path (i.e., z = x0), then there is no next child to exclude? Actually, for x0, we don't have a next child? So we take the entire subtree.

                        Otherwise, let c = the next child in the path (so the child of z that is the next node in the path).

                        Then the set S(z) is:

                            [in_time[z], in_time[c]-1] U [out_time[c]+1, out_time[z]]

                    Then we count:

                        count1 = count of nodes in [in_time[z], in_time[c]-1] that have node id <= min(Y0, n)   [if the interval is non-empty]
                        count2 = count of nodes in [out_time[c]+1, out_time[z]] that have node id <= min(Y0, n)   [if the interval is non-empty]

                    Then add count1+count2.

                Then total_count = count from all z.

          Then we want the smallest V such that total_count >= r.

          Then the answer for the query is: (x0-1)*n² + V.

        However, the binary search over V (from 0 to n²-1) would take O(log(n²)) = O(log n) iterations? Actually, n² can be 10^10, so log2(10^10) is about 34. But in each iteration we iterate over the entire path (which can be O(n)) and do two wavelet tree queries per node? So worst-case O(n * log n) per binary search step? Then worst-case O(n * log n * log n) per query? And we have up to 100000 queries -> worst-case 100000 * 100000 * (log n)^2 which is 10^10 * (log n)^2 -> too slow.

        We need to optimize.

        Alternate for the entire row: 

            The inner values are in the range [0, n²-1]. We are doing a binary search on V for the entire row. But note: the entire row has n elements. We are counting the number of y that yield inner value <= V. 

            We can try to avoid iterating over every node in the path? 

        How about: for the entire row, the inner value is a function of y. We want the r-th smallest. We can do a binary search over y? But note: the inner value is not monotonic in y because the base depends on the LCA, which changes with y.

        Alternatively, we can try to break the row by the base? The bases are multiples of n: base_z = (z-1)*n. And the bases for the path nodes are known. Then the inner values for a fixed z are in the interval [base_z, base_z + n-1]. 

        Therefore, we can iterate over the bases in increasing order? But the bases are (z-1)*n for z in the path, and we don't know the order of the node ids? Actually, we can sort the path by the base? But the base is (z-1)*n, so we can sort by z? 

        Actually, we can sort the nodes in the path by the base? The base is determined by the node id z. So if we sort the path by the node id z? 

        However, note: the inner value we are looking for might be in one of the segments. We can:

            For each base value (which is (z-1)*n) in the path, we can compute the maximum inner value that we can get from that segment: base_z + (n-1). 

        Then the entire row is partitioned into intervals by base. The smallest base segment comes first, then the next, etc. 

        Then we can:

            Step 1: sort the nodes in the path by base (which is by z: because base = (z-1)*n, so by z).

            Step 2: for each segment in increasing order of base:

                   Let z = current node, base = (z-1)*n.

                   The segment covers inner values from base to base + (n-1).

                   We want to count the number of y in S(z) that yield inner value <= V? Actually, we are counting the entire set in the segment? But note: the condition for a fixed z is: we count the y in S(z) with y <= V - base + 1.

                   Then the count for this segment for a given V is: 
                         count_seg = min( |S(z)|, count of nodes in S(z) with node id <= (V - base + 1) )

                   Then we accumulate the counts.

            Then we can binary search V as before? 

        But note: the segments are contiguous in the inner value? Yes, because the bases are multiples of n. So the entire inner values for the row are partitioned into contiguous intervals by base. Therefore, the entire set of inner values is the union of contiguous intervals? Actually, no: the sets for different z are disjoint and the inner values for different z are in disjoint intervals [base_z, base_z+n-1]. Therefore, the entire set is the union of these intervals, and the intervals are non-overlapping and increasing by base (if we sort by base).

        Therefore, we can:

            Sort the path by z (so by base).

            Then we can determine which segment the r-th element falls into: 

                total_so_far = 0
                for each z in sorted_path (by base, increasing):

                    count_z = |S(z)|   (which is the entire set of y in S(z)) -> but note: if we are looking for the r-th element, we don't know V yet.

            Actually, we can break the row into segments by base. Then the inner values in the segment for z are contiguous? No, because the set S(z) might not be contiguous in node id. The inner value for a y in S(z) is base_z + (y-1). Therefore, the inner values in the segment are not contiguous if the set S(z) is not contiguous in y? But note: the inner value is base_z + (y-1), and the set of y in S(z) is arbitrary. So the inner values are not contiguous.

        Therefore, we stick to the binary search on V.

        But we must optimize the counting per V. 

        We note: the path length is at most the depth of x0. In the worst-case (chain) the depth is n. And we have up to 100000 queries. And the binary search over V (which is 0 to n²-1) would be O(log(n²)) = O(log n) steps. Then total operations per query: O(depth * log n) for the counting in the wavelet tree? And the wavelet tree query is O(log n). Then per query: O(depth * log² n) worst-case? 

        Worst-case depth is 100000, and log² n is about (17)^2 = 289. Then 100000 * 289 = 28.9e6 per query. And 100000 queries? 2.89e12 -> too high.

        We need a different approach.

        Alternate for the entire row: 

            We want to find the r-th smallest inner value for fixed x0.

            The inner value is: (z-1)*n + (y-1) = base_z + (y-1).

            We can iterate over the segments by base in increasing order? (so sort the path by z, the node id). 

            Then:

                Let sorted_path = sorted(path, key=lambda z: (z-1)*n)   # which is the same as sorted by z.

                Then for each z in sorted_path:

                    The entire segment for z has base = (z-1)*n.

                    The number of y in S(z) is: |S(z)| = (size of the entire subtree of z) - (size of the subtree of the next child, if exists).

                    Then the inner values in this segment start at base and go to base + (n-1). But note: the inner values are base + (y-1) for y in S(z). So the smallest inner value in the segment is base + (min_y_in_S(z) - 1) and the largest is base + (max_y_in_S(z)-1). But we don't have min and max.

            Actually, we can do:

                We are going to iterate over the segments in increasing base. For the current segment, we can list the set of inner values: { base + (y-1) for y in S(z) }.

                Then we want to know: what is the k-th smallest inner value in the entire row? 

                Since the segments are disjoint and the base is increasing, the inner values in the current segment are all greater than the previous segments? Not necessarily: because the base for a larger node id z might be larger than a base for a smaller node id? But base = (z-1)*n, and if we sort by base (which is by z), then the segments are in increasing order of base. And the next segment has base at least as large as the current. Therefore, the entire set of inner values for the row is the union of the segments in increasing base order.

            Therefore, the entire row is sorted by base and then by y? 

            Then we can:

                total = 0
                for z in sorted_path (by base = (z-1)*n):

                    count_z = |S(z)|   (the number of y in S(z))

                    If total + count_z < r:
                         total += count_z
                    else:
                         The r-th element is in this segment. And it is the (r - total)-th smallest element in the segment.

                         The inner value for the segment is: base + (y-1) for y in S(z). So the smallest inner value in the segment is base + (min_y - 1) and the next is base + (min_y) ... but we don't have sorted y.

                    How to get the (r-total)-th smallest element in the segment?

                         The inner value = base + (y-1). Therefore, we want the (r-total)-th smallest y in the set S(z). Then the inner value = base + (y0 - 1).

                Then we break.

            Therefore, we avoid binary search over V. 

            How to get the (r-total)-th smallest y in the set S(z)? 

                We can use the wavelet tree: 

                    The set S(z) is represented by two DFS intervals: [in_time[z], in_time[c]-1] and [out_time[c]+1, out_time[z]].

                    We want the k-th smallest node id (by value) in the union of these two intervals.

                The wavelet tree can support k-th smallest in a range? Yes, typically wavelet trees support k-th smallest in a contiguous interval. But we have two disjoint intervals.

            We can do:

                We can combine the two intervals and then ask for the k-th smallest in the combined set? 

                How to combine? We can do two separate queries: 

                    We want to know: 

                         Let k0 = k.

                         In the first interval [L1, R1] = [in_time[z], in_time[c]-1], count = count1 = (R1 - L1 + 1) but we don't know the actual distinct node ids? Actually, we want the k0-th smallest in the union.

                    We can do:

                         count1 = wavelet.range_query_count(L1, R1, min_val, max_val)   -> but we don't have min_val and max_val? Actually, we want to count the entire interval? The wavelet tree can tell the total count in the interval? But we have the entire interval: the count is (R1 - L1 + 1) if the interval is valid? But wait: the wavelet tree built on the DFS array A (which is the node ids) can tell the number of elements in the interval? Actually, the wavelet tree we built is for the array A. The interval [L1, R1] in the DFS array has (R1 - L1 + 1) elements. But we need the k-th smallest by node id.

                The wavelet tree we built supports:

                    count = wavelet.range_query_leq(L, R, X)   -> counts the number of elements in A[L..R] that are <= X.

                How to get the k-th smallest? 

                    We can do a binary search on the value of y? 

                But note: we want the k-th smallest in the union of two intervals. We can combine the two intervals by doing:

                    total_count = count_in_interval1 + count_in_interval2.

                    Then we do a binary search over the node id Y in [1, n] for the smallest Y such that the total number of nodes in the two intervals that are <= Y is at least k0.

                Then Y is the k0-th smallest node id. Then the inner value = base + (Y-1).

            Then the entire value for the query is: (x0-1)*n² + base + (Y-1).

            Steps for a segment:

                k0 = r - total

                We want to find the smallest Y (node id) such that:

                    count1 = wavelet.range_query_leq(L1, R1, Y)   for the first interval
                    count2 = wavelet.range_query_leq(L2, R2, Y)   for the second interval
                    total_count = count1 + count2 >= k0

                Then Y0 = that smallest Y.

                Then inner_value = base + (Y0 - 1)

            Then the answer = (x0-1)*n² + inner_value.

        Then the entire query for a fixed x0 and r is:

            sorted_path = sorted(path, key=lambda z: (z-1)*n)   # sort by base, which is (z-1)*n -> and since base = (z-1)*n, this is the same as sorted by z (the node id).

            total = 0
            for z in sorted_path:
                # Compute S(z) as two intervals: 
                #   Let next_child = the child of z that is in the path? (if z is not x0)
                #   If z is not the last in the path (i.e., not x0), then we have a next child c.
                #   Otherwise, no next child.

                # How to get the next child? We have the entire path. We can precompute the next child for each z in the path? 

                # Actually, we have the path: [root, ..., x0]. So for a given z, we can look at the next node in the path: if there is a next node and that node is a child of z, then that is c.

                # Let c = None
                if z != x0:
                    # find the index of z in the path: then the next node is path[index+1]
                    # Actually, we stored the path as a list. We can precompute the next child for each z? 
                    # But note: we are iterating in sorted_path (by base) not in the original path order. We need the next child for the exclusion.

                    # How to get the next child? We must know the original path order to know which child to exclude.

                    # We stored the path in the order from root to x0. So we can also store for each z in the path, the next child (if any) that is in the path.

                    # But note: we are iterating by base, not by depth. We need the next child in the path for the exclusion. 

                    # We can precompute a dictionary: for each node in the path, we store the next child (the next node in the path) if it exists.

                    # Actually, we can compute the next child for z during the path construction: 
                    #   We have the path: [a0, a1, a2, ..., a_{m-1}] where a0=root, a_{m-1}=x0.
                    #   For an index i (0<=i<m-1), the next child of a_i is a_{i+1}. For a_{m-1} (x0), there is no next child.

                # But note: we did not store the next child in the path for each z? We only stored the list of nodes.

                # How about: we precompute for the entire path (when we compute the path for x0) a list of the next child for each node? Actually, we can do:

                #   When we build the path, we also build an array next_child such that for each node z in the path (except the last), next_child[z] = the next node in the path.

                #   But note: the same node z might appear in multiple paths? So we cannot store in a global array. We should cache per path? 

                #   Actually, we cache the entire path for x0. And we can also cache a dictionary for the next child for each node in the path? 

                #   Alternatively, we can store the path as a list and also store a list: next_child_list for the path: for each node in the path (by the order) the next child is the next node? 

                #   But we are iterating in sorted order by base. 

                #   We can store for each node z in the path (when we build the path) the next child (if exists). We can store a dictionary for the path: next_child_dict = {}
                #   for i in range(len(path)-1):
                #       next_child_dict[path[i]] = path[i+1]

                #   Then for z, we can get c = next_child_dict.get(z, None)

                #   We do this when we build the path.

            Implementation:

                When we build the path for x0, we store:

                    path_list = [root, ..., x0]
                    next_child_dict = {}
                    for i in range(len(path_list)-1):
                        next_child_dict[path_list[i]] = path_list[i+1]

                Then we store (path_list, next_child_dict) for x0.

            Then:

                for a node z in the path (in the sorted_path by base):

                    if z == x0:
                        c = None
                    else:
                        c = next_child_dict[z]

                Then the set S(z) is:

                    if c is None:
                        interval1 = (in_time[z], out_time[z])
                        interval2 = None   (so only one interval: [in_time[z], out_time[z]])
                    else:
                        interval1 = (in_time[z], in_time[c]-1)
                        interval2 = (out_time[c]+1, out_time[z])

                Then the size of S(z) is:

                    if c is None:
                        size_z = out_time[z] - in_time[z] + 1
                    else:
                        size_z = (in_time[c]-1 - in_time[z] + 1) + (out_time[z] - (out_time[c]+1) + 1)
                                 = (in_time[c] - in_time[z]) + (out_time[z] - out_time[c])

                Then if total + size_z < r:
                    total += size_z
                    continue

                else:
                    k0 = r - total   # the position in this segment

                    Now, we want the k0-th smallest node id in the set S(z). 

                    We have two intervals: 
                        I1 = [in_time[z], in_time[c]-1]   (if c exists, then this interval exists only if in_time[z] <= in_time[c]-1)
                        I2 = [out_time[c]+1, out_time[z]]   (if c exists, and exists only if out_time[c]+1 <= out_time[z])

                    If there is no c, then we have one interval: [in_time[z], out_time[z]]

                    How to get the k0-th smallest node id in the union of I1 and I2? 

                    We can do a binary search over the node id Y in the range [1, n] for the smallest Y such that:

                         count1 = wavelet.range_query_leq(I1[0], I1[1], Y)   if I1 is non-empty, else 0
                         count2 = wavelet.range_query_leq(I2[0], I2[1], Y)   if I2 is non-empty, else 0
                         total_count = count1+count2 >= k0

                    Then Y0 = that Y.

                    Then inner_value = base + (Y0-1)   [because base = (z-1)*n]

                    Then the entire value = (x0-1)*n² + inner_value

                    break out of the loop for the row.

        Then we output the entire value.

        Complexity per query: 

            The path length is the depth of x0. In the worst-case (chain) depth = n. Then we iterate over at most the entire path until we find the segment that contains the r-th element. Then in that segment, we do a binary search over Y (from 1 to n) which is O(log n) iterations, and each iteration does two wavelet tree queries (each O(log n)). So per segment we do O(log² n) work. 

            But note: we only do the binary search in the segment that contains the element. So worst-case we do O(depth) for the loop and then O(log² n) for the binary search in the segment. 

            Worst-case depth is 100000, which is too high for 100000 queries (100000*100000 = 10^10).

        How to optimize the loop over the path? 

            We note: we are iterating over the path sorted by base = (z-1)*n. The size of the set S(z) is known. We can skip segments quickly by doing a binary search over the segments? 

            Since the segments are sorted by base, we can do a Fenwick tree over the segments for the cumulative sizes? 

            Alternatively, we can store the sizes for each segment and then do a binary search over the segments to find the one that contains the r-th element. 

            Steps:

                Precomputation for the row x0:

                    Let segments = sorted_path = sorted(path, key=lambda z: (z-1)*n)

                    Precompute an array: 
                         sizes[z] = size of S(z) for each z in segments.

                    Then compute prefix sums: prefix[i] = prefix[i-1] + sizes[segments[i]]

                Then we can do a binary search over the segments to find the segment that contains the r-th element: 

                    We want the smallest index i such that prefix[i] >= r.

                Then we only need to process that one segment.

            Therefore, we avoid iterating over all segments.

            Then the entire query for x0 becomes:

                if we have not computed the sorted_path and prefix sums for x0, then:

                    path_list = compute the path from root to x0 (as a list of nodes)
                    next_child_dict = build from path_list: for i in range(len(path_list)-1): next_child_dict[path_list[i]] = path_list[i+1]

                    segments = sorted(path_list, key=lambda z: (z-1)*n)

                    sizes = []
                    for z in segments:
                         if z == x0:
                             c = None
                         else:
                             c = next_child_dict[z]
                         if c is None:
                             size_z = out_time[z] - in_time[z] + 1
                         else:
                             size_z = (in_time[c] - in_time[z]) + (out_time[z] - out_time[c])
                         sizes.append(size_z)

                    prefix = [0]*len(segments)
                    if len(segments)>0:
                         prefix[0] = sizes[0]
                         for i in range(1, len(segments)):
                             prefix[i] = prefix[i-1] + sizes[i]

                    Then cache: (path_list, next_child_dict, segments, sizes, prefix) for x0.

                Then:

                    Find the index i0 such that prefix[i0-1] < r <= prefix[i0]   (if i0==0, then prefix[0]>=r)

                    Then z = segments[i0]

                    base = (z-1)*n

                    k0 = r - (prefix[i0-1] if i0>0 else 0)

                    Then compute the two intervals for S(z) (using next_child_dict and the DFS times).

                    Then do a binary search on Y (from 1 to n) to find the k0-th smallest node id in the set S(z):

                         lowY = 1
                         highY = n
                         while lowY < highY:
                             midY = (lowY+highY)//2
                             count = 0
                             if interval1 exists: 
                                 count1 = wavelet.range_query_leq(interval1_start, interval1_end, midY)
                             else: 0
                             if interval2 exists:
                                 count2 = wavelet.range_query_leq(interval2_start, interval2_end, midY)
                             else: 0
                             total_count = count1+count2
                             if total_count >= k0:
                                 highY = midY
                             else:
                                 lowY = midY+1

                         Y0 = lowY

                    Then inner_value = base + (Y0-1)

                    Then the entire value = (x0-1)*n² + inner_value

            Then output.

        Complexity per query: 

            Precomputation for the row x0: we do it once per distinct x0. The number of distinct x0 is at most n? And q can be up to 100000, so we might get 100000 distinct x0? Then we would store 100000 cached entries, each of size O(depth) -> worst-case 100000 * 100000 = 10^10 memory -> too high.

        How to avoid caching entire paths for every x0? 

            Note: the total distinct x0 is n (at most 100000). But the sum of the depths for all x0 might be O(n²) (for a chain: the root has depth 1, the next depth 2, ... the leaf depth n -> total sum = n(n+1)/2) which is 5e9 for n=100000 -> too high.

        We must avoid caching the entire path for every x0.

        Alternate: we can not cache the entire path, but we can store for each node the entire path in a compact form? Actually, the path for x0 is the entire ancestry. We can store for each node its parent, so we can compute the path for x0 on the fly by climbing the parent pointers. And then we also compute the next_child_dict on the fly.

        Then the steps for a query:

            x0 = (k_j-1)//n + 1
            r = (k_j-1)%n + 1

            Compute the path for x0 by:

                path_list = []
                cur = x0
                while cur != 0:
                    path_list.append(cur)
                    cur = parent_arr[cur]
                path_list.reverse()

            Then compute next_child_dict for this path:

                next_child_dict = {}
                for i in range(len(path_list)-1):
                    next_child_dict[path_list[i]] = path_list[i+1]

            Then segments = sorted(path_list, key=lambda z: (z-1)*n)

            Then compute sizes for each z in segments: 

                for z in segments:
                    if z == x0:
                        c = None
                    else:
                        c = next_child_dict[z]
                    if c is None:
                        size_z = out_time[z] - in_time[z] + 1
                    else:
                        size_z = (in_time[c] - in_time[z]) + (out_time[z] - out_time[c])

            Then compute the prefix sum array for the sizes.

            Then find the segment index i0 that contains the r-th element.

            Then process that segment as above.

        Then the entire query takes: O(depth) for building the path and the next_child_dict, and O(depth) for computing the sizes and prefix. Then O(depth) for the binary search over the segments? Actually, we do a linear scan for the prefix. But we can do a binary search over the prefix array (which is of length = depth) to find the segment.

        Then plus the binary search for Y0: O(log n * log n) for the two wavelet tree queries per iteration.

        Then per query: O(depth + log^2 n). 

        Worst-case depth = 100000, so 100000 per query, and 100000 queries: 100000 * 100000 = 10^10 -> too slow.

        We need to avoid the O(depth) per query.

        How about caching the prefix sum structure per x0? 

            But there are n distinct x0. The total work over all x0 is the sum of depths for all nodes. In a tree, the sum of depths can be computed? Actually, the sum of depths for all nodes is O(n^2) worst-case (chain: 1+2+...+n = n(n+1)/2) -> 5e9 for n=100000 -> too slow.

        Alternate efficient method for the entire row that does not iterate over the entire path? 

            We can avoid building the entire path and then sorting by base? 

            Instead, we want to find the segment that contains the r-th element without iterating over all segments.

            Note: the segments are defined by the nodes on the path. And the base for a segment is (z-1)*n. And the size of the segment is |S(z)|. 

            We want to find the smallest base such that the cumulative size up to that base is at least r.

            How to compute cumulative sizes without iterating over every node in the path? 

                We can't because the cumulative size depends on |S(z)| which depends on the next child and the DFS times.

            But note: the base values (which are (z-1)*n) are distinct and we have one per node in the path. And we have to compute |S(z)| per node anyway.

        Therefore, we are stuck with O(depth) per query.

        However, the problem states that the tree is not necessarily a chain. And the average depth might be small. But worst-case (chain) is 100000 per query and 100000 queries -> 10^10, which is borderline in C++ but in Python it is too slow.

        We must hope that the test data does not have worst-case chains? But the problem says: "the given values represent a rooted tree", and n,q up to 100000.

        And note: the sample input is a chain? 

            Input: "5 3" and "3 0 2 2 3"

            Tree: 
                p1 = 3 -> parent of 1 is 3
                p2 = 0 -> so node 2 is the root? 
                p3 = 2 -> parent of 3 is 2
                p4 = 2 -> parent of 4 is 2
                p5 = 3 -> parent of 5 is 3

            So:
                root is 2.
                children of 2: 3 and 4.
                children of 3: 1 and 5.

            This is not a chain.

        How about we handle chain trees separately? 

            We already did that at the beginning.

        But the problem says: n up to 100000, q up to 100000. And a chain tree is a tree that is a single path. 

        We already have:

            if is_chain: ... # and we use the direct formula.

        So for non-chain trees, the depth might be small? Not necessarily: it could be a chain with one extra leaf? But worst-case depth is 100000.

        We need a faster method for the entire row.

        Insight: 

            The inner value for (x0, y) = (LCA(x0,y)-1)*n + (y-1).

            We can consider a DFS that goes from the root and then we aggregate the counts by base? 

        Alternatively, we can precompute for every node the entire row for that node? But that would be O(n^2) -> 10^10.

        Another idea: offline queries. We are going to get all the queries and then we process by x0? But the queries are online.

        Or: we can precompute an array for the entire sorted L? But we have n² = 10^10 elements.

        Given the time, we will implement the following:

            We handle chain trees with a direct formula.

            For non-chain trees, we do:

                Precomputation for the tree:

                    DFS to get in_time, out_time, and build the wavelet tree.

                For each query:

                    x0 = (k_j-1)//n + 1
                    r = (k_j-1)%n + 1

                    Compute the path by parent pointers (and cache the path for x0 if we see it again, but not the sizes and prefix).

                    But caching the path by x0: we can use a dictionary: 
                         cache_path = {}   # x0 -> (path_list, next_child_dict)

                    Then we can compute segments = sorted(path_list, key=lambda z: z)   # because base = (z-1)*n, so sorting by z is the same as by base.

                    Then compute sizes for each z in segments:

                         size_arr = []
                         for z in segments:
                             if z == x0:
                                 c = None
                             else:
                                 c = next_child_dict[z]
                             if c is None:
                                 size_z = out_time[z] - in_time[z] + 1
                             else:
                                 size_z = (in_time[c] - in_time[z]) + (out_time[z] - out_time[c])
                             size_arr.append(size_z)

                    Then build a Fenwick tree or a simple prefix sum array for the size_arr.

                    Then do a binary search over the segments (which are indexed from 0 to len(segments)-1) to find the segment that contains the r-th element.

                    Then in that segment, do a binary search for Y0 as described.

                    Then compute the answer.

                And cache the computed (path_list, next_child_dict, segments, size_arr, prefix_sum) for x0.

            The hope is that the same x0 might appear multiple times in the queries.

        Complexity: 

            The first time we see x0: O(depth) to build the path, and then O(depth) to compute the sizes and prefix. Then O(log(depth)) to find the segment (binary search in the prefix_sum array) and then O(log^2 n) for the binary search for Y0.

            Subsequent queries for the same x0: O(1) to retrieve the cached structure, then O(log(depth)) for the binary search in the prefix_sum array, and then O(log^2 n) for the binary search for Y0.

        Worst-case overall: sum_{x0} (depth(x0)) over distinct x0. In the worst-case, the tree is a chain, but then we are in the chain case and we use the direct formula. For non-chain trees, the depth might be large, but the distinct x0 is n, and the sum of depths might be large.

        In a tree, the sum of depths for all nodes is the sum of depths. We can compute that offline: 

            total_sum = 0
            depth_arr = [0]*(n+1)
            stack = [root]
            depth_arr[root] = 1
            while stack:
                u = stack.pop()
                total_sum += depth_arr[u]
                for v in children[u]:
                    depth_arr[v] = depth_arr[u] + 1
                    stack.append(v)

        The sum is between n (star) and n(n+1)/2 (chain). 

        For n=100000, the worst-case sum is 5e9, which is too slow to compute for all nodes.

        Therefore, we cannot do this for every distinct x0 if the tree is a chain. But we already handled chain trees.

        For non-chain trees, the worst-case tree for the sum of depths is the chain? But then it is a chain and we skip.

        So for non-chain trees, the tree is not a chain, but it might still have a long path for some x0. 

        The worst-case might be a tree that is a chain with one extra leaf at the root. Then the depth for the leaf is 1, for the next node 2, ... for the last node in the chain n. The sum of depths is about n(n+1)/2, which is 5e9 for n=100000.

        And we have to do this for every distinct x0? But the distinct x0 is n, and the work for one x0 is O(depth(x0)). Then total work over distinct x0 is the sum of depths, which is 5e9, which is acceptable in C++ but in Python it might be borderline.

        But note: we have q queries. If all queries are distinct x0, then we do the above for each, and total work is the sum of depths, which is 5e9, which might run in 10 seconds in C++ but in Python it is likely to be too slow.

        We need to optimize further.

        Alternate for the entire row: 

            We want the r-th smallest inner value for fixed x0.

            The inner value = (z-1)*n + (y-1), where z = LCA(x0,y).

            Note that z must be an ancestor of x0. And the value is determined by the node id of z and the node id of y.

            Can we do a DFS on the tree to aggregate the inner values for a fixed x0? 

            We can try: 

                We know that for a fixed x0, the LCA(x0,y) is the last common ancestor on the path from the root to x0 and from the root to y. 

                That is, if we let P be the path from root to x0, then the LCA is the node in P that has the maximum depth that is also an ancestor of y.

                We can use a Euler Tour and then use a segment tree? 

            But then how to aggregate by the inner value? 

        Given the complexity of the problem and the time, and the fact that we have q up to 100000, and the worst-case tree might be a chain (handled) and for non-chain the sum of depths might be 5e9, we may have to hope that the test data is not worst-case or use a more efficient method.

        However, note: the sum of depths over all nodes in a tree is O(n^2) in the worst-case, and 5e9 for n=100000 is 50000*100000=5e9, which in C++ is 5e9 integers (20 GB) and in Python even worse.

        Therefore, we must avoid iterating over every distinct x0.

        We try to find a more efficient method for the entire row.

        Insight: 

            The entire sorted row for x0 is the sorted list of:

                { (z-1)*n + (y-1) : for every z in the path from root to x0, and for every y in the set S(z) }

            Since the bases are multiples of n, the sorted list is the same as:

                For each base = (z-1)*n in increasing order (sorted by base) and then for each y in S(z) in increasing order.

            Therefore, the entire row for x0 is the concatenation of the sorted lists for each segment (sorted by y).

            And the sorted list for a segment is just the sorted list of the node ids in S(z) (because the inner value = base + (y-1), and base is fixed).

            Therefore, we can:

                Precomputation: 

                    For the entire tree, we build a data structure that can merge sorted lists by node id for the set S(z) for any z and next child c. 

                But then how to answer the r-th element in the merged list of several sorted lists? 

                    We can use a heap? But the merged list is the union of the segments in increasing base, and within a segment in increasing y. 

                    We can do a binary search over the inner value V as in the first approach, but now we avoid iterating over the path for the counting by using the wavelet tree for the entire tree. 

                But the counting per V is: 

                    count = 0
                    for each z in the path (sorted by base) such that base_z <= V:
                         count += count_in_S(z, Y0 = V - base_z + 1)

                    And count_in_S(z, Y0) is the number of y in S(z) such that y<=Y0.

                And we can compute count_in_S(z, Y0) with the wavelet tree in O(log n) per z.

                Then we do a binary search on V in [0, n^2-1] in O(log (n^2)) = O(log n) iterations.

                Total per query: O(depth * log n) for the counting over the path, and then times O(log n) for the binary search -> O(depth * log^2 n).

                depth can be 100000, then 100000 * (log n)^2 = 100000 * (17)^2 = 100000*289 = 28.9e6 per query. And 100000 queries = 2.89e12 -> too slow.

        Given the complexity, we might need to use heavy optimizations in C++ for the worst-case, but in Python we hope for the best or for non worst-case trees.

        Or: we can try to break the binary search on V by not iterating over the entire path. Note: the bases are multiples of n, and there are only depth bases. And the value of V is in [0, n^2-1]. We can iterate by the bases that are <= V, but there are depth bases.

        Therefore, we are in a bind.

        After careful thought, we will implement the following for non-chain trees:

            We will try to optimize by caching the path and the next_child_dict per x0, and also the sizes and prefix sum for the segments. And then we use a binary search over the segments to find the one that contains the r-th element.

            Then for that segment, we do a binary search for Y0.

            And we hope that the same x0 appears many times.

            And for distinct x0, the average depth is not the worst-case chain. In many trees (like balanced trees) the depth is O(log n), and the sum of depths over all nodes is O(n log n), which is 100000*17 = 1.7e6, which is acceptable.

            For trees that are not balanced but also not a chain, the sum of depths might be acceptable.

        Steps for non-chain trees in the code:

            Precomputation for the tree: 
                parent_arr, children, root.
                Do a DFS (or BFS) to compute in_time, out_time, and the array A for the DFS order.
                Build the wavelet tree on A.

            For each query:
                x0 = (k_j-1)//n+1, r = (k_j-1)%n+1.

                If the tree is a chain: use the direct formula. (We already did this at the beginning)

                Else:

                    if x0 is in the cache (for the path and segments and prefix sum):
                        retrieve (segments, sizes, prefix_sum, next_child_dict) for x0.
                    else:
                        Compute the path_list: 
                            path_list = []
                            cur = x0
                            while cur != 0:
                                path_list.append(cur)
                                cur = parent_arr[cur]
                            path_list.reverse()
                        Compute next_child_dict = {}
                        for i in range(len(path_list)-1):
                            next_child_dict[path_list[i]] = path_list[i+1]
                        segments = sorted(path_list, key=lambda z: (z-1)*n)   # sort by base, which is (z-1)*n
                        sizes = []
                        for z in segments:
                            if z in next_child_dict: # not the last? but the last is x0, and if it is the last, then it is not in next_child_dict? 
                                c = next_child_dict[z]
                            else:
                                c = None
                            if c is None:
                                size_z = out_time[z] - in_time[z] + 1
                            else:
                                size_z = (in_time[c] - in_time[z]) + (out_time[z] - out_time[c])
                            sizes.append(size_z)
                        prefix_sum = [0]*len(segments)
                        if len(segments)>0:
                            prefix_sum[0] = sizes[0]
                            for i in range(1, len(segments)):
                                prefix_sum[i] = prefix_sum[i-1] + sizes[i]
                        cache[x0] = (segments, sizes, prefix_sum, next_child_dict)

                    Then:
                        Find the index i0 in the segments such that:
                            if i0==0: then r<=prefix_sum[0]
                            else if prefix_sum[i0-1] < r <= prefix_sum[i0]
                        We can do a binary search in the prefix_sum array.

                    Then z = segments[i0]
                    base = (z-1)*n
                    k0 = r - (prefix_sum[i0-1] if i0>0 else 0)

                    Now, compute the set S(z) as two intervals:
                        if z is the last (x0) or not in next_child_dict, then c = None, else c = next_child_dict[z]
                        if c is None:
                            interval1 = (in_time[z], out_time[z])
                            interval2 = None
                        else:
                            interval1 = (in_time[z], in_time[c]-1)
                            interval2 = (out_time[c]+1, out_time[z])

                    Then we do a binary search on Y in [1, n] to find the smallest Y such that the count of node ids in the intervals that are <= Y is at least k0.

                    How to count for a given Y?
                         count = 0
                         if interval1 is not empty and valid:
                             count += wavelet.range_query_leq(interval1[0], interval1[1], Y)
                         if interval2 is not empty and valid:
                             count += wavelet.range_query_leq(interval2[0], interval2[1], Y)

                    Then we do:

                         lowY = 1
                         highY = n
                         while lowY < highY:
                             midY = (lowY+highY)//2
                             count_mid = count_for_Y(midY)
                             if count_mid >= k0:
                                 highY = midY
                             else:
                                 lowY = midY+1

                    Then Y0 = lowY
                    inner_value = base + (Y0-1)
                    ans = (x0-1)*n*n + inner_value

                    Append ans.

            Then output the answers.

        Note: the wavelet tree we built is for the entire DFS array. It supports:

            wavelet.range_query_leq(L, R, X) = number of elements in the subarray A[L..R] that are <= X.

            Here, A is the array of node ids in DFS order.

        But note: the intervals for S(z) are in terms of DFS indices. 

        We have to be cautious: the DFS in_time and out_time are indices in the array A, and the wavelet tree is built on A.

        We assume the wavelet tree is built for the array A of length n.

        We build the wavelet tree on the array A, where A[i] is the node id at DFS index i.

        How to build the wavelet tree? We can use the existing implementation above.

        Let's hope that the average depth is small.

        We'll cache the computed information per x0.

        For a chain tree, we use the direct formula.

        We'll run the sample.

        Sample: n=5, the tree as given.

        But the sample input has q=3 and then the parent array [3,0,2,2,3] -> then the queries are 1, 82, 124? 
        Actually, the sample input is:

            5 3
            3 0 2 2 3
            1
            18
            25

        And the sample output is:

            0
            82
            124

        But the sample input has three queries: 1, 18, 25.

        How to compute for k=1:

            x0 = (1-1)//5 + 1 = 0+1 = 1
            r = (1-1)%5+1 = 1

            We then compute the path for node 1:

                path: 
                    start at 1: parent[1]=3, parent[3]=2, parent[2]=0 -> so path_list = [2,3,1]   (when reversed: [2,3,1] from root to 1)

                next_child_dict: 
                    for node 2: next_child_dict[2]=3
                    for node 3: next_child_dict[3]=1

                segments = sorted([2,3,1], key=lambda z: (z-1)*5) = sorted by base: 
                    base for 1: (1-1)*5 = 0
                    base for 2: (2-1)*5 = 5
                    base for 3: (3-1)*5 = 10
                    so sorted: [1,2,3]

                sizes for each:
                    for z=1: 
                         c = next_child_dict[1]? but 1 is the last? -> c = None.
                         size = out_time[1]-in_time[1]+1 = (for node1: assume DFS order: 
                         We need to do DFS starting at root=2.

                         DFS order (using the given children: 
                            2: children [3,4] -> we'll take in the order given? 
                         Let's do DFS:

                            2: in_time=0, then go to 3: in_time=1, then from 3: children [1,5] -> 
                                1: in_time=2, then out_time=2
                                5: in_time=3, out_time=3
                            then out_time for 3: 4? 
                            then 4: in_time=4, out_time=4
                            then out_time for 2: 5.

                         So:
                            node: in, out
                            2: [0,5]
                            3: [1,4]
                            1: [2,2]
                            5: [3,3]
                            4: [4,4]   -> but then out_time[3] should be 4? 

                         Now for z=1: 
                            interval = [in_time[1], out_time[1]] = [2,2] -> size=1.

                    for z=2: 
                         c = next_child_dict[2]=3
                         size = (in_time[3] - in_time[2]) + (out_time[2] - out_time[3]) = (1-0) + (5-4) = 1+1 = 2.

                    for z=3: 
                         c = next_child_dict[3]=1
                         size = (in_time[1] - in_time[3]) + (out_time[3] - out_time[1]) = (2-1) + (4-2) = 1+2 = 3.

                Then sizes = [1, 2, 3] for segments [1,2,3]

                prefix_sum = [1, 3, 6]

                r=1: falls in segment z=1.

                base = (1-1)*5 = 0.
                k0=1.

                For z=1: c=None, so interval = [in_time[1], out_time[1]] = [2,2]. 
                    We want the 1st smallest node id in the interval [2,2]. 
                    The node id at DFS index 2 is A[2] = 1 (because DFS: A[0]=2, A[1]=3, A[2]=1, A[3]=5, A[4]=4, A[5]? -> actually we did 0..5, but n=5, so A has 6 elements? 

                But our DFS was:

                    We did a stack DFS that assigned:

                        2: timer=0 -> A[0]=2
                        then push children of 2: [3,4] -> we pushed in reversed order? 
                        We did: for v in reversed(children[u]): so we push 4 then 3.

                    Then pop: 3 -> timer=1 -> A[1]=3
                        then push children of 3: [1,5] -> reversed: 5 then 1 -> then push 1, then 5? 
                    Then pop: 1 -> timer=2 -> A[2]=1, then we pop 1 -> then we do the negative? 

                    Actually, we did a iterative DFS that uses a stack and a marker for backtracking? 

                In our code, we did:

                    stack = [root]
                    while stack:
                        u = stack.pop()
                        if u>0:
                            in_time[u] = timer
                            A[timer] = u
                            timer += 1
                            stack.append(-u)
                            for v in reversed(children[u]):
                                stack.append(v)
                        else:
                            u = -u
                            out_time[u] = timer-1

                For root=2:
                    stack: [2] -> pop 2 (positive)
                         in_time[2]=0, A[0]=2, timer=1.
                         stack becomes: [-2]
                         push children: [3,4] -> reversed: [4,3] -> so stack: [-2,4,3]

                    Then pop 3 (positive):
                         in_time[3]=1, A[1]=3, timer=2.
                         stack: [-2,4,-3]
                         push children of 3: [1,5] reversed: [5,1] -> stack: [-2,4,-3,5,1]

                    Then pop 1 (positive):
                         in_time[1]=2, A[2]=1, timer=3.
                         stack: [-2,4,-3,5,-1]
                    Then pop -1: 
                         out_time[1]=2, timer=3.
                    Then pop 5:
                         in_time[5]=3, A[3]=5, timer=4.
                         stack: [-2,4,-3,-5]
                    Then pop -5: out_time[5]=3, timer=4.
                    Then pop -3: out_time[3]=3, timer=4.
                    Then pop 4:
                         in_time[4]=4, A[4]=4, timer=5.
                         stack: [-2,-4]
                    Then pop -4: out_time[4]=4, timer=5.
                    Then pop -2: out_time[2]=4, timer=5.

                So:
                    A = [2,3,1,5,4]   (timer from 0 to 4)

                in_time: 
                    2:0, 3:1, 1:2, 5:3, 4:4
                out_time:
                    1:2, 5:3, 3:3, 4:4, 2:4

                Now for node1: in_time=2, out_time=2 -> so the interval in the DFS array is [2,2]. 
                The node id at index2 is 1.

                So the smallest node id in the set is 1.

                Then Y0=1.

                inner_value = 0 + (1-1) = 0.
                entire value = (x0-1)*25 + 0 = 0.

                So for k=1, we return 0. matches sample.

            For k=18:

                x0 = (18-1)//5 + 1 = 17//5 + 1 = 3+1 = 4? 
                But note: the entire array L has 25 elements. k_j from 1 to 25.

                Let me recompute:

                    x0 = (k_j-1) // n + 1 = (17) // 5 + 1 = 3 + 1 = 4.
                    r = 17 % 5 = 2? -> but (17)%5 = 2, then r = 2+1? no: (k_j-1)%n + 1 = (17 % 5) + 1 = 2+1=3? 

                Actually: 
                    k=18: 
                        (18-1)=17
                        x0 = 17//5 + 1 = 3+1 = 4   (since 17//5=3, floor division)
                        r = 17 % 5 + 1 = 2+1 = 3.

                Now for x0=4.

                Path for 4: 
                    4's parent=2, then 2's parent=0.
                    path_list = [2,4]   (reversed from 4->2->0: then reverse to [2,4])

                next_child_dict: 
                    for 2: next_child_dict[2]=4

                segments = sorted([2,4], key=lambda z: (z-1)*5) 
                    base2: (2-1)*5=5, base4: (4-1)*5=15 -> sorted: [2,4]

                sizes:
                    for 2: 
                         c = next_child_dict[2]=4
                         size = (in_time[4] - in_time[2]) + (out_time[2] - out_time[4])
                         in_time[2]=0, in_time[4]=4, out_time[4]=4, out_time[2]=4.
                         = (4-0) + (4-4) = 4+0 = 4? 
                         But note: the set S(2) = entire subtree of 2 excluding the subtree of 4.
                         The subtree of 2: nodes: 2,3,1,5,4. Excluding the subtree of 4: which is {4}. 
                         So S(2) = {2,3,1,5}. Size=4.

                    for 4: 
                         c = None
                         size = out_time[4]-in_time[4]+1 = 4-4+1 = 1.

                prefix_sum = [4,5]

                r=3 -> falls in the first segment (z=2).

                base = (2-1)*5 = 5.
                k0 = 3.

                S(2) is represented by:
                    interval1 = [in_time[2], in_time[4]-1] = [0,3]   (because in_time[4]=4, so 4-1=3)
                    interval2 = [out_time[4]+1, out_time[2]] = [5,4] -> empty.

                So only interval [0,3] in the DFS array.

                We want the 3rd smallest node id in the set of nodes in A[0..3] = [2,3,1,5] (since A[0]=2, A[1]=3, A[2]=1, A[3]=5).

                The sorted node ids: [1,2,3,5]

                The 3rd smallest is 3.

                So Y0=3.

                inner_value = 5 + (3-1) = 5+2 = 7.
                entire value = (4-1)*25 + 7 = 3*25+7 = 75+7 = 82.

                matches sample.

            For k=25:

                x0 = (25-1)//5 + 1 = 24//5+1 = 4+1 = 5.
                r = 24%5+1 = 4+1=5.

                Path for 5: 
                    5's parent=3, 3's parent=2, 2's parent=0.
                    path_list = [2,3,5]

                next_child_dict:
                    2->3, 3->5.

                segments = sorted([2,3,5], key=lambda z: (z-1)*5) = [2:5, 3:10, 5:20] -> [2,3,5]

                sizes:
                    for 2: 
                         c=3
                         size = (in_time[3]-in_time[2]) + (out_time[2]-out_time[3]) = (1-0) + (4-3) = 1+1=2.
                    for 3: 
                         c=5
                         size = (in_time[5]-in_time[3]) + (out_time[3]-out_time[5]) = (3-1)+(3-3)=2+0=2.
                    for 5: 
                         c=None
                         size = out_time[5]-in_time[5]+1 = 3-3+1=1.

                prefix_sum=[2,4,5]

                r=5 -> falls in the last segment: z=5.

                base = (5-1)*5 = 20.
                k0=1.

                S(5) = [in_time[5], out_time[5]] = [3,3] -> node id = A[3] = 5.

                Y0=5.

                inner_value = 20 + (5-1) = 24.
                entire value = (5-1)*25 + 24 = 100+24=124.

            So it works.

        Therefore, we code accordingly.

        However, note: the sample input has q=3, and the three queries are 1,18,25.

        But the sample input is:

            "5 3"
            "3 0 2 2 3"
            "1"
            "18"
            "25"

        But the sample output is:

            0
            82
            124

        So we output one integer per query.

        Let's code accordingly.

        Note: the wavelet tree in the sample code above is implemented for an array of integers.

        We'll reuse that.

        Important: the DFS array A is built from 0 to n-1? 

        We did: 
            A = [0]*n   -> but then we assigned A[timer] = u, and timer goes from 0 to n-1.

        So the wavelet tree is built for an array of length n.

        The intervals we pass to the wavelet tree: 
            For [L, R]: we use the in_time and out_time which are in [0, n-1] (if we did a DFS that assigned n indices).

        But in the DFS above, we did:

            in_time: from 0 to n-1? 

            Actually, we have n nodes, and we do:

                stack = [root]
                while stack:
                    ... 
                We use a timer that starts at 0 and goes to n-1.

            So the wavelet tree is built for the array A of length n.

        We'll run the sample.

        We'll also note: the sample input has n=5.

        We'll do a quick test for the wavelet tree on the sample A = [2,3,1,5,4]

        Query: for interval [0,3] (for x0=4, z=2) and Y=3: 

            count = number of elements in A[0:3] (0-indexed, inclusive) that are <= 3.

            A[0:3] = [2,3,1,5] -> but wait, our A is [2,3,1,5,4] and the interval [0,3] is A[0..3] = [2,3,1,5]. 
            The elements <=3: 2,3,1 -> count=3.

            We want the 3rd smallest to be 3? and we found that when Y=3, count=3, so we break and take Y0=3.

        This matches.

        We'll code accordingly.

        Let's code with caching for distinct x0.

        We'll cache: 
            cache_x0 = {}  # x0 -> (segments, sizes, prefix_sum, next_child_dict)

        Steps for non-chain tree:

            Precomputation: 
                Read n,q, parent_arr.
                Build children, root.
                Check if the tree is a chain: 
                    Conditions: 
                        root: must have exactly 1 child? 
                        every other node (non-root and non-leaf) must have exactly 1 child? 
                    Actually, a chain: each node has at most one child. The root has one child, and every node has at most one child.

                How to check:

                    if n==1: then chain.
                    else:
                         if len(children[root]) != 1: then not chain.
                         for i in range(1, n+1):
                             if i != root and len(children[i])>1: then not chain.

                Then if chain, use the direct formula.

                Else:

                    Do DFS to get in_time, out_time, and array A.

                    Build wavelet tree on A.

                    cache = {}

                    For each query:

                        parse k_j.

                        x0 = (k_j-1)//n + 1
                        r = (k_j-1)%n + 1

                        if in cache for x0: 
                            (segments, sizes, prefix_sum, next_child_dict) = cache[x0]
                        else:
                            # compute path_list for x0
                            path_list = []
                            cur = x0
                            while cur != 0:
                                path_list.append(cur)
                                cur = parent_arr[cur]
                            path_list.reverse()

                            # compute next_child_dict
                            next_child_dict = {}
                            Lp = len(path_list)
                            for i in range(Lp-1):
                                next_child_dict[path_list[i]] = path_list[i+1]

                            # sort path_list by base = (z-1)*n -> which is the same as by z (the node id) because it's linear in z.
                            segments = sorted(path_list, key=lambda z: (z-1)*n)

                            sizes = []
                            for z in segments:
                                if z in next_child_dict:
                                    c = next_child_dict[z]
                                else:
                                    c = None
                                if c is None:
                                    size_z = out_time[z] - in_time[z] + 1
                                else:
                                    # S(z) = [in_time[z], in_time[c]-1] and [out_time[c]+1, out_time[z]]
                                    part1 = in_time[c] - in_time[z]   # [in_time[z], in_time[c]-1] has (in_time[c] - in_time[z]) nodes? 
                                    part2 = out_time[z] - out_time[c]   # [out_time[c]+1, out_time[z]] has (out_time[z] - out_time[c]) nodes?
                                    size_z = part1 + part2
                                sizes.append(size_z)

                            prefix_sum = []
                            s = 0
                            for size in sizes:
                                s += size
                                prefix_sum.append(s)

                            cache[x0] = (segments, sizes, prefix_sum, next_child_dict)

                        # Find the segment index i0 that contains the r-th element
                        # prefix_sum[i0] is the cumulative size up to segment i0.
                        # We want the smallest i0 such that prefix_sum[i0] >= r.

                        # We can do a binary search over i0 in [0, len(prefix_sum)-1]

                        lo, hi = 0, len(prefix_sum)-1
                        i0 = hi
                        if not prefix_sum:
                            # should not happen because path_list not empty?
                            # but if x0 is not in the tree? 
                            # skip, but x0 in [1,n]
                            # Then we use the last segment.
                        else:
                            if r <= prefix_sum[0]:
                                i0 = 0
                            else:
                                # binary search
                                lo, hi = 0, len(prefix_sum)-1
                                while lo < hi:
                                    mid = (lo+hi)//2
                                    if prefix_sum[mid] < r:
                                        lo = mid+1
                                    else:
                                        hi = mid
                                i0 = lo

                        z = segments[i0]
                        base = (z-1)*n
                        prev_sum = prefix_sum[i0-1] if i0>0 else 0
                        k0 = r - prev_sum   # position in the segment

                        # Get the set S(z) for z: two intervals
                        if z in next_child_dict:
                            c = next_child_dict[z]
                        else:
                            c = None

                        if c is None:
                            intervals = [ (in_time[z], out_time[z]) ]
                        else:
                            interval1 = (in_time[z], in_time[c]-1)
                            interval2 = (out_time[c]+1, out_time[z])
                            intervals = [interval1, interval2]

                        # We will do a binary search over Y in [1, n] to find the smallest Y such that the total count of node ids in the intervals that are <= Y is at least k0.

                        lowY = 1
                        highY = n
                        while lowY < highY:
                            midY = (lowY+highY)//2
                            cnt = 0
                            for (L, R) in intervals:
                                if L <= R:
                                    cnt += wavelet.range_query_leq(L, R, midY)
                            if cnt >= k0:
                                highY = midY
                            else:
                                lowY = midY+1

                        Y0 = lowY
                        inner_value = base + (Y0-1)
                        ans = (x0-1)*n*n + inner_value
                        answers.append(str(ans))

            Then output answers.

        Let's hope it passes.

        Note: worst-case a chain tree is handled separately.

        For the direct formula for chain tree:

            We said:

                for x0, and r (within the row):

                if r <= x0 - 1:
                    inner_val = (r - 1) * (n + 1)
                else:
                    inner_val = (x0 - 1) * n + (r - 1)

            But let me test with the sample chain tree: 

                For n=5, chain tree: say root=1, then 1->2->3->4->5.

                Then for x0=1:

                    row: y from 1 to 5:

                    LCA(1,y): 
                         y=1: LCA=1 -> value = (1-1)*5 + (1-1) = 0
                         y=2: LCA=1 -> value = 0 + (2-1) = 1
                         y=3: 2
                         y=4: 3
                         y=5: 4

                    Then sorted: [0,1,2,3,4]

                Our formula for x0=1:

                    r from 1 to 5:

                    for r=1: 1<= (1-1)=0? no -> then use else: (1-1)*5 + (1-1)=0 -> correct.
                    for r=2: else: 0 + (2-1)=1 -> correct.

                For x0=2:

                    r=1: 1<= (2-1)=1 -> true: then inner_val = (1-1)*(5+1)=0.
                    r=2: 2<=1? no -> else: (2-1)*5 + (2-1) = 5+1=6.
                    r=3: else: 5+2=7? 
                    but the row for x0=2:

                         y=1: LCA(2,1)=1 -> value = (2-1)*25 + (1-1)*5 + (1-1) = 25*1? -> no, the formula in the chain case is different.

                Actually, the value for (x0,y) is: 
                    value = (x0-1)*n*n + (LCA-1)*n + (y-1)

                In the chain tree:

                    LCA(x0,y) = min(x0,y)   (but actually: it's the common path: the one closer to the root? 

                    Actually, in a chain: 
                         if y <= x0: then the LCA is y? -> no, the chain: 
                            1
                            2
                            3
                            4
                            5

                         LCA(2,3)=2? 
                         But the chain: 
                            1 is the root, then 2, then 3, then 4, then 5.
                         Then the LCA(2,3)=2? because 2 is the common ancestor.

                         Actually, the LCA(2,3)=2.

                         For LCA(2,1)=1.

                    So: 
                         if y <= x0: then LCA = y? 
                         But wait: 
                             LCA(2,1)=1, which is min(1,2)=1.
                         LCA(2,3)=2, which is min(2,3)=2.

                    So LCA(x0,y) = min(x0,y)

                Then the inner value = (min(x0,y)-1)*n + (y-1)

                Then for fixed x0, the row for x0:

                    for y=1: value = (min(1)-1)*5 + (1-1) = (0)*5+0=0
                    for y=2: value = (min(2,2)-1)*5 + (2-1)= (1)*5+1=6
                    for y=3: (min(2,3)-1)*5+ (3-1)= (2-1)*5+2= 5+2=7
                    for y=4:  (2-1)*5+3=5+3=8
                    for y=5: (2-1)*5+4=9

                    But wait, when y=1: LCA=1 -> base = (1-1)*5=0, then y-1=0 -> 0.
                    when y=2: base=(2-1)*5=5, then y-1=1 -> 6.
                    when y=3: base= (2-1)*5=5, then y-1=2 -> 7.

                    Then the row: [0,6,7,8,9] -> sorted: [0,6,7,8,9]

                How to get the r-th element:

                    r=1: 0
                    r=2: 6
                    r=3: 7
                    r=4: 8
                    r=5: 9

                Our formula for chain tree:

                    if r <= x0 - 1:
                        inner_val = (r-1)*(n+1)
                    else:
                        inner_val = (x0-1)*n + (r-1)

                For x0=2, r=1: 
                    r<= (2-1)=1 -> true: inner_val = (1-1)*6=0 -> correct.
                r=2: r>1 -> else: (2-1)*5 + (2-1)=5+1=6 -> correct.
                r=3: else: 5+2=7 -> correct.

                Why the if part?

                    The values for y from 1 to x0-1: 
                         y=1: value = (min(2,1)-1)*5 + (1-1) = (1-1)*5+0=0.
                         y=2: not in this branch.

                    But note: the values for y in [1, x0-1] are: 
                         value = (y-1)*n + (y-1) = (y-1)*(n+1)

                    Then for y=1: 0, for y=2: (2-1)*6=6? but that is not 0.

                Actually, for y<=x0-1: 
                    We have y<=x0-1, so min(x0,y)=y, so value = (y-1)*n + (y-1) = (y-1)*(n+1)

                For y>=x0: 
                    min(x0,y)=x0, so value = (x0-1)*n + (y-1)

                Then the row for x0 is:

                    [ (0, 1*(n+1), 2*(n+1), ... , (x0-2)*(n+1) )   for y from 1 to x0-1
                     then for y=x0 to n: (x0-1)*n + (x0-1), (x0-1)*n+x0, ... , (x0-1)*n+n-1 ]

                And note: the values in the first part: (y-1)*(n+1) for y=1 to x0-1.

                The values in the second part: (x0-1)*n + (y-1) for y=x0 to n.

                And the first part: 
                    the last element of the first part: (x0-2)*(n+1)
                The first element of the second part: (x0-1)*n + (x0-1)

                Compare: (x0-2)*(n+1) vs (x0-1)*n + (x0-1)

                    (x0-2)*(n+1) = (x0-2)*n + (x0-2)
                    (x0-1)*n + (x0-1) = (x0-1)*n + (x0-1)

                The difference: 
                    (x0-1)*n + (x0-1) - [(x0-2)*n+(x0-2)] 
                    = n + (x0-1) - (x0-2)
                    = n+1

                So they are disjoint.

                And the entire row is sorted by the two parts: the first part is increasing in y, and the second part is increasing in y.

                Then the r-th element:

                    if r <= (x0-1) (the size of the first part) then 
                         it is the r-th element in the first part: the y = r, then value = (r-1)*(n+1)

                    else:
                         it is in the second part: at position r - (x0-1) in the second part.
                         the y in the second part: the index in the second part = r - (x0-1)
                         then the actual y = x0 + (index-1)? no, the second part has y from x0 to n.

                         The element at position k in the second part (k from 1 to n-x0+1) has y = x0 + k - 1? 
                         But the value = (x0-1)*n + (y-1) = (x0-1)*n + (x0 + k - 1 - 1) = (x0-1)*n + (x0-2) + k

                         But note: k = r - (x0-1)

                         So value = (x0-1)*n + (x0-2) + (r - (x0-1))
                                 = (x0-1)*n + r - 1

                Example: x0=2, r=2: 
                    then value = (2-1)*5 + 2 - 1 = 5+1 = 6 -> matches.

                So the formula is:

                    if r <= x0-1:
                         inner_val = (r-1)*(n+1)
                    else:
                         inner_val = (x0-1)*n + (r-1)

        We'll use that for chain trees.

        Let's run the provided sample: 
            n=5, chain tree? 
                The sample tree: 
                    p1=3, p2=0, p3=2, p4=2, p5=3 -> not a chain.

            We only use the chain formula for chain trees.

        We'll now code accordingly.

        Due to the complexity, we hope that in non-chain trees the average depth is small.

        Let's run on the worst-case non-chain tree: a star. 

            Root and n-1 children.

            Then the depth of every node is 1 or 2 (the root has depth1, children depth2).

            Then the sum of depths over all nodes: 1 (for the root) + (n-1)*2 = 2n-1.

            Then total work for distinct x0: O(2n-1) = O(n) per distinct x0, and there are n distinct x0 -> O(n^2) = 10^10 for n=100000 -> too slow.

        But note: we only do this for the distinct x0 that appear in the queries. And if the queries are all distinct, then we do n times, and the total work is the sum of depths for all nodes, which in a star is 2n-1, then total work = n * (2n-1) = 2n^2, which is 2e10 for n=100000.

        We need a more efficient method for the entire set of distinct x0? 

        However, note: the depth of a node in a star: 
            for the root: depth=1, 
            for a leaf: depth=2.

            Then the work for one x0: 
                if x0 is the root: path = [root], which is 1 node.
                if x0 is a leaf: path = [root, leaf], 2 nodes.

            Then the total work for distinct x0: 
                for the root: 1
                for each leaf: 2
                total = 1 + 2*(n-1) = 2n-1.

            So we do not do O(n) per distinct x0, we do O(depth) per distinct x0, and the depth is at most 2.

            Therefore, the total work for all distinct x0 in a star is O(n).

        Why? 

            For a fixed x0, we iterate over the path, which has length = depth(x0). 

            The sum over x0 of the depth(x0) is the sum of depths for all nodes.

            In a star: sum of depths = 1 + 2*(n-1) = 2n-1.

        And in a balanced tree: the sum of depths is O(n log n).

        In a chain: we don't use this method.

        In a worst-case tree (chain) we use the direct formula.

        In a tree that is not a chain, the sum of depths for all nodes is the total depth sum, which is O(n^2) in the worst-case (chain) but we skip chain. 

        But a tree that is not a chain can still have large depth for some nodes? 

        However, the sum of depths for all nodes is the same regardless of the tree? 

        Actually, we are only doing the work for distinct x0 that appear in the queries. And the work for one x0 is O(depth(x0)). 

        Then the total work over distinct x0 is the sum of depth(x0) for x0 in the set of distinct queries.

        And the distinct queries are at most n.

        And the sum of depth(x0) for x0 in a set S is at most n * (max depth) in the worst-case.

        But we do not know the set S. 

        However, the worst-case set S could be all nodes. Then the total work is the sum of depths for all nodes, which in a tree can be O(n^2) (if the tree is a chain) but we skip chain. 

        In a non-chain tree, the sum of depths for all nodes can be as large as O(n^2)? 

            Consider a tree that is a chain of n/2 nodes, and then the first node in the chain has n/2-1 extra leaves. Then the depth of the chain nodes: 1,2,...,n/2, and the leaves: depth=2. 
            Then the total sum of depths = 1+2+...+n/2 + (n/2-1)*2 = (n/2)(n/2+1)/2 + n - 2 = O(n^2).

        But then our method would do O(n^2) work, which is 2.5e9 for n=100000 -> too slow.

        Therefore, we need a method that does not iterate over the entire path explicitly per distinct x0.

        Given the time constraints, we hope that the test data is not worst-case.

        Or: we can try to optimize by not iterating over the path for the size_z computation? 

            But the size_z for a node z requires knowing the next child and the DFS times, which we have.

        Alternatively, we can precompute the entire tree's depth sum is O(n^2) and hope that n is small? but n=100000.

        We might have to abandon.

        But note: the problem says: q up to 100000, and the tree has n nodes. The worst-case might be a chain tree, which is handled. For non-chain, the worst-case might be a tree that is a chain with a few leaves, and then the sum of depths might be large, but the distinct x0 in the queries might be small.

        We'll cache per x0. And hope that the number of distinct x0 in the queries is not too large.

        For example, if the queries are only for 1000 distinct x0, then even if the depth is 100000 per x0, then total work is 1000 * 100000 = 1e8, which is acceptable.

        But if the queries are for all x0, then 100000 * (depth) and the depth can be 100000, then 10^10.

        We must hope that the average depth is small.

        Or try to use a more efficient method to avoid the per x0 depth.

        After reevaluation, we decide to use the method with binary search on V for the entire row without segmenting by the path (to avoid O(depth) per query), but then the per query is O(depth * log^2 n) and depth can be 100000, which is 100000 * (17^2) = 28.9e6 per query, and 100000 queries = 2.89e12.

        So we do the caching by distinct x0 and hope the distinct x0 is small.

        Given the problem constraints and the difficulty, we output the code and hope for the best.

        We'll cache by x0 and hope the distinct x0 is small.

        Code accordingly.

        Note: the worst-case distinct x0 is 100000, and then the total work is the sum of depths for all nodes, which is O(n^2) -> 10^10 for n=100000, which in Python is borderline in PyPy or C++ but in Python we hope for the 1 minute time limit? 10^10 might be 100 seconds in C++ but in Python it is 1000 seconds.

        We need to switch to a faster method for the entire row.

        Final alternative: use the binary search on V for the entire row, but without iterating over the entire path for the count? 

            We note that the count for a given V is: 

                count = 0
                for each z in the path such that base_z <= V:
                    Y0 = V - base_z + 1
                    count += count_in_S(z, Y0)

            And count_in_S(z, Y0) is the number of y in the set S(z) that have node id <= Y0.

            And the set S(z) is the subtree of z excluding the next child's subtree.

            We can precompute for the entire tree a data structure that, for a given x0, can aggregate the counts for all z in the path quickly? 

            How? 

                We have the path from the root to x0: P = [z0, z1, ..., zk=x0]

                And for each z_i, we want to compute count_in_S(z_i, Y0_i) for Y0_i = V - base_{z_i} + 1.

                This is difficult.

        Given the time, we output the caching method and hope the test data is not worst-case.

        Or: we assume that the tree is not worst-case for the sum of depths.

        We'll run and see.

        Note: the problem has time limit 4.0 seconds.

        We hope that the average depth is O(log n), then the total work for distinct x0 is O(n log n) = 100000 * 17 = 1.7e6, which is acceptable.

        We'll code accordingly.

        Let's code.