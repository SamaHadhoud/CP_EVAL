We are given T test cases. For each test case, we are given:
  N: the range of integers (1 to N)
  K: the number of already generated numbers
  A list A of K integers (each between 1 and N)

Condition: at least one integer in [1, N] has not appeared at least twice.

We are to compute the expected number of additional numbers until every integer from 1 to N has appeared at least twice.

Observations:

The state of the system can be described by how many numbers have been seen 0, 1, or at least 2 times. Specifically, let:
  a = number of integers that have appeared exactly 0 times
  b = number of integers that have appeared exactly 1 time
  c = number of integers that have appeared at least 2 times

Note: a + b + c = N.

Initially, we can compute (a, b, c) from the first K numbers.

The stopping condition is when c = N (i.e., every integer has appeared at least twice).

We are to compute the expected additional steps from the current state (a, b, c) until we reach state (0,0,N).

We can model this as an absorbing Markov chain. The states are (a, b) (since c = N - a - b). The absorbing state is (0,0). We are interested in the expected steps to absorption.

State transitions:
  In state (a, b), we generate a number uniformly at random from 1 to N.

  The next number can be:
    - One of the a numbers that have not appeared at all: probability p1 = a/N.
        Then: 
          a decreases by 1, b increases by 1: new state (a-1, b+1)
    - One of the b numbers that have appeared once: probability p2 = b/N.
        Then:
          b decreases by 1, c increases by 1: new state (a, b-1)
    - One of the c numbers that have already appeared at least twice: probability p3 = c/N = (N - a - b)/N.
        Then: the state remains (a, b)

Let E(a, b) be the expected additional steps from state (a, b).

We can write the recurrence:
  E(a, b) = 1 + (a/N)*E(a-1, b+1) + (b/N)*E(a, b-1) + (c/N)*E(a, b)

Rearranging:
  E(a, b) - (c/N)*E(a, b) = 1 + (a/N)*E(a-1, b+1) + (b/N)*E(a, b-1)
  => ( (N - c) / N ) * E(a, b) = 1 + (a/N)*E(a-1, b+1) + (b/N)*E(a, b-1)

But note: N - c = a + b.

So:
  ( (a + b) / N ) * E(a, b) = 1 + (a/N)*E(a-1, b+1) + (b/N)*E(a, b-1)

Multiply both sides by N:
  (a + b) * E(a, b) = N + a * E(a-1, b+1) + b * E(a, b-1)

Therefore:
  E(a, b) = [ N + a * E(a-1, b+1) + b * E(a, b-1) ] / (a + b)

Boundary conditions:
  When a = 0 and b = 0: E(0,0) = 0.

However, note that we have a condition: the state must not be absorbing at the beginning (at least one number has not been seen twice). But in our state (a,b), the absorbing state is (0,0). 

But note: if a=0 and b>0, then we still need to get the numbers that have been seen only once to appear again.

The recurrence is defined for:
  a in [0, N], b in [0, 2*N]? But note: a+b cannot exceed N? Actually, a+b = N - c, and c>=0 so a+b <= N.

But actually, the state (a, b) must satisfy: a >= 0, b>=0, and a+b <= N.

However, note that when we start with K numbers, we have:
  a = number of integers that never appeared
  b = number of integers that appeared exactly once

Then the recurrence holds for all states that are not absorbing (i.e., (0,0)).

But note: the recurrence for E(a,b) uses E(a-1,b+1) and E(a,b-1). So we must compute E for states with lower a and b, and also states with same a but lower b? Actually, the state (a-1, b+1) has the same a+b (because (a-1)+(b+1)=a+b) and state (a, b-1) has a+b-1. 

So the recurrence is defined in terms of states with the same total a+b (for the first term) and a state with a+b reduced by 1 (for the second term). Therefore, we can compute E(a,b) by increasing order of (a+b). 

Specifically, the value of a+b can range from 0 to N (because initially a+b <= N, and in the recurrence we only go to states with the same a+b or less). 

Therefore, we can iterate by s = a+b from 0 to N. For each s, we consider all pairs (a, b) such that a+b = s? Actually, no: the recurrence for E(a,b) might require E(a-1, b+1) which has a+b = (a-1)+(b+1)=a+b, same as s. So we have to iterate by s (the total a+b) and then for each a (from 0 to s) and b = s - a? But note: in the recurrence for E(a,b), we have:
  E(a,b) = [ N + a * E(a-1, b+1) + b * E(a, b-1) ] / (a+b)

But note: (a-1, b+1) has the same s = a+b. So we have a dependency within the same s. Therefore, we must iterate over the states with the same s in a particular order? 

Alternatively, we can rearrange the recurrence to isolate E(a,b). However, note that the term E(a-1, b+1) is for a state that is not computed in the same s group? Actually, if we fix s = a+b, then for a given state (a,b) (with a>=1) the state (a-1, b+1) is also in the same s. But if we traverse a from s down to 0, then when computing E(a,b), we might have already computed E(a-1, b+1) because a-1 is less than a? Actually, we can traverse a in decreasing order? 

But note: in the state (a,b), we also have E(a, b-1). However, (a, b-1) has s' = a + (b-1) = s-1, which is less than s. So we can compute all states with s' = s-1 first. Then for a fixed s, we have the recurrence:

  E(a,b) = [ N + a * E(a-1, b+1) + b * E(a, b-1) ] / s

But the term E(a-1, b+1) is for state (a-1, b+1) which has the same s. So we have:

  s * E(a,b) = N + a * E(a-1, b+1) + b * E(a, b-1)

We can write this for each (a,b) with a+b = s. However, we have multiple states with the same s. How to break the dependency? 

Notice that for a fixed s, we have states: (a, b) = (a, s-a) for a from 0 to s. But note: we cannot have a>N or b>N? Actually, we have the constraint that a<=N and b<=N, and also s<=N.

In the recurrence, when a=0, then the term with E(a-1, b+1) does not exist. Similarly, when b=0, the term E(a, b-1) does not exist. So:

  If a == 0 and b == 0: E(0,0)=0.

  For state (a,b) with a+b = s (s>0):

    If a == 0, then:
        E(0, b) = [ N + 0 + b * E(0, b-1) ] / b   [because a=0, so the term with E(-1, b+1) is skipped? Actually, we skip the term with a]
        => E(0, b) = (N + b * E(0, b-1)) / b

    If a > 0 and b == 0, then:
        E(a, 0) = [ N + a * E(a-1, 1) + 0 ] / a
        => E(a,0) = (N + a * E(a-1, 1)) / a

    If a>0 and b>0, then:
        E(a,b) = [ N + a * E(a-1, b+1) + b * E(a, b-1) ] / s

But note: the term E(a-1, b+1) is for a state (a-1, b+1) that has the same s. So we have a dependency on a state that we haven't computed yet? 

Alternatively, we can use a different ordering: for a fixed s, traverse a from s down to 0? Then when we compute E(a,b) (with b=s-a), we would have already computed E(a-1, b+1) because that state has a' = a-1 and b'=b+1, and a' = a-1 < a? But note: when we traverse a from high to low, then a-1 is the next state to be computed? Actually, no: we are going from high a to low a. So when we compute E(a,b) (with a high), then we haven't computed E(a-1, b+1) because a-1 is less than a and we are going downwards? 

Alternatively, we can note that the term E(a-1, b+1) is for a state that has the same s, but with a decreased by 1 and b increased by 1. This state has a' = a-1 and b'=b+1. We can express the recurrence as:

  s * E(a,b) = N + a * E(a-1, b+1) + b * E(a, b-1)

But note that E(a, b-1) is for s-1, which we have already computed for all states. So the only unknown in the equation for E(a,b) is E(a-1, b+1). However, we also have the state (a-1, b+1) which is in the same s. 

So we have a system of linear equations for the states in the same s. The number of states per s is at most s+1 (which is <= N+1). And N is at most 3000. Therefore, we could solve for each s by setting up a tridiagonal system? 

But note: the recurrence for state (a, s-a) is:

  s * E(a, s-a) = N + a * E(a-1, s-a+1) + (s-a) * E(a, s-a-1)

But note: E(a, s-a-1) is for s-1 (because a + (s-a-1) = s-1) and we have already computed all states for s-1. So the only unknown from the same s is E(a-1, s-a+1). 

How does the state (a-1, s-a+1) relate to the current state? It is the state with a'=a-1 and b'= (s - a) + 1 = s - (a-1) - 1? Actually, b' = s - (a-1) is not true: because in state (a-1, ?) the total s is fixed: a-1 + b' = s, so b' = s - (a-1) = s - a + 1.

Therefore, the recurrence for state (a, s-a) involves E(a-1, s-a+1) and E(a, s-a-1). 

We can write for each a in [0, s] (with state (a, s-a)):

  For a=0: 
      s * E(0, s) = N + 0 + (s) * E(0, s-1)   [but note: E(0, s-1) is for s-1? Actually, the state E(0, s-1) has a=0, b=s-1, so total s-1. But wait: the state (0, s) has a=0 and b=s? Then the term E(0, s-1) is for the same a=0 but with b=s-1? Actually, that state has total a+b = 0+(s-1)=s-1. So we have:

      s * E(0, s) = N + s * E(0, s-1)   --> but this is not correct because the term for b is (b * E(a, b-1)) and here b = s, so we have s * E(0, s-1). 

      => E(0, s) = (N + s * E(0, s-1)) / s.

  For a = s (then b=0): 
      s * E(s,0) = N + s * E(s-1, 1) + 0

  For 0 < a < s: 
      s * E(a, s-a) = N + a * E(a-1, s-a+1) + (s-a) * E(a, s-a-1)

Notice that the term E(a, s-a-1) is for the state (a, s-a-1) which has total s-1, so we know it. But the term E(a-1, s-a+1) is for the state (a-1, s-a+1) which has total s. 

Therefore, the equations for the same s are not independent: they form a chain? Actually, if we look at the states in the same s, we can index by a. The recurrence for state a uses state a-1. So we can express:

  E(a, s-a) = (N + a * E(a-1, s-a+1) + (s-a) * E(a, s-a-1)) / s

But note: the state (a-1, s-a+1) has the same s and we are going to compute it? How? We can traverse a from 0 to s? Then when we compute state a, we have not computed state a-1? 

Alternatively, we can traverse a from high to low? Then when we compute state a, we have already computed state a-1? But note: the recurrence for state a uses state a-1 (which we have computed if we traverse from high to low) and state (a, s-a-1) which is from s-1 and we have computed all states for s-1.

But wait: the state (a-1, s-a+1) is the same as (a-1, (s) - (a-1))? Actually, the total of state (a-1, s-a+1) is (a-1) + (s-a+1) = s. So it's in the same group. 

If we traverse a from s down to 0, then when we compute state (a, s-a), we would have already computed state (a-1, s-a+1) because a-1 < a and we traverse from high a to low a? Actually, no: if we traverse from high a to low a, then we compute state (s,0) first, then (s-1,1), then (s-2,2), ... until (0,s). Then for state (a, s-a), we need E(a-1, s-a+1) which is the state we computed just before? Because (a-1, s-a+1) has a'=a-1 which is less than a? But in our downward traversal, we compute state (a, s-a) and then next we compute state (a-1, s - (a-1)) = (a-1, s-a+1). So we have not computed E(a-1, s-a+1) at the time of computing E(a, s-a). 

Alternatively, we can traverse a from 0 to s? Then at state a, we have computed all states with a' < a? But then the recurrence for state a uses state a-1 (which we have computed) and state (a, s-a-1) (which is from s-1 and we have computed). But it also uses state (a-1, s-a+1) which is the same as (a-1, (s) - (a-1))? Actually, that state has a'=a-1, which we have computed? 

However, note: the state (a-1, s-a+1) has a'=a-1 and b'=s-a+1. But in our iteration for fixed s, we are iterating a from 0 to s. When we are at state a, we have already computed state a-1? Yes. But the state a-1 we computed in this same s group: (a-1, s - (a-1)) = (a-1, s - a + 1). So that is exactly the state we need.

Therefore, we can iterate a from 0 to s for each s from 0 to N.

But note: the recurrence for a=0 and a=s are special. And for 0<a<s:

  E(a, s-a) = [ N + a * E(a-1, s-a+1) + (s-a) * E(a, s-a-1) ] / s

But wait: E(a, s-a-1) is the state (a, s-a-1) which has total s-1. We assume that we have computed all states for s-1 already. And we process s from 0 to N.

So the plan:

  Precompute E[a][b] for all a in [0, N] and b in [0, N] such that a+b <= N.

  We iterate s = 0, 1, 2, ..., N:
      if s==0: then a=0, b=0 -> E[0][0] = 0.
      for s from 1 to N:
          for a from 0 to s:
              b = s - a
              if a==0:
                  E[0][b] = (N + b * E[0][b-1]) / b   [but note: when b=0? then s=0, so we skip s>=1. Also, for a=0 and b>0: valid]
              if a==s (so b=0):
                  E[a][0] = (N + a * E[a-1][1]) / a   [but note: a>0 here because s>=1 and a=s>=1]
              else: (0 < a < s)
                  E[a][b] = (N + a * E[a-1][b+1] + b * E[a][b-1]) / s

But note: the recurrence for the same s uses E[a-1][b+1] which is the state (a-1, b+1) = (a-1, s - a + 1) and we have computed E[a-1][?] for a'=a-1 in the same s? And we are iterating a from 0 to s. So when we are at a, we have already computed E[a-1][b+1] in the same s group? 

Yes: because for a fixed s, we are iterating a from 0 to s. So when we compute state (a,b), we have already computed state (a-1, b+1) because we did a-1 (which is less than a) in the same s.

Therefore, we can compute E[a][b] for all a, b with a+b<=N with two nested loops: s from 0 to N, and a from 0 to s (then b = s - a).

However, note: the state (a,b) must satisfy a>=0, b>=0, a<=N, b<=N, and a+b<=N. And our iteration: s from 0 to N, and a from 0 to s, and b=s-a, then a<=s<=N and b=s-a<=s<=N, and a>=0, b>=0. So we cover exactly the states we need.

But note: the problem states that the total K over test cases is at most 100,000. However, N is up to 3000. The number of states is about (N+1)*(N+2)/2 which is about 3000*3001/2 = 4.5e6. This is acceptable for precomputation? But T can be up to 100,000. However, we cannot precompute a table of size 4.5e6 for each test case. And T can be up to 100,000? Actually, the input says the sum of K is at most 100,000, but T can be up to 100,000. However, N is at most 3000, but note that the constraints on T: T up to 100,000 and the sum of K is 100,000. But the value of N per test case? It can be different per test case? The problem says: "It is guaranteed that the sum of the value of K on all testcases is not more than 100,000", but there is no constraint on the sum of N.

But note: N is at most 3000, and T up to 100,000, but the condition on the input: the first line of each test case: N and K. However, the sample input has T=4. 

We must design:

  Option 1: Precompute a table for all (a,b) for a in [0, 3000] and b in [0, 3000] such that a+b<=3000. Then for each test case, we compute the state (a,b) from the input and then output E[a][b].

  The table size: the number of states is about (3001*3002)//2 ~ 4.5e6. And each state is a double. That is about 4.5e6 * 8 bytes = 36 MB. And we can precompute once at the beginning.

  Then for each test case, we do:

      Read N, K, and the list A of K integers.

      But note: the precomputation was done for a fixed global N? However, the recurrence depends on N (the total numbers). Therefore, we cannot precompute a global table independent of N.

  Therefore, we must precompute for each distinct N? But the problem states that N is at most 3000, but T can be up to 100,000. And the distinct values of N? We cannot precompute for every N from 1 to 3000? That would be 3000 * (number of states per N) = 3000 * (N*(N+1)/2) for N up to 3000? That is about 3000 * (3000*3001/2) = 3000 * 4.5e6 = 13.5e9 states, which is too high.

Alternative: we note that the recurrence for a fixed N is independent of the test cases. Therefore, we can precompute a table for each N that we encounter? But T can be 100,000 and N can be any integer from 1 to 3000. The distinct values of N might be 3000. Then we precompute for each distinct N? Then the total memory would be 3000 * (N*(N+1)/2) which is about 3000 * (3000*3001/2) = 13.5e9 states, and each state is a double (8 bytes) -> 108e9 bytes = 108 GB. Not feasible.

We must avoid precomputation for every distinct N. 

Alternative: we can compute E(a,b) on the fly for a given N? But the state space is O(N^2) and N=3000, so 4.5e6 states per N. And if we do this for each distinct N, we would do it for each distinct N that appears in the test cases. However, worst-case T=100,000 and each test case has a distinct N? Then we would do 100,000 times a O(N^2) computation. The total operations: 100,000 * (3000^2) = 100,000 * 9e6 = 900e9 operations -> which is too high.

But note: the problem says T is at most 100,000 and the sum of K is at most 100,000. But the values of N? The constraint says 1<=N<=3000. However, we can have many distinct N. But worst-case, 100,000 distinct N? Then we cannot do 100,000 * O(N^2) for N=3000.

We need a more efficient method.

Alternative: we note that the recurrence for E(a,b) for a fixed N is linear and we can compute it by iterating s from 0 to N, and then for each s, iterating a from 0 to s. The inner loop for a fixed s and fixed N is O(s). The total time per N is O(N^2). 

But if we have T test cases and each with a distinct N, then we do T * O(N^2) = 100,000 * 3000^2 = 900e9, which is too high.

We must avoid doing O(N^2) per distinct N.

But note: the constraints on the input: the total K over test cases is at most 100,000. Also, the values of N per test case: but N is at most 3000. However, worst-case we have 100,000 test cases and each test case has a distinct N. Then we would have to precompute 100,000 tables of size O(N^2) each. That is 100,000 * 4.5e6 states = 450e9 states, which is too high.

Alternative: we can precompute a global table for all N? Actually, we can precompute a three-dimensional table? But that is even worse.

We need to notice: the recurrence for E(a,b) for a fixed N is:

  E(a,b) = (N + a * E(a-1, b+1) + b * E(a, b-1)) / (a+b)   [for a+b>0]

But the recurrence only depends on states with a+b-1 and states with the same a+b. 

Alternatively, we can use generating functions or a closed form? 

But note: we can use the linearity of expectation and express the expectation as a sum of contributions. There is a known technique for coupon collector problems with multiple copies.

We are in state (a,b). We need to collect two copies of each coupon. The process can be broken into stages:

  We have:
    - a coupons that have 0 copies
    - b coupons that have 1 copy
    - c coupons that have at least 2 copies.

The process stops when we have collected two copies of each coupon.

We can use the method of states with the following:

  Let T be the stopping time. We can write:

      T = X_1 + X_2 + ... 

  where X_i is the time to leave the current state.

But the standard approach is to use states (a,b) as above.

However, we note that the recurrence is the same as the one for the coupon collector with two copies. There is a known solution? 

Alternatively, we can use the linearity of expectation and consider the expected time to finish from the current state as the sum of expected steps to get each new coupon in a certain way.

But note: the state (a,b) is sufficient.

We can also note: the recurrence for fixed N is independent of the test cases, so we can precompute a table for each N that we see for the first time and then cache it. Then if two test cases have the same N, we can reuse the table. 

Given that T is up to 100,000, but the distinct values of N might be only 3000 (because N in [1,3000])? Actually, the distinct values of N are at most 3000. Then we can precompute for each distinct N once and then use it for all test cases with that N.

But the total memory: for each distinct N, we store a table of size (N+1) * (N+1) but actually we only need for a+b<=N. The number of states for a fixed N is (N+1)*(N+2)/2. So for distinct N from 1 to 3000, the total memory is:

  Sum_{n=1}^{3000} (n+1)*(n+2)/2 

= (1/2) * Sum_{n=1}^{3000} (n^2 + 3n + 2)

= (1/2) * [ (3000*3001*6001)/6 + 3*(3000*3001)/2 + 2*3000 ] 

This is about (1/2) * (about 27e9) -> 13.5e9 states? Actually, we are summing the number of states per distinct N, which is about 3000 * (3000^2)/2? 

Actually, no: we are summing the number of states for each distinct n (n from 1 to 3000). The number of states for a fixed n is about (n+1)*(n+2)/2. So the total memory is:

  Total_states = sum_{n=1}^{3000} (n+1)*(n+2)/2 

We can compute:

  (n+1)*(n+2)/2 = (n^2+3n+2)/2

  Sum_{n=1}^{3000} n^2 = 3000*3001*6001/6 = 3000*3001*1000.166... ~ 3000*3001*1000 = 9e9 -> but we have 3000*3001*6001/6 = 3000*3001*1000.1666... ~ 3000*3001*1000.1666 = 9,009,004,500 (exactly: 3000*3001*6001//6 = 3000*3001*6001//6 = 500*3001*6001 = 500*18012601 = 9006300500)

  Sum_{n=1}^{3000} n = 3000*3001/2 = 4501500

  Sum_{n=1}^{3000} 1 = 3000

  So:

    Total_states = (1/2) * (9006300500 + 3*4501500 + 2*3000)
                = (1/2) * (9006300500 + 13504500 + 6000)
                = (1/2) * (9019806950 + ...) 

  This is about 4.5e9 states? And each state is a double (8 bytes) -> 36e9 bytes = 36 GB. This is too high.

Therefore, we must avoid storing all tables for all N from 1 to 3000.

Alternative: we precompute the table for a given N only when we see that N for the first time, and then we store it. But worst-case, we might see 3000 distinct N, and then we store 3000 tables. The total memory is about 4.5e9 states? (as above) which is 36 GB? Not acceptable.

We need a different approach.

We note: the recurrence for a fixed N and fixed s (from 0 to N) and then a from 0 to s: 

  For each distinct N, we compute the entire table in O(N^2). The time to compute one table for a fixed N is O(N^2) = 3000^2 = 9e6 operations per distinct N. Then for 3000 distinct N, the total time is 3000 * 9e6 = 27e9 operations. This might be acceptable in C++ in 2 seconds? Probably not: 27e9 operations is 27 seconds in C++? And we have T=100,000 test cases, but the distinct N is 3000, and then for each test case we only do O(1) to get the state (a,b) from the precomputed table? But 27e9 operations is 27 seconds in C++? And 2 seconds time limit? 

But note: the problem says the time limit is 2 seconds per test case? Or total? Actually, the problem says "time limit: 2.0s", meaning total for all test cases? But the constraints say T up to 100,000, and the sum of K up to 100,000. 

But worst-case: distinct N=3000, and we do 3000 tables each in O(N^2) = 9e6, so total 3000 * 9e6 = 27e9 operations. In C++ we can do about 100e6 operations per second? Then 27e9 is 270 seconds -> 4.5 minutes. Too high.

We must optimize.

Alternative: we can avoid precomputation for every distinct N by noticing that the recurrence for different N is similar? Or we can express the expectation in terms of N and the state (a,b) without precomputation? 

We note: the recurrence:

  E(a,b) = (N + a * E(a-1, b+1) + b * E(a, b-1)) / (a+b)

But this recurrence depends on N. How? The numerator has N. 

We can try to find a closed form? 

Alternatively, we can use the linearity of expectation. The overall process can be broken into stages:

  Let T be the total additional time. We can write:

      T = T_0 + T_1 + ... 

  where T_i is the time to get the next coupon that advances the state.

  The states are characterized by the number of coupons that we are still missing a second copy. Actually, we are in state j (number of coupons that have been collected exactly once) and k (number of coupons that have not been collected at all) but we have a and b.

  Actually, the state (a,b) is the same as having a coupons not seen at all and b coupons seen once.

  The process:

      We are in state (a,b). We draw a coupon:

        - With probability a/N: we move to (a-1, b+1)
        - With probability b/N: we move to (a, b-1) [and one coupon now has two copies] 
        - With probability (N-a-b)/N: we stay in (a,b)

      The time to leave the state (a,b) is geometric: the expected number of trials to leave is N/(a+b). And then we move to:

          (a-1, b+1) with probability a/(a+b)
          (a, b-1) with probability b/(a+b)

      Therefore, we have:

          E(a,b) = N/(a+b) + [ a/(a+b) * E(a-1, b+1) + b/(a+b) * E(a, b-1) ]

      => (a+b)*E(a,b) = N + a * E(a-1, b+1) + b * E(a, b-1)

      which is the same as before.

There might be a known formula for the coupon collector with two copies? 

We can derive by defining:

  Let f(a,b) = E(a,b) for fixed N.

  Then we have:

      f(a,b) = N/(a+b) + a/(a+b) * f(a-1, b+1) + b/(a+b) * f(a, b-1)

  with f(0,0)=0.

  We can try to expand:

      f(a,b) = N/(a+b) + a/(a+b) * [ N/(a+b-1) + (a-1)/(a+b-1)*f(a-2,b+2) + (b+1)/(a+b-1)*f(a-1,b) ] 
               + b/(a+b) * [ N/(a+b-1) + a/(a+b-1)*f(a-1,b) + (b-1)/(a+b-1)*f(a,b-2) ]

  This becomes messy.

Alternatively, we might guess that the solution is of the form:

      f(a,b) = N * ( ... )

But I don't know.

We can use the recurrence to compute f(a,b) for a given (a,b) and N without precomputation of the entire table? 

We note that the state (a,b) and we need to compute f(a,b). The recurrence calls f(a-1,b+1) and f(a,b-1). This is a recursive relation. The depth of recursion is a+b, which can be up to N (<=3000). But the branching factor is 2. The total number of states visited is O(2^(a+b)) which is too high.

We must use dynamic programming for the entire table for a fixed N. But then we are back to O(N^2) per distinct N.

Given the constraints that the distinct values of N are at most 3000, and the worst-case distinct N is 3000, then we would do 3000 * O(N^2) = 3000 * (3000^2) = 27e9 states, which is too slow.

But note: the problem says that the total K over test cases is at most 100,000. Also, T is at most 100,000. However, the distinct N can be up to 3000. But 3000 is only the maximum value of N, and the distinct values of N might be only the integers from 1 to 3000, but we won't have more than 3000 distinct values.

But 27e9 states is too many.

Alternative: we can use iterative dynamic programming per distinct N, but we do it only once per distinct N and then cache. And then for each test case, we get the state (a,b) from the table.

 But the issue is the memory and time for the 3000 distinct N: the total number of states is sum_{n=1}^{3000} (n+1 choose 2) = about 4.5e9 states as above, which is too high in memory (36 GB) and time (27e9 operations).

We must seek a more efficient method.

Insight: the coupon collector with two copies can be reduced to a standard coupon collector with different thresholds.

 We can think: the expected time to get two copies of each coupon is the same as the expected time to get two copies of the coupons that are not yet finished.

 We are in state (a,b): 
   - a coupons have 0 copies
   - b coupons have 1 copy.

 We need to collect:
   - at least one copy for the a coupons, and two copies for the b coupons? 
   But note: the b coupons only need one more copy.

 The process is: we keep sampling until we have two copies of everything.

 We can break the additional time into:

   Let X = additional time.

   We can write X = X_1 + X_2 + ... + X_{a} + X_{a+1} + ... + X_{a+b} 
   where the first a terms are for the coupons that are currently missing altogether, and the next b terms are for the coupons that are currently missing one copy.

   But note: the geometric distribution here is not independent.

 However, we can use the linearity of expectation and consider the expected time to get a new copy (either first copy for a coupon in a or second copy for a coupon in b) and then the time to get the next, etc.

  In the state (a,b), the probability of drawing a coupon that advances the state (either by getting a new coupon from the a ones or by getting a second copy for a coupon in the b ones) is (a+b)/N. So the time to get one of these events is geometric with success probability (a+b)/N, so expected waiting time is N/(a+b).

  After this event, we move to either (a-1, b+1) or (a, b-1).

  But then we have to wait for the next event, and so on.

  The entire process can be seen as a sum over states of the waiting times.

  Specifically, let's define a state by the number of coupons that have been collected twice so far? But it's complicated.

  Alternatively, we can use the known result for the coupon collector with multiple copies. The expected time to get at least m copies of each coupon is:

        E = \int_0^\infty [1 - \prod_{i=1}^{n} (1 + \frac{x}{1!} + \frac{x^2}{2!} + \cdots + \frac{x^{m-1}}{(m-1)!}) e^{-x}] dx

  but for m=2, it might simplify.

  Or we can use the linearity of expectation by considering the coverage of each coupon.

  However, the coupons are not independent because of the condition of at least two.

  Another known approach: the expected time to get two copies of every coupon is the same as the maximum over coupons of the time to get two copies of that coupon? But the maximum is not linear.

  We can use the linearity of expectation in a different way: 

      Let T be the additional time.

      We can write: 
          T = \sum_{t=1}^{\infty} I_{t}

      where I_t = 1 if the process has not stopped by time t, and 0 otherwise.

      Then E[T] = \sum_{t=1}^{\infty} P(T>t)

      Now, T>t means that in the additional t draws, there is at least one coupon that does not appear at least twice. 

      Let A_i be the event that coupon i appears fewer than 2 times in the additional t draws.

      Then by inclusion-exclusion:

          P(T>t) = P(\cup_{i=1}^{N} A_i) 
                  = \sum_{i} P(A_i) - \sum_{i<j} P(A_i \cap A_j) + \sum_{i<j<k} P(A_i \cap A_j \cap A_k) - ... 

      But note: we already have some draws (K draws). So we condition on the current state.

      For coupon i, let:
          c_i = current count (0, 1, or 2 or more).

      But note: in the additional t draws, we require that coupon i appears at least (2 - current_count_i) times. If the current_count_i>=2, then we don't require any. If current_count_i=1, then we require at least 1. If current_count_i=0, then we require at least 2.

      Let d_i = max(0, 2 - current_count_i). Then the event A_i is that coupon i appears fewer than d_i times in the additional t draws.

      However, the events are not independent, and inclusion-exclusion is complicated for N=3000 and t up to infinity.

      But note: the additional time might be bounded? Not necessarily, but the probability decays exponentially.

      We can compute:

          E[T] = \sum_{t=0}^{\infty} P(T>t)

      and we have:

          P(T>t) = P(at least one coupon i has not reached two copies in the first t additional draws)

      = \sum_{S \subseteq [N], S\neq \emptyset} (-1)^{|S|+1} P(\forall i\in S: coupon i appears < d_i times in t draws)

      But the probability for a fixed set S: 

          = \prod_{i \in S} [ P(coupon i appears < d_i times in t draws) ] ? 

        -> Actually, no: because the draws are independent for coupons? No, the draws are without replacement? No, the generator returns one coupon uniformly at random each time. The draws are independent. Therefore, the events for different coupons are independent given the multinomial distribution.

        However, the events "coupon i appears < d_i times" are not independent across i because the draws are multinomial: one coupon per draw. But for inclusion-exclusion, we use the probability that for a fixed set S, none of the coupons in S appear enough times. This means that in the t draws, the coupons in S must appear in such a way that for each i in S, the number of times i appears is < d_i.

        This is the same as: the t draws must avoid giving coupon i enough times for every i in S. 

        And since the draws are independent, the probability is the same as: 

            = [ (1 - \sum_{i \in S} p_i)^t ] ? 

        But no, because we are not fixing the number of times for each coupon. 

        Actually, the probability that in t draws, for every i in S, coupon i appears fewer than d_i times is:

            = \sum_{n_1=0}^{min(t, d_1-1)} \sum_{n_2=0}^{min(t-n_1, d_2-1)} ... \binom{t}{n_1, n_2, ..., n_{|S|}} (\prod_{i\in S} p_i^{n_i}) * (1 - \sum_{i\in S} p_i)^{t - \sum_{i\in S} n_i} 

        This is complicated.

        But note: the coupons are symmetric? Only in the set S, and the probability for each coupon is 1/N. 

        Then the probability becomes:

            = \sum_{n=0}^{t} \binom{t}{n} (|S|/N)^n (1 - |S|/N)^{t-n} * [ the probability that given n balls distributed among |S| coupons, each coupon gets fewer than d_i times ]

        And then we need to know the current state for the coupons in S: but the d_i for i in S may be different: for some i in S, if the current count is 0, then d_i=2; if the current count is 1, then d_i=1.

        Let S = S0 ∪ S1, where S0 are coupons in S that currently have count 0, and S1 have count 1.

        Then for a coupon in S0, we require <2 in the additional draws -> so at most 1.
        for a coupon in S1, we require <1 in the additional draws -> so exactly 0.

        Therefore, in the additional t draws, for the set S = S0 ∪ S1, we require:

            - For i in S1: must not appear at all.
            - For i in S0: must appear at most once.

        So the event: the additional t draws use only coupons not in S1, and among the coupons in S0, each appears at most once.

        The probability = [ (1 - |S1|/N) ]^t * [ the probability that in a multinomial distribution over S0 and outside, with the condition that each coupon in S0 appears at most once ]

        But note: the draws that land in S0 must be distributed with no coupon in S0 appearing twice.

        The probability that in t draws, no coupon in S0 appears twice and no coupon in S1 appears at all, is:

            = \sum_{k=0}^{\min(t, |S0|)} [ \binom{|S0|}{k} * \frac{ \binom{t}{k} * k! }{ N^t } * (N - |S0| - |S1|)^{t-k} ] 

        But wait: we are also conditioning on the draws not landing in S1. Actually, the entire mass for S1 is avoided, and the draw must land in the remaining N - |S1| coupons. And within the remaining, we have S0 and the others. 

        The probability for one draw to avoid S1 and also avoid duplicate in S0: 

          However, for inclusion-exclusion over S, we are in the probability space of the additional t draws.

          Given that a draw lands in the set of coupons not in S1, the probability for a coupon in S0 is (1) / (N - |S1|) each.

        Then the probability that in t draws, we have at most one for each in S0 and avoid S1, is:

            = ( (N - |S1|)_{\downarrow t} ) / (N - |S1|)^t   ? 

        where (n)_{\downarrow t} = falling factorial = n*(n-1)*...*(n-t+1) = \binom{n}{t} * t! 

        But only if we require distinct in S0 and any in the outside? Actually, no: because the draws can land in the coupons not in S and not in S1 as well, and there is no restriction on these coupons.

        Actually, the event is: 
            - no draw lands in S1.
            - and the draws that land in S0 are distinct.

        The falling factorial here is for the entire set of size N - |S1|? But we don't require distinctness for the coupons not in S0 and not in S1.

        Therefore, the probability is:

            = [ (N - |S1| - |S0|) + \sum_{k=0}^{min(t,|S0|)} \binom{|S0|}{k} * \binom{t}{k} * k! * (N - |S1| - |S0|)^{t-k} ] / N^t 

        But note: we can avoid the binomial by generating functions.

        The generating function for one draw: 
            For a coupon in S1: 1 (cannot appear) -> generating function: 1 (or x^0) for these.
            For a coupon in S0: we can have at most one -> generating function: 1 + x
            For others: generating function: e^x (unrestricted) but wait, for one draw, we have only one outcome.

        Actually, for one draw, the generating function for the count vector is:

            F = (1) for S1 + (1 + x) for each coupon in S0 + (1 + x + x^2/2! + ... ) for others? -> no, generating functions for multinomial are not exponential.

        Alternatively, the probability is the same as: 

            = [ (N - |S1|)! / (N - |S1| - |S0|)! ) * \binom{t}{k} wait, not exactly.

        This is getting messy.

 Given the complexity, and the constraints that the current state is only defined by (a,b) and a+b<=N, and a<=3000, b<=3000, and the distinct N might be up to 3000, and the worst-case distinct N is 3000, and we have 3000 distinct N, then the total number of states to compute is about 4.5e9, which is too high.

 We must seek to optimize the dynamic programming or to find a closed-form.

 Alternatively, we can note that the recurrence for a fixed N is:

      for s in 0..N:
          for a in 0..s:
              b = s - a
              if s == 0:
                 dp[a][b] = 0
              else:
                 if a == 0:
                     dp[0][b] = (N + b * dp[0][b-1]) / b
                 elif b == 0:
                     dp[a][0] = (N + a * dp[a-1][1]) / a
                 else:
                     dp[a][b] = (N + a * dp[a-1][b+1] + b * dp[a][b-1]) / s

 This is O(N^2) per distinct N.

 We can try to do it in a bottom-up fashion and reuse the dp array for the next N? But the states are for different N.

 However, note: the recurrence for a given N only depends on N and not on previous N. 

 Given the time constraints, we must hope that the distinct N in the test cases are few. 

 But the worst-case distinct N is 3000, and then the total number of states is sum_{n=1}^{3000} (n+1 choose 2) = about 4.5e9 states, and we cannot do that.

 We must therefore hope that in the test cases, the distinct N are not too many. The problem says T up to 100,000 and the sum of K up to 100,000. This implies that many test cases must have the same N. Because if we have 100,000 test cases, and the sum of K is 100,000, then the average K is 1. So many test cases have K=0 or K=1. 

 Therefore, the distinct values of N might be not 3000, but much less. For example, only a few hundred distinct N.

 We can then do:

   Precomputation for each distinct N on the fly: 
      We will cache a dictionary: dp_dict = {} # key: N, value: a 2D dp array for a in [0..N] and b in [0..N] with a+b<=N.

   For each test case:
        read N, K, then the list A.

        if N is in the cache, then get the state (a,b) and then output dp_array[a][b].
        else, 
            compute the dp array for this N: 
                s we go from 0 to N, and for each s, a from 0 to s.
            store in cache.
            then output dp_array[a][b] for the state.

   Then for the state (a,b) of the test case, we do:
        a = number of integers in [1, N] that never appeared in A.
        b = number of integers in [1, N] that appeared exactly once in A.

   How to compute a and b from A?
        We can use an array count[1..N] (initialized to 0) to count the frequency.
        Then:
            a = number of i in [1,N] with count[i]==0
            b = number of i in [1,N] with count[i]==1

   Note: c = N - a - b = number of i in [1,N] with count[i]>=2.

   And the condition: at least one integer has not appeared twice (i.e., c < N) is given.

   But note: it is given in the input.

   The complexity per distinct N: O(N^2) = 3000^ = 9e6.
   If we have D distinct N, then the total time is D * 9e6.

   Given that the sum of K is 100,000, and T up to 100,000, then D (distinct N) might be as large as 3000 in the worst-case, but hopefully the test cases will have many repeated N.

   However, worst-case, if the test cases have distinct N, then D = min(T, 3000) = 3000, and then total time = 3000 * 9e6 = 27e9 operations, which is 27 seconds in C++ (which might be acceptable in C++ in a fast machine? but in Pyton, not likely).

   But the problem says time limit: 2 seconds.

   Therefore, we must hope that the distinct N are few. We can also note that the input says the sum of K is 100,000, and K for each test case is at most 100,000. This means that there are test cases with K=0 or small K, and the value of N might be repeated.

   In sample: T=4, distinct N=3 (1,2,3) -> then we would do 3 * (N_i^2) = 1^2 + 2^2 + 3^2 = 1+4+9 = 14 states.

   So for the sample, distinct N=3, and we do for N=1: states: s=0: a=0,b=0 -> 0; s=1: a=0,b=1 -> (0+1=1): then for a=0,b=1: 
        E(0,1) = (1 + 1 * E(0,0)) / 1 = (1 + 0) / 1 = 1.
        then for a=1,b=0: 
        s=1: a=1,b=0: E(1,0) = (1 + 1 * E(0,1)) / 1 = (1+1)/1 = 2.

   But the sample input:
        "1 0" -> then we have a=1 (because only integer 1 has not appeared), b=0.
        additional expected = 2.0.

        "1 1" -> then a=0, b=1? but there is only one integer. We have one number already: 1. Then count[1]=1, so a=0, b=1 -> expected additional = 1.0.

        "2 10" -> then we have to compute a and b for N=2 and a list of 10 numbers. It is: [3,0] -> wait, the sample input is:

         2 10
         3 0  -> but this is not within [1,2]? 

        Actually, the sample input is:

         4
         1 0

         1 1
         1

         2 10
         3 0  -> wait, the sample input says: "2 10" and then "3 0" -> but then there are 10 numbers: but the next line has two numbers: 3 and 0? -> and 3 is out of [1,2]. 

        This must be a formatting issue. 

        Let me read the sample input carefully:

         4
         1 0   -> test case 1: N=1, K=0 -> then the next line is empty? But the problem says: the second line of each test case contains K integers. Here K=0, so we read an empty line.

         then: 
            1 1   -> test case 2: N=1, K=1, then next line: 1
            2 10   -> test case 3: N=2, K=10, then next line: 3 0  -> but the integers should be between 1 and 2? 

        The sample input says: 
            2 10
            3 0

        This is invalid. 

        The sample input provided in the problem is:

         4
         1 0

         1 1
         1
         2 10
         3 0

        But then the sample output has 4 lines: 
          2.000000000
          1.000000000
          4.000000000
          9.638888889

        So the last test case has N=3, K=0? because the input has:

          3 0

        So the test cases are:

          test1: N=1, K=0 -> a=1, b=0 -> then we need the table for N=1: 
             states: 
                s=0: (0,0) -> 0
                s=1: 
                   a=0,b=1: 
                      E(0,1) = (1 + 1 * E(0,0)) / 1 = (1+0)/1 = 1.0
                   a=1,b=0: 
                      E(1,0) = (1 + 1 * E(0,1)) / 1 = (1+1)/1 = 2.0
                test1: state (1,0) -> 2.0

          test2: N=1, K=1, list=[1] -> count[1]=1, so a=0, b=1 -> then E(0,1)=1.0

          test3: N=2, K=10, list=[3,0] -> but wait, this has K=10 and then a list of 10 integers? The input says: "2 10" and then "3 0", but then there are only 2 integers? 

        The sample input in the problem statement might be:

          4
          1 0

          1 1
          1
          2 10
          3 0   -> this is the fourth test case: which is "3 0", meaning N=3, K=0.

        then the fourth test case: state (3,0) for N=3.

        We compute for N=3:
          s=0: (0,0) -> 0
          s=1: 
             a=0,b=1: E(0,1) = (3 + 1 * E(0,0)) / 1 = 3/1 = 3.0
             a=1,b=0: E(1,0) = (3 + 1 * E(0,1)) / 1 = (3+3)/1 = 6.0
          s=2:
             a=0,b=2: E(0,2) = (3 + 2 * E(0,1)) / 2 = (3+2*3)/2 = 9/2 = 4.5
             a=1,b=1: E(1,1) = (3 + 1 * E(0,2) + 1 * E(1,0)) / 2 = (3 + 1*4.5 + 1*6) / 2 = (3+4.5+6)/2 = 13.5/2 = 6.75
             a=2,b=0: E(2,0) = (3 + 2 * E(1,1)) / 2 = (3+2*6.75)/2 = (3+13.5)/2 = 16.5/2 = 8.25
          s=3:
             a=0,b=3: E(0,3) = (3 + 3 * E(0,2)) / 3 = (3+3*4.5)/3 = (3+13.5)/3 = 16.5/3 = 5.5
             a=1,b=2: E(1,2) = (3 + 1 * E(0,3) + 2 * E(1,1)) / 3 = (3 + 5.5 + 2*6.75) / 3 = (3+5.5+13.5)/3 = 22/3 ≈ 7.3333
             a=2,b=1: E(2,1) = (3 + 2 * E(1,2) + 1 * E(2,0)) / 3 = (3 + 2*(22/3) + 8.25) / 3
                     = (3 + 44/3 + 33/4) / 3   [8.25=33/4]
                     = ( (36 + 176 + 99) / 12 ) / 3   [using common denominator 12: 3=36/12, 44/3=176/12, 33/4=99/12]
                     = (311/12) / 3 = 311/36 ≈ 8.638888889
             a=3,b=0: E(3,0) = (3 + 3 * E(2,1)) / 3 = 1 + 8.638888889 = 9.638888889

        So the fourth test case outputs 9.638888889.

        Therefore, the distinct N in the sample are 1,2,3. For T=4, distinct N=3.

 In the worst-case, distinct N = min(T, 3000) = 3000, and 3000 * (3000^2) = 27e9 states, which is 27e9 double operations (assignments and arithmetic). In C++ we might do 1e9 per second, so 27 seconds. In Python, we might do 1e7 per second, so 2700 seconds.

 Given the constraints of the problem (2 seconds) and that the sum of K is only 100,000, we hope that the distinct N are not 3000.

 Alternatively, we can try to only compute the states that are needed for the test cases? But then for a given N, we might have many test cases with the same N but different (a,b). And we need the entire table anyway for a fixed N.

 So the plan in code:

   cache = {}
   for each test case:
        read T
   for i in range(T):
        read N, K
        read list A of K integers

        if N not in cache:
            # compute dp for all states with a in [0..N] and b in [0..N] with a+b<=N.
            dp = 2D array of size (N+1) by (N+1), initialize to 0.0
            # But we only need a+b<=N. We can iterate s from 0 to N:
            for s in range(0, N+1):   # s = a+b
                for a in range(0, s+1):
                    b = s - a
                    if s == 0:
                        dp[a][b] = 0.0
                    else:
                        if a == 0:
                            # then b>0
                            dp[0][b] = (N + b * dp[0][b-1]) / b
                        elif b == 0:
                            dp[a][0] = (N + a * dp[a-1][1]) / a
                        else:
                            dp[a][b] = (N + a * dp[a-1][b+1] + b * dp[a][b-1]) / s
            cache[N] = dp
        else:
            dp = cache[N]

        # compute a and b from the list A
        count = [0]*(N+1)   # index from 1 to N
        for x in A:
            if 1<=x<=N:
                count[x] += 1
        a = 0
        b = 0
        for i in range(1, N+1):
            if count[i] == 0:
                a += 1
            elif count[i] == 1:
                b += 1
        # output: dp[a][b]

   However, note: the state (a,b) must have a+b<=N, and we have computed for it.

   But also note: it is guaranteed that at least one integer has not been returned at least twice, so we are not in the state (0,0) at the beginning? But we might be in a state where a=0 and b=0 if the current counts are at least 2 for every integer -> but the condition says not. So a>=1 or b>=1.

   But our state (a,b) is well-defined.

   Let's hope that the distinct N are not too many. Given that the sum of K is only 100,000, and T can be up to 100,000, then there might be up to 100,000 distinct N? Only if each test case has a distinct N and N is not repeated.

   But N is in [1,3000], so distinct N is at most 3000.

   Therefore, the worst-case distinct N is 3000, and the total number of states is about 4.5e9, which is too high in Python.

 We must therefore optimize the dp for one N.

 We not can use a 1D array for the current s, and reuse for the next s.

   For a fixed N, we only need to compute for s in [0..N], and for each s, we compute states (a, s-a) for a in [0,s]. 
   We can use a 1D array for the current s and then next s? But the recurrence for a fixed s uses states from s and from s-1.

   Specifically, for state (a,b) with a+b=s, we need:
        E(a-1, b+1) from the same s (which we are computing together) and E(a, b-1) from s-1.

   We can do:

        Let's have an array of size s+1 for the current s.

        We want to compute dp[s][a] = E(a, s-a) for a in [0,s].

        How to compute for a fixed s?
            We know for the same s, we have:
               for a=0: 
                   dp[0] = (N + s * dp_prev[0]) / s   [where dp_prev[0] = E(0, s-1) from s-1? Actually, from s-1, the state (0, s-1) is stored in an array for s-1. And for a=0, we don't have a term from the same s.

               for a>0: 
                   dp[a] = (N + a * (value from the same s at a-1) + (s-a) * (from s-1: state (a, s-a-1)) ) / s

            But the term from the same s: when a>0, we need the value for a-1 in the same s.

        So we can iterate a from 0 to s:

            if a==0:
                dp0 = (N + s * dp_prev[0]) / s   # because in the previous array for s-1, the state (0, s-1) is stored in dp_prev[0]? 
                But wait, for s-1, we have states: a in [0, s-1]. The state (0, s-1) is the first state in the array for s-1? 
                Actually, for s-1, we stored an array of length s: for a=0 to a=s-1, the state (a, s-1-a). 
                The state (0, s-1) is when a=0, and we stored it in the first element.

            if a>0:
                dp[a] = (N + a * dp[a-1] + (s-a) * dp_prev[a]) / s 
                        because: 
                            a * E(a-1, s-a+1) = a * dp[a-1]   [because state (a-1, s-a+1) is the state in the current s at index a-1? 
                            But note: state (a-1, s-a+1) = (a-1, (s) - (a-1)) -> this is the state at a-1 in the current s array.
                            and b * E(a, b-1) = (s-a) * E(a, s-a-1) = (s-a) * (state in the previous array for s-1 at a) 
                                because in the array for s-1, the state (a, s-1 - a) = (a, s-a-1) is stored at index a.

        Therefore, we can do:

            dp_prev = [0]   # for s=0: state (0,0) -> then for s=1, we use dp_prev of length 1 (s=0: only one state)

            for s in range(1, N+1):
                dp_curr = [0]*(s+1)   # for a in [0..s]
                for a in range(0, s+1):
                    if a == 0:
                        # state (0,s)
                        # from s-1: state (0, s-1) is stored in dp_prev[0] (because in the array for s-1, the first state is a=0, then a=1, ...)
                        # but wait: in the array for s-1, we stored states for a=0,1,...,s-1. The state (0, s-1) is at index0.
                        dp_curr[0] = (N + s * dp_prev[0]) / s   if s>=1, then a=0: b=s>0, so the recurrence: (0,s) = (N + s * E(0, s-1)) / s.
                    else:
                        # a>=1
                        # state (a, s-a)
                        #   = [ N + a * E(a-1, s-a+1) + (s-a) * E(a, s-a-1) ] / s
                        # Here, E(a-1, s-a+1) = state (a-1, s - (a-1)) in the current s array -> which is dp_curr[a-1]
                        #        because: in the current array, for index a-1: we have state (a-1, s - (a-1)) = (a-1, s - a + 1)
                        # And E(a, s-a-1) = state in the previous array (s-1) for a: because in the array for s-1, the state (a, (s-1)-a) = (a, s-1-a) = (a, s-a-1) is stored at index a (if a <= s-1). 
                        #   But note: s-1>=a, because s>=a+1 (since a>=1 and s>=a+1? because a<=s, but also s-1 might be <a? -> a<=s-1 is required for the state (a, s-a-1) to be in the s-1 array.

                        # However, a can be from 1 to s. For a=s, then b=0, and we use a separate branch.
                        if a == s:
                            # then b=0
                            # recurrence: 
                            #   E(s,0) = (N + s * E(s-1,1)) / s
                            # But E(s-1,1) = state (s-1, 1) in the array for s-1? 
                            #   But the array for s-1 has states for a in [0, s-1]. The state (s-1, 1) would be: a = s-1, b = 1 -> and a+b = (s-1)+1 = s, so it is not in the array for s-1, it is in the array for s.
                            #   This is a mistake.

                        Let me reexamine: 
                          In the recurrence for state (a,b) with a+b=s, we have:
                             if a>0 and b>0: we use E(a-1, b+1) and E(a, b-1)
                             if a>0 and b=0: then we are in the state (a,0) and the recurrence is: 
                                 E(a,0) = (N + a * E(a-1,1)) / a

                          The state (a-1,1) has a'=a-1, b'=1, and a'+b'=a, which is less than s (since s=a) and a>=1, so a>=1 and s=a> a-1+1 = a, so it is in the array for s'=a, but not in the array for s-1.

                        Therefore, we must not have a=s and b=0 in the a>0 and b>0 branch. In fact, we already have a branch for b=0.

                        So in the loop for a in [0,s], we can do:

                            if a==0: handled
                            else if a==s: then b=0, so we use the recurrence for b=0:
                                 dp_curr[a] = (N + a * (value from state (a-1,1)) ) / a

                                 But the state (a-1,1) has total (a-1)+1 = a, which is the same as s? -> it is in the current s array? 
                                 However, we are in the process of computing the current s array, and we are at a=s. But we haven't computed state (a-1,1) because a-1 = s-1, and in the current s array, we have only computed from a=0 to a=s-1.

                        So we can do for a from 0 to s, but we must do a=0 and then a=1,2,...,s. And for a=s, we have the value for state (s-1,1) from the current array? -> but (s-1,1) is in the current array at index=s-1? -> no, the state (s-1,1) has a=s-1 and b=1, and a+b = s, so it is in the current array at index = s-1.

                        And we have just computed it at a=s-1.

                        Therefore, we can do:

                            for a in range(0, s+1):
                                if a==0:
                                    dp_curr[0] = (N + s * dp_prev[0]) / s   [because dp_prev[0] = E(0, s-1) from array for s-1]
                                else:
                                    # a>=1
                                    if a == s:
                                        # then b=0, and we need E(s-1,1) = the state (s-1,1) = in the current array at a=s-1: that is dp_curr[s-1]
                                        dp_curr[a] = (N + a * dp_curr[a-1]) / a   # wait, (a-1,1) is at a-1 in the current array? 
                                        # But note: the state (a-1,1) is (s-1,1) and we have a=s, so a-1 = s-1. 
                                        # And the state (s-1,1) is stored at index = s-1 in the current array? 
                                        # But in the current array, we index by a. The state (s-1,1) has a = s-1, and in the current array for s, we have an element at index=s-1: which we computed in the previous iteration of a (a=s-1) in this loop.
                                        # Therefore, we have already computed dp_curr[s-1] when a=s-1.
                                        # But the recurrence for a=s uses state (a-1,1) = (s-1,1) = dp_curr[s-1] (which is stored at index=s-1) and also there is no term for b (because b=0) and the recurrence is: 
                                        #   E(s,0) = (N + s * E(s-1,1)) / s
                                        #   so = (N + s * dp_curr[s-1]) / s
                                        # But note: in the recurrence it is written as: 
                                        #   E(a,0) = (N + a * E(a-1,1)) / a 
                                        #   for a>0. So a = s, then = (N + s * E(s-1,1)) / s = (N + s * dp_curr[s-1]) / s.
                                        # However, we can write: 
                                        #   dp_curr[s] = (N + s * dp_curr[s-1]) / s
                                    else:
                                        # 1<=a<=s-1: then b = s-a >0.
                                        term1 = a * dp_curr[a-1]   # E(a-1, b+1) = E(a-1, s-a+1) = state in current array at a-1, which we computed in the previous a (a-1) in this loop.
                                        term2 = (s-a) * dp_prev[a]   # E(a, s-a-1) = state in the array for s-1 at a: because in s-1, the state (a, (s-1)-a) = (a, s-a-1) is stored in dp_prev[a] (if a<=s-1, which it is because a<=s-1)
                                        dp_curr[a] = (N + term1 + term2) / s

                        But note: for a=1, we use dp_curr[0] which we have computed.

                        However, we must ensure that in the previous array (dp_prev) we have enough length? dp_prev is for s-1 and has length s. And we are accessing dp_prev[a] for a from 1 to s-1, and a<=s-1, so it is valid.

   Then we set dp_prev = dp_curr for the next s.

   For the next test case with the same N, we can reuse the entire computation.

   The memory: for a fixed N, we store one array for the current s, and we iterate s from 1 to N. The total memory is O(N^2)? But we only store the last dp_prev and the current dp_curr, so O(N) per N.

   But we are caching the entire dp table for N: because after we finish the entire N, we cache dp_prev? But we don't need the entire table for future test cases with the same N? 

   Actually, for a future test case, we only need the value for a specific (a,b). But we might need any (a,b) with a+b<=N. How to store? 

   We can store a 2D array for all (a,b) with a+b<=N. But that is O(N^2) per distinct N. 

   Or we can store a 1D array for each s, but then when the test case comes, we know a and b, and s=a+b, and we need the element at a in the array for s.

   But we are not storing all s arrays, we are only storing the last s array. 

   So to cache for a given N, we must store a 2D array or at least a list of arrays for s=0..N.

   Alternatively, we can store a 1D array of size (N+1) for the states: for each a, we store the value for state (a, s-a) for the s that we computed? But then we lose b.

   Actually, we want for a given (a,b), to return dp[a][b]. We can store a 2D array of size (N+1) by (N+1), but we only fill the triangle a+b<=N.

   However, the memory for one N is O(N^2), and for 3000 distinct N, the total states is about 4.5e9 states, which is 36 GB.

   Given the constraints, we might have to hope that the distinct N are not 3000.

   Given the time of 2 seconds and the fact that the sum of K is only 100,000, it is likely that the test cases have small N.

   Or we can try to not cache the entire table, but only the last state of the dp (which is for s=N) and then discard the rest? But then for a test case with the same N, we would have to recompute.

   We can cache the entire table for each distinct N, but only for the ones that appear, and hope that the number of distinct N is small.

   Given the sample has distinct N=3, and the worst-case distinct N=3000, and 3000 is not small, we are in a bind.

   But note: the problem constraints on T: the sum of K is only 100,000. This means that there are many test cases with small K. It is possible that there are also many test cases with small N.

   And the condition: N<=3000, but the typical N might be small.

   We will implement and hope.

   Alternatively, we can use a different formula.

   There is a known result: the expected time to get two copies of each coupon from state (a,b) is:

        E(a,b) = N * ( H_{N} - H_{a} ) + N * \sum_{i=1}^{b} \frac{1}{a+i} 

   where H_n = harmonic number = 1 + 1/2 + ... + 1/n.

   Let me test with the sample: 
        test1: N=1, a=1, b=0: 
            = 1 * (H_1 - H_1) + 1 * (sum_{i=1}^{0}) = 0 + 0 = 0 -> but expected is 2.

   So this is not matching.

   Another known approach: the expected time from state (a,b) is the same as the expected time to get a new coupon (which is not needed) and then for the ones that are needed.

   It is not straightforward.

   Given the time, we go with the dynamic programming and hope that the distinct N are small.

   Or we can use a different recurrence that is faster? 

   We can note that the recurrence is:

        f(a,b) = N/(a+b) + [ a/(a+b) * f(a-1, b+1) + b/(a+b) * f(a, b-1) ]

   and we can iterate by b first and then a, but it's the same.

   Or we can use: 
        f(a,b) = \frac{N}{a+b} + \frac{a}{a+b} f(a-1, b+1) + \frac{b}{a+b} f(a, b-1)

   then:
        f(a,b) - f(a, b-1) = \frac{N}{a+b} + \frac{a}{a+b} [ f(a-1, b+1) - f(a, b-1) ] 

   but this doesn't seem to help.

   We might try to guess that f(a,b) = N * ( g(a) + h(b) ) or something.

   But I don't know.

 Given the complexity of the problem and the constraints on the input (sum of K is only 100,000), we hope that the distinct N are not more than a few hundred.

   If distinct N = 100, then the total number of states = sum_{n=1}^{100} (n+1 choose 2) = about 100*101*201/6 = 338350, which is acceptable.

   If distinct N = 300, then the total states = sum_{n=1}^{300} (n+1 choose 2) = about 300*301*302/6 = 300*301*50.333 = about 4.5e6, acceptable.

   If distinct N = 1000, then total states = sum_{n=1}^{1000} (n+1 choose 2) = (1000*1001*1002)/6 = 167,167,000, which is 167e6, acceptable in C++ in 2 seconds, but in Python, 167e6 might be acceptable.

   If distinct N=2000, then total states = (2000*2001*2002)/6 = 1335,334,000, about 1.3e9, which might be borderline in C++ but in Python it is too high.

   But distinct N might be only the ones that appear in the test cases, and there are T test cases. The distinct N is at most 3000, but the sum of the number of states over distinct N is sum_{n=1}^{N_max} (n+1 choose 2) for each distinct n.

   However, if a particular N appears many times in test cases, we only compute its table once.

   Therefore, the time is dominated by the sum over distinct n in [1,3000] of (n+1 choose 2).

   The sum_{n=1}^{3000} (n+1 choose 2) = sum_{n=1}^{3000} (n+1)*n/2 = (1/2) * sum_{n=1}^{3000} (n^2 + n) = (1/2) * [ n(n+1)(2n+1)/6 + n(n+1)/2 ] 
        = (1/2) * [ 3000*3001*6001/6 + 3000*3001/2 ] 
        = (1/2) * [ 9004501500 + 4501500 ]
        = (1/2)*9009003000 = 4504501500.

   That is 4.5e9 states, and for each state we do a few arithmetic, in Python we might do 1e7 states per second, so 4504501500 / 1e7 = 450 seconds = 7.5 minutes.

   Therefore, in Python we must hope that the distinct N are small or we need a faster method.

   Given the problem constraints and the sample, it is not for Python? 

   But the problem says: your answer will be considered correct if the relative or absolute error <= 1e-6.

   And the sample in Python should pass for distinct N=3.

   We might need to use a faster language or optimize in PyPy or C++.

   Since the problem does not specify the language, we must provide an editorial for the intended solution.

   The intended solution is to use dynamic programming per distinct N and to use the recurrence with optimized order (iterating s then a) and caching.

   In C++ with 27e9 states, and each state a few operations (3-5), and compiler optimization, it might run in 10-20 seconds.

   But the time limit is 2 seconds.

   Therefore, we need a more efficient method.

   After research, there is a known formula for the expected time to get at least m copies of each coupon in terms of the current count. 
   But I don't know.

   Alternatively, we can use the recurrence in a more efficient way by not precomputing the entire table, but only the states that are needed for the test case. However, the recurrence for one state (a,b) might call a-1 and b+1, which in turn calls a-2 and b+2, and so on, and also calls (a,b-1) which calls (a,b-2), etc. The state (a,b) might spread to many states. And the total states in the entire space is O(N^2) anyway.

   Given the time, we output the solution with dynamic programming and caching of the entire table per distinct N, and hope that the distinct N are small.

   Or note: the sample input may have only small N.

   The problem says: 1<=N<=3000, but also 0<=K<=100000, and the sum of K over test cases <=100000.

   This means that there are at most 100,000 numbers in the lists A. 

   And the distinct values of N in the test cases might be at most 3000, but the test cases with large N are few.

   In summary, we do:

      cache = {}
      for each test case:
          read N, K, then list A of K integers.
          if N is in cache, then get the dp_table = cache[N]
          else:
              dp_table = 2D array of size (N+1) x (N+1) (but only the triangle a+b<=N is filled)
              compute as described (iterating s from 0 to N, and a from 0 to s) with the recurrence.
              cache[N] = dp_table

          compute a = count of i in [1,N] with count[i]==0
          compute b = count of i in [1,N] with count[i]==1
          output dp_table[a][b]

   We hope that the distinct N are not too many.

   For the sample, it works.

   For the worst-case of 3000 distinct N, it is 4.5e9 states, which in C++ might be borderline (4.5e9 assignments, each a few operations, and a modern computer might do 1e9 per second, so 4.5 seconds) in C++. 

   Given that the problem has time limit 2 seconds, this might be borderline in C++ for the worst-case.

   But the problem says the sum of K is only 100,000, so likely there are not 3000 test cases. 

   Note: T is up to 100,000, but the distinct N is at most 3000.

   Therefore, we do at most 3000 pre computations, each O(N^2) for its own N.

   Total states = sum_{n=1}^{3000} (n+1 choose 2) = 4.5e9, which in C++ might run in 5-10 seconds.

   So the intended solution in C++ might use this and optimize the inner loop.

   We can use a vector and iterate only for a+b<=N.

   In C++:

      for (int n_value: distinct_N_list) {
          int N = n_value;
          vector<vector<double>> dp(N+1, vector<double>(N+1, 0.0));
          // or we can use one dimensional indexing: by a and b, but we iterate by s.
          // or we can use a vector<double> for the current s.
          // to save memory and time (cache-friendly), we do a nested loop: s from 0 to N, and a from 0 to s.

          // But we use a 2D dp[a][b] for a in [0..N], b in [0..N] with a+b<=N.
          for (int s=0; s<=N; s++) {
              for (int a=0; a<=s; a++) {
                  int b = s - a;
                  if (s==0) {
                      dp[a][b] = 0.0;
                  } else {
                      if (a==0) {
                          dp[0][b] = (N + b * dp[0][b-1]) / b;
                      } else if (b==0) {
                          dp[a][0] = (N + a * dp[a-1][1]) / a;
                      } else {
                          dp[a][b] = (N + a * dp[a-1][b+1] + b * dp[a][b-1]) / s;
                      }
                  }
              }
          }
          cache[N] = dp;
      }

   Then for a test case, we compute a and b, and output dp[a][b].

   Given the worst-case distinct N=3000, and the total number of states is about 4.5e9, and in C++ each state is a double assignment and a few arithmetic, it might be acceptable in optimized C++ (with -O2) on a fast machine.

   Or we can use a different order: by s from 0 to N, and within s, a from 0 to s, and use a 1D array for the current s and reuse the array for s-1.

   Specifically, as the one with dp_prev and dp_curr to avoid a 2D array of size O(N^2), and also to be cache friendly.

   In C++ for one fixed N:

        vector<double> dp_prev = {0.0}; // s=0
        vector<vector<double>> full_dp; // if we want to store the whole thing for caching for this N, we will store by s then a.
        // But for the test case, we only need one value (a,b) at the end.

        // However, for caching, we need to store the entire table? or can we answer a test case immediately after the entire table is computed?
        // But we have many test cases for the same N. So we must store the entire table.

        // So we might as well use a 2D array.

   Given the constraints and the worst-case distinct N=3000 and total states=4.5e9, and in C++ a vector of size 4.5e9 doubles is 36 GB, which is above the memory limit 256 MB.

   Therefore, we must not store the entire table for all distinct N.

   But for one distinct N, the table for that N is O(N^2) = 3000^2 = 9e6 states, and for 3000 distinct N, the total is 4.5e9 states, 36 GB, which is above 256 MB.

   We must therefore not cache the tables for all distinct N. 

   However, then if we have many test cases with the same N, we would have to recompute the table for that N every time? 
        No, we can cache for the distinct N that have appeared, but then if distinct N is 3000, the total memory is 36 GB, which is not acceptable.

   This is a dilemma.

   We must therefore only cache for a limited time. We can use an LRU cache for the tables, but the problem says T up to 100,000, and the distinct N only 3000, and we would need to cache all.

   Given the memory limit 256 MB, we can only cache a few tables. For example, if we cache only one table, then if test cases interleave different N, we will recompute.

   The a and b for each test case: the state (a,b) we can compute from the list A in O(K) or O(N). 

   Therefore, the only hope is that the distinct N are few. If the distinct N are few (say 200), then the total memory is sum_{n in distinct_N} (n+1 choose 2) = for n up to 3000, but distinct_N=200, then the largest n is 3000, and the table for n=3000 is 3000^2=9e6 states, which is 72 MB (9e6 * 8 bytes). And for 200 such tables, the memory is 200 * 72e6/10 = 14400 MB = 14 GB, not acceptable.

   Therefore, we must not cache, and hope that the test cases with the same N are consecutive, and then we can cache only the current N.

   Specifically:

        sort the test cases by N.

        or group by N.

        then for each group of test cases with the same N, we compute the table once and then for each test case in the group, we compute a and b and output dp[a][b].

   This way, we only store one table at a time.

   The memory for one table is O(N^2) = for the largest N in the group, and we only do one N at a time.

   The total memory is the table for the current N, which is O(N^2) = 3000^2 = 9e6 doubles = 72 MB, which is within 256 MB.

   The time is: sum_{n in distinct_N} O(n^2) = 4.5e9 for distinct N=1..3000, which in C++ is 4.5 seconds.

   Given that the problem has 2 seconds in C++, this might be borderline.

   However, the sum_{n=1}^{3000} (n^2) = 3000*3001*6001/6 = 90045005000 ? 
        no: sum_{n=1}^{3000} (n^2) = 3000*3001*6001/6 =  approximately 3000*3000*6000/6 = 3000*3000*1000 = 9e9.

   But note: the number of states for one N is about (N+1)*N/2, so the total number of states is sum_{n=1}^{3000} (n+1)*n/2 = (1/2) sum_{n=1}^{3000} (n^2+n) = (1/2) [ n(n+1)(2n+1)/6 + n(n+1)/2 ] = (1/2) ( approximately 2n^3/6 = n^3/3 ) = 3000^3/3 = 9e9, which is 9e9 states, and in C++ doing 9e9 states might take 9 seconds.

   Therefore, in C++ we must hope that the distinct N are only the ones that appear and that the typical N is small.

   Or use a faster method.

   There is a known solution: 
        Let i = a, j = b.
        Then: 
           answer = (2*N - i) + (N-1) * harmonic(i) - (N) * harmonic(j) 
        is not.

   Given the complexity, we output the solution as dynamic programming in C++ with grouping by N and only one table at a time, and hope that the average N is small.

   Given the sample, it works.

   For the worst-case where all test cases have the same N (T=100,000) then distinct N=1, and we do one table of size O(N^2) = 9e6 states, and then for each test case we do O(K) to compute a and b, and then output dp[a][b]. The total time is O(N^2) + T  * O(K) = 9e6 + 100,000 * (sum of K is 100,000) -> 100,000*(some constant for each test case) = acceptable.

   Therefore, the solution in C++ (or any fast language) is:

        Read T.
        Read test cases and store (N, K, list A) for each test case.

        Group test cases by N.

        For each group ( with a specific N, and a list of test cases in this group):
            // Compute the dp_table for this N: a 2D array for a in [0..N] and b in [0..N] with a+b<=N.
            // But we only need states that appear in the test cases? -> we need the entire table.
            // Use: 
                vector<vector<double>> dp ( N+1, vector<double>(N+1, 0.0 );
                for (int s=0; s<=N; s++) {
                    for (int a=0; a<=s; a++) {
                        int b = s - a;
                        if (s==0) {
                            dp[a][b] = 0.0;
                        } else {
                            if (a==0) {
                                // use state (0, b-1) from the table
                                dp[0][b] = (N + b * dp[0][b-1]) / b;
                            } else if (b==0) {
                                // use state (a-1,1) from the table: 
                                dp[a][0] = (N + a * dp[a-1][1]) / a;
                            } else {
                                dp[a][b] = (N + a * dp[a-1][b+1] + b * dp[a][b-1]) / s;
                            }
                        }
                    }
                }

            // Then for each test case in the group:
                // compute a and b by:
                vector<int> count(N+1, 0);
                for ( each number in the list A of this test case) {
                    if (x>=1 && x<=N) count[x]++; 
                }
                a = 0, b = 0;
                for (int i=1;<=N; i++) {
                    if (count[i]==0) a++;
                    else if (count[i]==1) b++;
                }
                printf("%.9f\n", dp[a][b]);

        // then free the dp table for this N.

   This meets the memory limit: only one table of size (N+1) * (N+1) at a time, and the largest N is at most 3000, so (3000+1)^2 = about 9e6 doubles = 72 MB.

   The time for grouping: T up to 100,000, and then the inner loop for each test case in the group: the work to compute a and b is O(N+K_i), and the sum of K_i over test cases is 100,000, but also there is a loop over i=1..N for each test case. The sum over one group: if there are t test cases in the group, then the total work for a and b is t * O(N) + (sum of K_i in the group).

   The sum of K_i over the entire test cases is 100,000, so that is acceptable.

   However, the work for the group might be t * N, and if a group has many test cases, and N is large, it might be O(t * N) which could be 100,000 * 3000 = 30e6 per group, and there are groups for each distinct N, but the sum over groups of (t * N) might be (number of test cases) * (their own N) -> not directly.

   Actually, for a group with a given N, the work for the test cases in the group is: 
        total_work_group = (number of test cases in the group) * (N) + (sum of K_i in the group)
   The sum over groups of (sum of K_i in the group) = total_sum_K = 100,000.
   The sum over groups of (number of test cases in the group * N) = \sum_{n} ( (number of test cases for n) * n )

   In the worst-case, if there is a group with n=3000 and test cases=100,000, then work = 100,000 * 3000 = 300e6, which is acceptable in C++.

   Therefore, the solution is to group by N.

   Summary of the solution for the editorial:

   - Group the test cases by the value of N.
   - For each group with a fixed N:
        Precompute a 2D DP table for all states (a, b) such that a>=0, b>=0, and a+b<=N.
        The recurrence: 
             dp[a][b] = 0 if a=0 and b=0.
             otherwise:
                 if a==0: 
                     dp[0][b] = (N + b * dp[0][b-1]) / b
                 else if b==0:
                     dp[a][0] = (N + a * dp[a-1][1]) / a
                 else:
                     dp[a][b] = (N + a * dp[a-1][b+1] + b * dp[a][b-1]) / (a+b)

        This is computed by iterating s = a+b from 0 to N, and within each s, iterating a from 0 to s.
   - For each test case in the group:
        a = number of integers in [1, N] that do not appear in the list A.
        b = number of integers in [1, N] that appear exactly once in the list A.
        The answer is dp[a][b].

   - Note: the condition in the problem guarantees that the state (a,b) is not (0,0) initially.

   This solution has a time complexity of O(N^2) per distinct N and O(N + K_i) per test case.

   Given the constraints, the worst-case distinct N is 3000, and the sum of the number of states over distinct N is about 4.5e9, which in C++ may be acceptable in a few seconds.

   We must output the answer with 9 decimal places.

   The sample will be as given.

   We hope that in the intended data, the distinct N are not 3000, or the N are small.

   This is the best we can do.